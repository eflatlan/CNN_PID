{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eflatlan/CNN_PID/blob/models_sacved/newArc_segment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrPLFO_92Cvr",
        "outputId": "c4d22941-98e1-4a29-ec10-cbb27fae515b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (3.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "!pip install h5py numpy\n",
        "\n",
        "import os\n",
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import numpy as np\n",
        "\n",
        "def extract_neighborhood_map(candidate_positions, mip_positions, neighborhood_size, map_size):\n",
        "    num_samples = candidate_positions.shape[0]\n",
        "    num_candidates = candidate_positions.shape[1]\n",
        "\n",
        "\n",
        "    print(f\"candidate_positions shape = {candidate_positions.shape}\")\n",
        "\n",
        "    # Calculate distances between candidate positions and MIP positions\n",
        "    distances = candidate_positions - mip_positions[:, np.newaxis, :]\n",
        "\n",
        "    # Calculate the norm of distances to get the Euclidean distance\n",
        "    distances = np.linalg.norm(distances, axis=-1)\n",
        "\n",
        "    # Create an empty map for each particle type (pion, kaon, proton)\n",
        "    neighborhood_maps = np.zeros((num_samples, neighborhood_size, neighborhood_size))\n",
        "\n",
        "\n",
        "\n",
        "    print(f\"apply mask:\")\n",
        "# Check if candidate falls within the neighborhood and update the map\n",
        "    mask = distances <= neighborhood_size\n",
        "    map_indices = np.round(candidate_positions).astype(int)\n",
        "\n",
        "    # Adjust the indices to ensure they are within valid range\n",
        "    map_indices[:, :, 0] = np.clip(map_indices[:, :, 0], 0, neighborhood_size - 1)\n",
        "    map_indices[:, :, 1] = np.clip(map_indices[:, :, 1], 0, neighborhood_size - 1)\n",
        "\n",
        "    neighborhood_maps[np.arange(num_samples)[:, np.newaxis],\n",
        "                      map_indices[:, :, 1], map_indices[:, :, 0]] = mask\n",
        "\n",
        "    print(f\"neighborhood_maps shape = {neighborhood_maps.shape}\")\n",
        "    map_sample = neighborhood_maps[1]\n",
        "\n",
        "    # Plot the map\n",
        "    plt.imshow(map_sample, cmap='gray')\n",
        "    plt.colorbar()\n",
        "    plt.title(\"Neighborhood Map for num_samples = 1\")\n",
        "    plt.xlabel(\"X-axis\")\n",
        "    plt.ylabel(\"Y-axis\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    map_sample = neighborhood_maps[2]\n",
        "\n",
        "    # Plot the map\n",
        "    plt.imshow(map_sample, cmap='gray')\n",
        "    plt.colorbar()\n",
        "    plt.title(\"Neighborhood Map for num_samples = 2\")\n",
        "    plt.xlabel(\"X-axis\")\n",
        "    plt.ylabel(\"Y-axis\")\n",
        "    plt.show()\n",
        "    return neighborhood_maps\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, Input\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, Input\n",
        "\n",
        "\n",
        "def create_cnn_model(input_shape=None, name=\"default_name\"):\n",
        "    # Create CNN layers\n",
        "    cnn_input = Input(shape=input_shape)\n",
        "    x = layers.Conv2D(32, (3, 1), activation='relu', name=f\"conv2D_1_{name}\")(cnn_input)\n",
        "    # x = layers.MaxPooling2D((2, 1), name=f\"MaxPooling2D_1_{name}\")(x)\n",
        "    # x = layers.Conv2D(64, (5, 1), activation='relu', name=f\"conv2D_2_{name}\")(x)\n",
        "    # x = layers.MaxPooling2D((2, 1), name=f\"MaxPooling2D_2_{name}\")(x)\n",
        "    # x = layers.Conv2D(64, (7, 1), activation='relu', name=f\"conv2D_3_{name}\")(x)\n",
        "    cnn_output = layers.Reshape((-1, 1), name=f\"Reshape_{name}\")(x)\n",
        "\n",
        "    return models.Model(inputs=cnn_input, outputs=cnn_output)\n"
      ],
      "metadata": {
        "id": "APUgd22_ffd2"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "_clF1C9YOo3R"
      },
      "outputs": [],
      "source": [
        "#!wget https://raw.githubusercontent.com/eflatlan/CNN_PID/dev_floatmap/helper_functions.py\n",
        "#from helper_functions.py import print_points, plot_mapsm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def pad_and_stack(sequences, max_length=None):\n",
        "    # Your existing code\n",
        "    try:\n",
        "        # Try padding, if max_length is not None, pad or truncate to that length\n",
        "        padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', dtype='int32')  # Changed dtype to 'int32'\n",
        "    except ValueError:\n",
        "        # Fallback: manually pad with zeros\n",
        "        max_len = max_length if max_length is not None else max(len(seq) for seq in sequences)\n",
        "        padded_sequences = np.array([np.pad(seq, (0, max_len - len(seq)), 'constant', constant_values=0) for seq in sequences])\n",
        "\n",
        "    return padded_sequences\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def classify_candidates_with_pad_sequences(x_values_data, y_values_data, chi2_values_data, q_values_data, xe_values_data, ye_values_data, candStatus_values_data, max_length_nested):\n",
        "\n",
        "\n",
        "    # Pad the sequences\n",
        "    print(f\"x_values_data Type of sequences: {type(x_values_data)}, shape of sequences: {np.shape(x_values_data)}\")\n",
        "    print(f\"Type of sequences: {type(y_values_data)}, shape of sequences: {np.shape(y_values_data)}\")\n",
        "    print(f\"Type of sequences: {type(chi2_values_data)}, shape of sequences: {np.shape(chi2_values_data)}\")\n",
        "    print(f\"Type of sequences: {type(q_values_data)}, shape of sequences: {np.shape(q_values_data)}\")\n",
        "    print(f\"Type of sequences: {type(candStatus_values_data)}, shape of sequences: {np.shape(candStatus_values_data)}\")\n",
        "\n",
        "    print(type(x_values_data), np.shape(x_values_data))\n",
        "    if np.ndim(x_values_data) == 1:\n",
        "        x_values_data = np.expand_dims(x_values_data, axis=-1)\n",
        "\n",
        "    x_padded = pad_and_stack(x_values_data, max_length=max_length_nested)\n",
        "    y_padded = pad_and_stack(y_values_data, max_length=max_length_nested)\n",
        "    chi2_padded = pad_and_stack(chi2_values_data, max_length=max_length_nested)\n",
        "    q_padded = pad_and_stack(q_values_data, max_length=max_length_nested)\n",
        "    xe_padded = pad_and_stack(xe_values_data, max_length=max_length_nested)\n",
        "    ye_padded = pad_and_stack(ye_values_data, max_length=max_length_nested)\n",
        "    candStatus_padded = pad_and_stack(candStatus_values_data, max_length=max_length_nested)\n",
        "    candStatus_padded = candStatus_padded.astype(int)\n",
        "    # Stack the data into a single array\n",
        "    padded_data = np.stack([x_padded, y_padded, chi2_padded, q_padded, xe_padded, ye_padded, candStatus_padded], axis=-1)\n",
        "\n",
        "    # Create masks for different particle types based on 'candStatus'\n",
        "    pion_mask = (candStatus_padded & 4).astype(bool)\n",
        "    kaon_mask = (candStatus_padded & 2).astype(bool)\n",
        "    proton_mask = (candStatus_padded & 1).astype(bool)\n",
        "\n",
        "    # Initialize arrays for particle candidates\n",
        "    pion_candidates = np.zeros_like(padded_data)\n",
        "    kaon_candidates = np.zeros_like(padded_data)\n",
        "    proton_candidates = np.zeros_like(padded_data)\n",
        "    non_candidates = np.zeros_like(padded_data)\n",
        "\n",
        "    # Populate particle candidates arrays\n",
        "    pion_candidates[pion_mask] = padded_data[pion_mask]\n",
        "    kaon_candidates[kaon_mask] = padded_data[kaon_mask]\n",
        "    proton_candidates[proton_mask] = padded_data[proton_mask]\n",
        "    non_candidates[~(pion_mask | kaon_mask | proton_mask)] = padded_data[~(pion_mask | kaon_mask | proton_mask)]\n",
        "\n",
        "    #return pion_candidates, kaon_candidates, proton_candidates, non_candidates\n",
        "    return non_candidates, non_candidates, non_candidates, non_candidates, candStatus_padded\n"
      ],
      "metadata": {
        "id": "aCiVl64YHGlc"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_candidates(x_values_data, y_values_data, chi2_values_data,  q_values_data, xe_values_data,  ye_values_data, candStatus_values_data):\n",
        "    pion_candidates = []\n",
        "    kaon_candidates = []\n",
        "    proton_candidates = []\n",
        "    non_candidates = []\n",
        "\n",
        "    for i in range(len(x_values_data)):\n",
        "        candStatus = int(candStatus_values_data[i])\n",
        "\n",
        "        complete_candidate_data = [\n",
        "            x_values_data[i], y_values_data[i], chi2_values_data[i],  q_values_data[i], xe_values_data[i],  ye_values_data[i], candStatus_values_data[i]\n",
        "        ]\n",
        "\n",
        "        is_candidate = False\n",
        "        # ef : now removed classification\n",
        "        if true or candStatus & 4:  # Pion\n",
        "            pion_candidates.append(complete_candidate_data)\n",
        "            is_candidate = True\n",
        "\n",
        "        if true or candStatus & 2:  # Kaon\n",
        "            kaon_candidates.append(complete_candidate_data)\n",
        "            is_candidate = True\n",
        "\n",
        "        if true or candStatus & 1:  # Proton\n",
        "            proton_candidates.append(complete_candidate_data)\n",
        "            is_candidate = True\n",
        "\n",
        "        if not is_candidate:  # None of the above\n",
        "            non_candidates.append(complete_candidate_data)\n",
        "\n",
        "    #return pion_candidates, kaon_candidates, proton_candidates, non_candidates\n",
        "    return non_candidates, non_candidates, non_candidates, non_candidates\n"
      ],
      "metadata": {
        "id": "k0ryVGNiN5ZW"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_first_instance(X_pion, X_kaon, X_proton, MIP_position, RAD_position, phiP, thetaP):\n",
        "    i = 5\n",
        "    # Extract values for the first instance\n",
        "    phiP_val = phiP[i]\n",
        "    thetaP_val = thetaP[i]\n",
        "\n",
        "    print(f\"plot_first_instance {X_pion[i].shape} \")\n",
        "    XYp = X_pion[i]\n",
        "    Xp = XYp[:, 0]  # All rows of the first column (X-coordinate)\n",
        "    Yp = XYp[:, 1]  # All rows of the second column (Y-coordinate)\n",
        "\n",
        "    XYk = X_kaon[i]\n",
        "    Xk = XYk[:, 0]  # All rows of the first column (X-coordinate)\n",
        "    Yk = XYk[:, 1]  # All rows of the second column (Y-coordinate)\n",
        "\n",
        "    XYpr = X_proton[i]\n",
        "    Xpr = XYpr[:, 0]  # All rows of the first column (X-coordinate)\n",
        "    Ypr = XYpr[:, 1]  # All rows of the second column (Y-coordinate)\n",
        "\n",
        "    # Plotting X_pion, X_kaon, and X_proton\n",
        "    plt.scatter(Xp, Yp, color='red', label='X_pion')\n",
        "    plt.scatter(Xk, Yk, color='blue', label='X_kaon')\n",
        "    plt.scatter(Xpr, Ypr, color='black', label='X_proton')\n",
        "\n",
        "    xm, ym = MIP_position[i]\n",
        "    xr, yr = RAD_position[i]\n",
        "\n",
        "    # Plotting MIP_position and RAD_position\n",
        "    plt.scatter(xm, ym, color='green', marker='x', label='MIP_position')\n",
        "    plt.scatter(xr, yr, color='orange', marker='x', label='RAD_position')\n",
        "\n",
        "    # Adding phiP and thetaP to the title (converting to float)\n",
        "    plt.title(f\"First Instance\\nphiP: {float(phiP_val):.2f}, thetaP: {float(thetaP_val):.2f}\")\n",
        "\n",
        "    # Setting the aspect ratio to equal\n",
        "    plt.axis('equal')\n",
        "\n",
        "    # Adding legend\n",
        "    plt.legend()\n",
        "\n",
        "    # Displaying the plot\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "ui9UZvaI8ri6"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "Y9ZOOuC_Dhay"
      },
      "outputs": [],
      "source": [
        "#@title Default title text\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy.linalg import norm\n",
        "import os\n",
        "import h5py\n",
        "import tensorflow as tf\n",
        "\n",
        "# to check the impact of resolution in the 2d-map;\n",
        "# print the difference between the filledBins vector versus the map (map is restricted by resolution)\n",
        "def print_points(filled_bins_array = None, map_array = None, mip_position_array = None, resolution = 10):\n",
        "\n",
        "    length = map_array.shape[0]\n",
        "    distances_bins_list = []\n",
        "    distances_map_list = []\n",
        "\n",
        "    print(f\"filled_bins_array shape = {filled_bins_array.shape}\")\n",
        "    print(f\"map_array shape = {map_array.shape}\")\n",
        "    print(f\"mip_position_array shape = {mip_position_array.shape}\")\n",
        "\n",
        "\n",
        "    for i in range (1, length):\n",
        "\n",
        "        filled_bins = np.array(filled_bins_array[i])\n",
        "        map = np.array(map_array[i, :,:])\n",
        "        mip_pos = np.array(mip_position_array[i, :])\n",
        "\n",
        "        #print(f\"filled_bins shape = {filled_bins.shape}\")\n",
        "        #print(f\"map shape = {map.shape}\")\n",
        "        #print(f\"mip_pos shape = {mip_pos.shape}\")\n",
        "\n",
        "        _mip_position = []\n",
        "        #_mip_position.append(mip_position_array[])\n",
        "        distances2 = []\n",
        "\n",
        "        distances_bins = [norm(np.array(pos) - mip_pos) for pos in filled_bins]\n",
        "\n",
        "        distances_map = []\n",
        "        for y in range(map.shape[0]):\n",
        "            for x in range(map.shape[1]):\n",
        "                if map[y, x] == 1:\n",
        "                    point = (x, y)\n",
        "                    distance = np.linalg.norm(np.array(point) - mip_pos*resolution)\n",
        "                    distances_map.append(distance)\n",
        "\n",
        "\n",
        "\n",
        "        distances_bins_list.append(distances_bins)\n",
        "        distances_map_list.append(distances_map)\n",
        "\n",
        "\n",
        "    # Print the distances for each element in map_data_list\n",
        "    print(f\"Element {i+1} distances:\")\n",
        "    for j, (distances_bins, distances_map) in enumerate(zip(distances_bins_list, distances_map_list)):\n",
        "        print(f\"  Point {j+1}: Distance bins: {distances_bins}\\n, Distance map: {distances_map}\")\n",
        "    print()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def plot_maps(filled_bins_array=None, map_array=None, mip_position_array=None, X_momentum=None, X_refractive_index=None, percentage_to_plot=5, resolution = 10):\n",
        "  \"\"\"\n",
        "  Args : filled_bins_array : array that holds the vectors of filled pads\n",
        "         map_array : 2d  map with a determined resolution (the points in the filled_bins_array element, just restricted by the resolution)\n",
        "         mip_position_array : array of the MIP {x, y} positions\n",
        "\n",
        "         TODO : add mass_category and actual mass?\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  #percentage_to_plot = 0.05 / 10\n",
        "\n",
        "  # Calculate the starting index of the samples to plot\n",
        "  num_samples = map_array.shape[0]\n",
        "  start_index = -num_samples\n",
        "\n",
        "  # Create a subplot with the number of rows based on the number of samples\n",
        "  fig, axes = plt.subplots(nrows=5, ncols=1, figsize=(8, 20))\n",
        "\n",
        "  # Iterate over the samples and plot each map with information\n",
        "  for i, ax in enumerate(axes):\n",
        "      # Get the map and corresponding information\n",
        "      map_data = map_array[start_index + i, :, :]\n",
        "      #mass_category = particle_vector[start_index + i].mass_category\n",
        "      PDG = PDG[start_index + i]\n",
        "      mip_position = mip_position_array[start_index + i]\n",
        "      momentum = X_momentum[start_index + i]\n",
        "      refractive_index = X_refractive_index[start_index + i]\n",
        "\n",
        "      # Plot the map\n",
        "      ax.imshow(map_data, cmap='gray')\n",
        "\n",
        "\n",
        "\n",
        "      #try :\n",
        "      # Add a red dot at the MIP position\n",
        "      ax.plot(mip_position[0]*resolution, mip_position[1]*resolution, 'ro')\n",
        "      #Except exception as e :\n",
        "      #  print(\"caught non mip pos \")\n",
        "      # Set the title with the information\n",
        "      #ax.set_title(f\"Mass: {mass_category}, CKOV: {ckov}, MIP Position: {mip_position}, Momentum: {momentum},  refractive_index: {refractive_index}\")\n",
        "      ax.set_title(f\"PDG: {PDG}, MIP Position: {mip_position}, Momentum: {momentum},  refractive_index: {refractive_index}\")\n",
        "\n",
        "      ax.axis('off')\n",
        "\n",
        "  # Adjust the spacing between subplots\n",
        "  plt.tight_layout()\n",
        "\n",
        "  # Show the plot\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "pcY6LVGpxZQI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec38328d-0045-453b-8ab3-18c889e06aed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-07 11:29:03--  https://raw.githubusercontent.com/eflatlan/CNN_PID/models_sacved/helper_functions.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14594 (14K) [text/plain]\n",
            "Saving to: ‘helper_functions.py.4’\n",
            "\n",
            "\rhelper_functions.py   0%[                    ]       0  --.-KB/s               \rhelper_functions.py 100%[===================>]  14.25K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-09-07 11:29:03 (81.0 MB/s) - ‘helper_functions.py.4’ saved [14594/14594]\n",
            "\n",
            "--2023-09-07 11:29:03--  https://raw.githubusercontent.com/eflatlan/CNN_PID/populateAll/plot_helper_functions.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10809 (11K) [text/plain]\n",
            "Saving to: ‘plot_helper_functions.py.4’\n",
            "\n",
            "plot_helper_functio 100%[===================>]  10.56K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-09-07 11:29:03 (55.6 MB/s) - ‘plot_helper_functions.py.4’ saved [10809/10809]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def create_lr_scheduler(num_epochs=10, warmup_epochs=50):\n",
        "\n",
        "\n",
        "    # endre warmup_epochs\n",
        "\n",
        "    start_lr = 0 # Starting learning rate is 0 for the warm-up\n",
        "    peak_lr = 0.001/(5*1.2) # was /5 # The learning rate we reach at the end of the warm-up\n",
        "    end_lr = 5e-6   # The final learning rate at the end of training\n",
        "\n",
        "    # Calculate decay rate based on peak and end learning rate\n",
        "    # Notice we adjust the total number of epochs by subtracting the warm-up period\n",
        "    exp_decay = -np.log(end_lr / peak_lr) / (num_epochs - warmup_epochs)\n",
        "\n",
        "    # Warm-up followed by exponential decay\n",
        "    def schedule(epoch):\n",
        "        if epoch < warmup_epochs:\n",
        "            return start_lr + ((peak_lr - start_lr) / (warmup_epochs - 1)) * epoch\n",
        "        else:\n",
        "            return peak_lr * np.exp(-exp_decay * (epoch - warmup_epochs))\n",
        "\n",
        "    return tf.keras.callbacks.LearningRateScheduler(schedule)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def plot_lr(num_epochs = 10, history = None):\n",
        "  div = num_epochs/4\n",
        "  lrs = 1e-4 * (10 ** (np.arange(num_epochs)/div))\n",
        "  plt.figure(figsize=(10, 7))\n",
        "  plt.semilogx(lrs, history.history[\"loss\"]) # we want the x-axis (learning rate) to be log scale\n",
        "  plt.xlabel(\"Learning Rate\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "\n",
        "\n",
        "  plt.title(\"Learning rate vs. loss\");\n",
        "\n",
        "!wget https://raw.githubusercontent.com/eflatlan/CNN_PID/models_sacved/helper_functions.py\n",
        "\n",
        "!wget https://raw.githubusercontent.com/eflatlan/CNN_PID/populateAll/plot_helper_functions.py\n",
        "\n",
        "from plot_helper_functions import plot_training_history\n",
        "#from plot_helper_functions import plot_training_history#, plot_dist2mip_histograms, plot_maps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "VRoviFHFMvUa"
      },
      "outputs": [],
      "source": [
        "#def plot_worst_(model, y_test, X_test_map, X_test_momentum, X_test_refractive_index, X_test_ckov, X_test_mip_position, y_pred):\n",
        "def plot_maps(filled_bins_array=None, map_array=None, mip_position_array=None, X_momentum=None, X_refractive_index=None, X_ckov=None, percentage_to_plot=5, resolution = 10):\n",
        "  #  print(\"Shape of y_pred: \", y_pred.shape)\n",
        "  # 1. Predict labels on validation data\n",
        "  #plot_worst(model, y_test, X_test[\"X_test_map\"], X_test[\"X_test_momentum\"], X_test[\"X_test_refractive_index\"], X_test[\"X_test_ckov\"], X_test[\"X_test_mip_position\"], y_pred_test)\n",
        "\n",
        "  # 2. Calculate the difference between predicted and actual labels\n",
        "  losses = tf.keras.losses.categorical_crossentropy(y_test, y_pred).numpy()\n",
        "\n",
        "  # Sort the indices of the losses from highest to lowest\n",
        "  sorted_indices = np.argsort(losses)[::-1]\n",
        "\n",
        "  # Get the indices of the worst performing 10%\n",
        "  worst_10_percent_indices = sorted_indices[:int(0.1*len(sorted_indices))]\n",
        "\n",
        "  # Create figure and axes\n",
        "  num_plots = len(worst_10_percent_indices)\n",
        "  #fig, axes = plt.subplots(num_plots, 1, figsize=(8, 20))\n",
        "  fig, axes = plt.subplots(num_plots,figsize=(8, 20))\n",
        "\n",
        "  # Define mass categories\n",
        "  mass_categories = [\"pion\", \"kaon\", \"proton\"]\n",
        "\n",
        "  # 3. Create plots for these cases, including their feature information and predicted vs actual labels\n",
        "  for i, index in enumerate(worst_10_percent_indices):\n",
        "      # Get the map and corresponding information\n",
        "      map_data = map_array[index, :, :]\n",
        "      actual_mass_category = mass_categories[np.argmax(y_test[index])]\n",
        "\n",
        "      print(f\"y_test[index] = {y_test[index]}\")\n",
        "\n",
        "      predicted_mass_category = mass_categories[np.argmax(y_pred[index])]\n",
        "      ckov = X_test_ckov[index]\n",
        "      mip_position = X_test_mip_position[index]\n",
        "      momentum = X_test_momentum[index]\n",
        "      refractive_index = X_test_refractive_index[index]\n",
        "\n",
        "      mass_actual = momentum * np.sqrt(refractive_index**2 * np.cos(ckov)*np.cos(ckov) - 1)\n",
        "\n",
        "      # Check if the value is NaN (invalid Cherenkov angle)\n",
        "      if np.isnan(mass_actual):\n",
        "          mass_actual = \"Invalid\"\n",
        "\n",
        "      # Plot the map\n",
        "      axes[i].imshow(map_data, cmap='gray')\n",
        "\n",
        "      # Add a red dot at the MIP position\n",
        "      axes[i].plot(mip_position[0]*resolution, mip_position[1]*resolution, 'ro')\n",
        "\n",
        "      # Set the title with the information\n",
        "      axes[i].set_title(f\"Actual Mass\")#: {actual_mass_category}, Predicted Mass: {predicted_mass_category},\\nMass: {mass_actual}, Mass_prob = {y_pred[index]} \\nCKOV: {ckov}, MIP Position: {mip_position}, \\nMomentum: {momentum}, Refractive Index: {refractive_index}\")\n",
        "      #\n",
        "      axes[i].set_title(f\"Actual Mass: {actual_mass_category}, Predicted Mass: {predicted_mass_category},\\nMass: {mass_actual}, Mass_prob = {y_pred[index]}, MIP Position: {mip_position}, \\nMomentum: {momentum}, Refractive Index: {refractive_index}\")\n",
        "\n",
        "      #axes[i].set_title(f\"Actual Mass: {actual_mass_category}, Predicted Mass: {predicted_mass_category}, Mass: {mass_actual}\\nCKOV: {ckov}, MIP Position: {mip_position}, Momentum: {momentum}, Refractive Index: {refractive_index}\")\n",
        "      axes[i].axis('off')\n",
        "\n",
        "      print(\"\\n\")\n",
        "      print(f\"  Actual Mass: {actual_mass_category}, Predicted Mass: {predicted_mass_category},\\n Mass: {mass_actual}, Mass_prob = {y_pred[index]} , MIP Position: {mip_position}, \\n  Momentum: {momentum}, Refractive Index: {refractive_index}\")\n",
        "  # Adjust the spacing between subplots\n",
        "  plt.tight_layout()\n",
        "\n",
        "  # Show the plot\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helping class and imports\n"
      ],
      "metadata": {
        "id": "oXEUp3H8NoS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#resolution = 4\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import cycle\n",
        "\n",
        "import sys\n",
        "\n",
        "print(sys.getrecursionlimit()) # Prints 1000\n",
        "\n",
        "print_vals = False\n",
        "from numpy.linalg import norm\n",
        "from tensorflow.keras.backend import expand_dims\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import precision_recall_curve, confusion_matrix\n",
        "\n",
        "from scipy.signal import find_peaks\n",
        "\n",
        "from __future__ import print_function\n",
        "import os\n",
        "import h5py\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Activation, Input, Conv2D, Lambda, Flatten, Dense, concatenate, BatchNormalization, MaxPooling2D, Dropout, LeakyReLU, Masking, Embedding\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# create a callback\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss', # you can monitor 'val_loss' or 'val_accuracy'\n",
        "    patience=100, # stop training if the monitored quantity does not improve for 50 epochs\n",
        "    restore_best_weights=True, # restore model weights from the epoch with the best value\n",
        ")\n",
        "\n",
        "class Constants:\n",
        "    PION_MASS = 0.1396\n",
        "    KAON_MASS = 0.4937\n",
        "    PROTON_MASS = 0.938\n",
        "\n",
        "np.set_printoptions(precision=4)\n",
        "\n",
        "@staticmethod\n",
        "def calculate_mass(momentum, refractiveIndex, ckov):\n",
        "    \"\"\" args : momentum, refractiveIndex, ckov\n",
        "        returns : mass\n",
        "    \"\"\"\n",
        "    mass = momentum * np.sqrt((refractiveIndex * np.cos(ckov))**2 - 1)\n",
        "    return mass\n",
        "\n",
        "\n",
        "class ParticleDataUtils:\n",
        "\n",
        "    def __init__(self, filename = \"default_filename.h5\", percentage_to_read = 100):\n",
        "        self.filename = filename\n",
        "        self.percentage_to_read = percentage_to_read\n",
        "        self.particle_vector = []\n",
        "        self.load_data(filename)\n",
        "\n",
        "        #\n",
        "        self.particle_info = self.process_data(self.particle_vector, self.percentage_to_read)\n",
        "        self.num_particles = len(self.particle_info)\n",
        "\n",
        "\n",
        "        # new scalers to be created :\n",
        "        self.momentum_scaler, self.momentum_stats = self.create_scalar_scaler(\"momentum\")\n",
        "        self.phi_scaler, self.phi_stats = self.create_scalar_scaler(\"phiP\")\n",
        "        self.theta_scaler, self.theta_stats = self.create_scalar_scaler(\"thetaP\")\n",
        "        self.refractive_index_scaler, self.refractive_index_stats = self.create_scalar_scaler(\"refractiveIndex\")\n",
        "\n",
        "        print(\"Created scaler for scalars\")\n",
        "\n",
        "        # 2D\n",
        "        self.mip_scaler, self.mip_stats = self.create_2D_scaler(\"mip_position\")\n",
        "        self.rad_scaler, self.rad_stats = self.create_2D_scaler(\"rad_position\")\n",
        "\n",
        "        # # vector of 2D\n",
        "        # self.proton_scaler, self.proton_stats = self.create_vector_scaler(\"proton_candidates\")\n",
        "        # self.kaon_scaler, self.kaon_stats = self.create_vector_scaler(\"kaon_candidates\")\n",
        "        # self.pion_scaler, self.pion_stats = self.create_vector_scaler(\"pion_candidates\")\n",
        "\n",
        "\n",
        "\n",
        "        #self.ckov_scaler, self.ckov_stats = self.create_scaler(\"ckov\")\n",
        "        #self.distances_scaler, self.distances_stats = self.create_scaler(\"distances\")def classify_candidates(candidates_data):\n",
        "\n",
        "\n",
        "    class Candidate2:\n",
        "        def __init__(self, x_values, y_values, chi2_values, q_values, xe_values, ye_values, candStatus_values):\n",
        "            self.x_values = x_values\n",
        "            self.y_values = y_values\n",
        "            self.chi2_values = chi2_values\n",
        "            self.q_values = q_values\n",
        "            self.xe_values = xe_values\n",
        "            self.ye_values = ye_values\n",
        "            self.candStatus_values = candStatus_values\n",
        "\n",
        "\n",
        "    class ParticleInfo: # p\n",
        "        def __init__(self,  momentum, refractiveIndex, xRad, yRad, xMIP, yMIP, thetaP, phiP, mCluCharge, mCluSize, none_candidates, pion_candidates, kaon_candidates, proton_candidates, mTrackPdg):\n",
        "            self.momentum = momentum # this dhould be with\n",
        "            self.refractiveIndex = refractiveIndex # with\n",
        "            self.xRad = xRad # with\n",
        "            self.yRad = yRad # with\n",
        "\n",
        "            self.xMIP = xMIP # with\n",
        "            self.yMIP = yMIP # with\n",
        "\n",
        "            self.mCluCharge = mCluCharge # with\n",
        "            self.mCluSize = mCluSize # with\n",
        "\n",
        "            self.thetaP = thetaP# with\n",
        "            self.phiP = phiP# with\n",
        "            self.none_candidates = none_candidates # with the field candStatus is a int that is 0..7, please make it categorical\n",
        "            self.rad_position = [xRad, yRad]\n",
        "            self.mip_position = [xMIP, yMIP]\n",
        "\n",
        "\n",
        "            self.pion_candidates = pion_candidates # pion_candidates = [1 if (int(candStatus) & 4) == 4 else 0 for candStatus in candsCombined]\n",
        "            self.kaon_candidates = kaon_candidates # = kaon_candidates [1 if (int(candStatus) & 2) == 2 else 0 for candStatus in candsCombined]\n",
        "            self.proton_candidates = proton_candidates # = proton_candidates[1 if (int(candStatus) & 1) == 1 else 0 for candStatus in candsCombined]\n",
        "\n",
        "\n",
        "            self.mTrackPdg = mTrackPdg # with\n",
        "\n",
        "            abs_mTrackPdg = abs(self.mTrackPdg)  # Take the absolute value\n",
        "\n",
        "            # Set particleType based on absolute PDG code\n",
        "            if abs_mTrackPdg == 211:\n",
        "                self.particleType = 'pion'\n",
        "            elif abs_mTrackPdg == 321:\n",
        "                self.particleType = 'kaon'\n",
        "            elif abs_mTrackPdg == 2212:\n",
        "                self.particleType = 'proton'\n",
        "            else:\n",
        "                self.particleType = 'other'\n",
        "\n",
        "\n",
        "        @staticmethod\n",
        "        def infer_mass_category_from_ckov(momentum, refractiveIndex, ckov):\n",
        "            mass = momentum * np.sqrt((refractiveIndex * np.cos(ckov))**2 - 1)\n",
        "\n",
        "            mass_category = \"unknown\"\n",
        "            if abs(mass - Constants.PION_MASS) < 1e-4:\n",
        "                mass_category = \"pion\"\n",
        "            elif abs(mass - Constants.KAON_MASS) < 1e-4:\n",
        "                mass_category = \"kaon\"\n",
        "            elif abs(mass - Constants.PROTON_MASS) < 1e-4:\n",
        "                mass_category = \"proton\"\n",
        "            if print_vals:\n",
        "              print(f\"\\ninfer_mass_category_from_ckov :  momentum = {momentum}|  mass_calc = {mass} |  mass_category={mass_category} | refractiveIndex = {refractiveIndex} | ckov = {ckov}\")\n",
        "            return mass_category\n",
        "\n",
        "        @staticmethod\n",
        "        def infer_mass_category(mass):\n",
        "            if abs(mass - Constants.PION_MASS) < 1e-6:\n",
        "                return \"pion\"\n",
        "            elif abs(mass - Constants.KAON_MASS) < 1e-6:\n",
        "                return \"kaon\"\n",
        "            elif abs(mass - Constants.PROTON_MASS) < 1e-6:\n",
        "                return \"proton\"\n",
        "            else:\n",
        "                return \"unknown\"\n",
        "\n",
        "        def __str__(self):\n",
        "            if print_vals:\n",
        "              return (f\"ParticleInfo(momentum={self.momentum} | mass={self.mass} |  mass_category={self.mass_category} | \"\n",
        "                      f\"refractiveIndex={self.refractiveIndex} | ckov={self.ckov} | rad_position={len(self.rad_position)}, \"\n",
        "                      f\"mip_position={self.mip_position})\")\n",
        "\n",
        "    #  ''' def calculate_distances_to_mip(self):\n",
        "    #       \"\"\"Calculate Euclidean distances from all filled bins to MIP position\"\"\"\n",
        "    #       filledBins_np = np.array(self.filledBins)\n",
        "    #       mip_position_np = np.array(self.mip_position)\n",
        "\n",
        "    #       distances = np.linalg.norm(filledBins_np - mip_position_np, axis=1)\n",
        "    #       return distances'''\n",
        "\n",
        "\n",
        "    def load_data(self, filename):\n",
        "      drive_path = '/content/drive/MyDrive/Colab Notebooks/CERN_ML/CNN_PID/'\n",
        "      file_path = os.path.join(drive_path, filename)\n",
        "\n",
        "      # Lists to store scalar and array-like attributes\n",
        "      momentum_list = []\n",
        "      refractiveIndex_list = []\n",
        "      xRad_list = []\n",
        "      yRad_list = []\n",
        "      xMIP_list = []\n",
        "      yMIP_list = []\n",
        "      thetaP_list = []\n",
        "      phiP_list = []\n",
        "      mCluCharge_list = []\n",
        "      mCluSize_list = []\n",
        "      mTrackPdg_list = []\n",
        "\n",
        "      x_values_data_list = []\n",
        "      y_values_data_list = []\n",
        "      chi2_values_data_list = []\n",
        "      q_values_data_list = []\n",
        "      xe_values_data_list = []\n",
        "      ye_values_data_list = []\n",
        "      candStatus_values_data_list = []\n",
        "\n",
        "\n",
        "      max_length_nested = 0\n",
        "      with h5py.File(file_path, 'r') as file:\n",
        "          for i, group_name in enumerate(file):\n",
        "              group = file[group_name]\n",
        "\n",
        "              # Store scalar attributes into lists\n",
        "              momentum_list.append(group.attrs['Momentum'])\n",
        "              refractiveIndex_list.append(group.attrs['RefractiveIndex'])\n",
        "              xRad_list.append(group.attrs['xRad'])\n",
        "              yRad_list.append(group.attrs['yRad'])\n",
        "              xMIP_list.append(group.attrs['xMip'])\n",
        "              yMIP_list.append(group.attrs['yMip'])\n",
        "              thetaP_list.append(group.attrs['ThetaP'])\n",
        "              phiP_list.append(group.attrs['PhiP'])\n",
        "              mCluCharge_list.append(group.attrs['CluCharge'])\n",
        "              mCluSize_list.append(group.attrs['CluSize'])\n",
        "              mTrackPdg_list.append(group.attrs['TrackPdg'])\n",
        "\n",
        "\n",
        "\n",
        "              # Store array-like attributes into lists\n",
        "              x_values_data_list.append(group['x_values'][...])\n",
        "              y_values_data_list.append(group['y_values'][...])\n",
        "              chi2_values_data_list.append(group['chi2_values'][...])\n",
        "              q_values_data_list.append(group['q_values'][...])\n",
        "              xe_values_data_list.append(group['xe_values'][...])\n",
        "              ye_values_data_list.append(group['ye_values'][...])\n",
        "              candStatus_values_data_list.append(group['candStatus_values'][...])\n",
        "\n",
        "\n",
        "              # cehck the length of current entry inn   ye_values_data_list and update max_length_nested if logner\n",
        "              current_length = len(group['ye_values'][...])\n",
        "              if current_length > max_length_nested:\n",
        "                  max_length_nested = current_length\n",
        "                  print(f\" i {i} max_length_nested {max_length_nested}\")\n",
        "\n",
        "      # Perform your vectorized operations here\n",
        "      # ...\n",
        "\n",
        "      # Create ParticleInfo objects\n",
        "\n",
        "      # Assuming that classify_candidates_with_pad_sequences can work on lists\n",
        "      # of data arrays, and it pads them to a uniform size\n",
        "      pion_candidates, kaon_candidates, proton_candidates, non_candidates, cand_combined = classify_candidates_with_pad_sequences(\n",
        "          x_values_data_list, y_values_data_list, chi2_values_data_list,\n",
        "          q_values_data_list, xe_values_data_list, ye_values_data_list, candStatus_values_data_list,\n",
        "          max_length_nested\n",
        "      )\n",
        "\n",
        "      particle_vector = [None] * len(momentum_list)\n",
        "      for i in range(len(momentum_list)):\n",
        "          particle_info = ParticleDataUtils.ParticleInfo(\n",
        "              momentum_list[i], refractiveIndex_list[i], xRad_list[i], yRad_list[i],\n",
        "              xMIP_list[i], yMIP_list[i], thetaP_list[i], phiP_list[i],\n",
        "              mCluCharge_list[i], mCluSize_list[i], non_candidates[i], pion_candidates[i],\n",
        "              kaon_candidates[i], proton_candidates[i], mTrackPdg_list[i]\n",
        "          )\n",
        "          particle_vector[i] = particle_info\n",
        "          self.particle_vector.append(particle_info)\n",
        "\n",
        "\n",
        "\n",
        "    def process_data(self, particle_vector, percentage):\n",
        "\n",
        "        # Calculate the number of particles based on the percentage\n",
        "        num_particles = int(len(self.particle_vector) * (percentage / 100.0))\n",
        "\n",
        "        # Slice the particle_vector to the desired percentage\n",
        "        particle_vector = self.particle_vector[:num_particles]\n",
        "        return particle_vector\n",
        "\n",
        "\n",
        "    # TODO: add more scalers here!\n",
        "    def create_scalar_scaler(self, feature):\n",
        "        if feature in [\"momentum\", \"refractiveIndex\", \"phiP\", \"thetaP\"]:\n",
        "            values = np.array([getattr(info, feature) for info in self.particle_info]).reshape(-1, 1)\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid feature: {feature}\")\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        scaled_values = scaler.fit_transform(values)\n",
        "        stats = {\n",
        "            \"mean\": scaler.mean_[0],\n",
        "            \"std\": scaler.scale_[0]\n",
        "        }\n",
        "        return scaler, stats\n",
        "\n",
        "\n",
        "\n",
        "    def create_2D_scaler(self, feature):\n",
        "        if feature in [\"mip_position\", \"rad_position\"]:\n",
        "            # Get the original 2D array\n",
        "            values = np.array([getattr(info, feature) for info in self.particle_info])\n",
        "            # Flatten the array before fitting the scaler\n",
        "            flat_values = values.flatten().reshape(-1, 1)\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid feature: {feature}\")\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        # Fit the scaler on the flattened array\n",
        "        scaler.fit(flat_values)\n",
        "        # Transform the original 2D array using the fitted scaler\n",
        "        scaled_values = scaler.transform(values.reshape(-1, 1)).reshape(values.shape)\n",
        "        stats = {\n",
        "            \"mean\": scaler.mean_[0],\n",
        "            \"std\": scaler.scale_[0]\n",
        "        }\n",
        "        return scaler, stats\n",
        "\n",
        "\n",
        "\n",
        "    def create_vector_scaler(self, feature):\n",
        "        if feature in [\"pion_candidates\", \"kaon_candidates\", \"proton_candidates\"]:\n",
        "            # Get the original 3D array of 2D points\n",
        "            # Assume each element in `info.feature` is a 2D point (e.g., [x, y])\n",
        "            values = np.array([getattr(info, feature) for info in self.particle_info])\n",
        "            # Flatten the array to 1D before fitting the scaler\n",
        "            flat_values = values.reshape(-1, 1)\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid feature: {feature}\")\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        # Fit the scaler on the flattened array\n",
        "        scaler.fit(flat_values)\n",
        "        # Transform the flattened array using the fitted scaler and then reshape back to original 3D\n",
        "        scaled_values = scaler.transform(flat_values).reshape(values.shape)\n",
        "        stats = {\n",
        "            \"mean\": scaler.mean_[0],\n",
        "            \"std\": scaler.scale_[0]\n",
        "        }\n",
        "        return scaler, stats\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# TODO : denne skal mulgiens fjernes helt ?\n",
        "# create a map, the resolution is the \"inverse\"\n",
        "def create_map(filledBins=None, resolution=4):\n",
        "    # Add an offset to your map shape calculation to handle edge cases\n",
        "    offset = 0\n",
        "    map_shape = (int(144 * resolution + offset), int(160 * resolution + offset))\n",
        "    map_data = np.zeros(map_shape, dtype=np.int32)\n",
        "    if filledBins is not None:\n",
        "        filledBins_np = np.array(filledBins)\n",
        "        indices = (filledBins_np * resolution).astype(int)\n",
        "        #print(f\"create_map : indices shape : {np.array(indices).shape}\")\n",
        "\n",
        "        map_data[indices[:, 1], indices[:, 0]] = 1\n",
        "\n",
        "        ind = np.where(map_data == 1)\n",
        "        #print(f\"create_map : ind shape : {np.array(ind).shape}\")\n",
        "    return map_data\n"
      ],
      "metadata": {
        "id": "es3UneK1Nmof",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b64d46b8-62d2-4f9d-ab43-4985f0026fa5"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pN8kSaAzUDxj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "8U_5qogtNs15"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "IjrxNzz0LMqU"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MassClassifier:\n",
        "  def __init__(self, percentage_to_read = 10, resolution = 4):\n",
        "      self.model = None\n",
        "      self.utils = None\n",
        "      self.percentage_to_read = percentage_to_read\n",
        "      self.resolution = resolution\n",
        "      self.particle_vector = None\n",
        "\n",
        "\n",
        "  def load_data(self, filename):\n",
        "\n",
        "      print(f\"MassClassifier __ load_data\")\n",
        "\n",
        "      self.utils = ParticleDataUtils(filename, percentage_to_read = self.percentage_to_read) # specify percentage of particles to read..\n",
        "      # when init on ParticleDataUtils is called, it calls its member function function         self.particle_vector = self.load_data(filename)\n",
        "      if  self.particle_vector is None :\n",
        "        print(f\"self.particle_vector is None\")\n",
        "\n",
        "        self.particle_vector = self.utils.particle_vector\n",
        "      else :\n",
        "        print(f\"self.particle_vector is not None\")\n",
        "        self.particle_vector.extend(self.utils.particle_vector)\n",
        "\n",
        "      print(f\"Number of particles: {len(self.particle_vector)}\")\n",
        "\n",
        "  def preprocess_data(self):\n",
        "      particle_info = self.utils.particle_info\n",
        "\n",
        "\n",
        "\n",
        "      from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "      # Convert pion_candidates, kaon_candidates, proton_candidates to lists\n",
        "      pion_candidates_list =[info.pion_candidates for info in particle_info]\n",
        "      kaon_candidates_list = ([info.kaon_candidates for info in particle_info])\n",
        "      proton_candidates_list = ([info.proton_candidates for info in particle_info])\n",
        "\n",
        "      #plot_function(X_train_pion_candidates, X_train_kaon_candidates, X_train_proton_candidates, X_train_mip_position, X_train_rad_position, X_train_phi, X_train_theta)\n",
        "      print(\"Fields in the first vector of X_train:\")\n",
        "      print(\"pion_candidates_list shape:\",  np.array(pion_candidates_list).shape)\n",
        "      print(\"kaon_candidates_list shape:\",  np.array(kaon_candidates_list).shape)\n",
        "      print(\"proton_candidates_list shape:\",  np.array(proton_candidates_list).shape)\n",
        "\n",
        "\n",
        "      # Pad the lists to the longest vector per dimension\n",
        "      X_pion_candidates =  np.array(pion_candidates_list)#pad_sequences(pion_candidates_list, padding='post')\n",
        "      X_kaon_candidates =  np.array(kaon_candidates_list)#pad_sequences(kaon_candidates_list, padding='post')\n",
        "      X_proton_candidates = np.array(proton_candidates_list)# pad_sequences(proton_candidates_list, padding='post')\n",
        "\n",
        "\n",
        "      print(f\"PADDED \\n:\")\n",
        "      print(\"X_pion_candidates shape:\", X_pion_candidates.shape)\n",
        "      print(\"X_kaon_candidates shape:\", X_kaon_candidates.shape)\n",
        "      print(\"X_proton_candidates shape:\", X_proton_candidates.shape)\n",
        "\n",
        "\n",
        "      # scalars\n",
        "      X_momentum = np.array([info.momentum for info in particle_info])#.reshape(-1, 32, 32, 1)\n",
        "      X_refractive_index = np.array([info.refractiveIndex for info in particle_info])#.reshape(-1, 32, 32, 1)\n",
        "\n",
        "      # this should not be included? :\n",
        "      print(f\"PADDED \\n:\")\n",
        "\n",
        "      X_phi = np.array([particle.phiP for particle in particle_info])      # new\n",
        "      X_theta = np.array([particle.thetaP for particle in particle_info])  # new\n",
        "\n",
        "      X_phi = np.array([particle.phiP for particle in particle_info])      # new\n",
        "      X_theta = np.array([particle.thetaP for particle in particle_info])  # new\n",
        "\n",
        "\n",
        "\n",
        "      # x,y pairs (2,1) :\n",
        "      X_mip_position = np.array([info.mip_position for info in particle_info]) # was already\n",
        "      X_rad_position = np.array([info.rad_position for info in particle_info]) # new\n",
        "\n",
        "      X_mCluCharge = np.array([info.mCluCharge for info in particle_info]) # was already\n",
        "      X_mCluSize = np.array([info.mCluSize for info in particle_info]) # new\n",
        "\n",
        "      # Normalize the inputs NB commented out scaling !!!\n",
        "      # X_pion_candidates = self.utils.pion_scaler.transform(X_pion_candidates.reshape(-1, 1))#.reshape(-1, 32, 32, 1)\n",
        "      # X_kaon_candidates = self.utils.kaon_scaler.transform(X_kaon_candidates.reshape(-1, 1))#.reshape(-1, 32, 32, 1)\n",
        "      # X_proton_candidates = self.utils.proton_scaler.transform(X_proton_candidates.reshape(-1, 1))#.reshape(-1, 32, 32, 1)\n",
        "\n",
        "\n",
        "      # X_momentum = self.utils.momentum_scaler.transform(X_momentum.reshape(-1, 1))#.reshape(-1, 32, 32, 1)\n",
        "      # X_phi = self.utils.phi_scaler.transform(X_phi.reshape(-1, 1))#.reshape(-1, 32, 32, 1)\n",
        "      # X_theta = self.utils.theta_scaler.transform(X_theta.reshape(-1, 1))#.reshape(-1, 32, 32, 1)\n",
        "      # X_energy = self.utils.energy_scaler.transform(X_energy.reshape(-1, 1))#.reshape(-1, 32, 32, 1)\n",
        "      # X_refractive_index = self.utils.refractive_index_scaler.transform(X_refractive_index.reshape(-1, 1))#.reshape(-1, 32, 32, 1)\n",
        "\n",
        "      #X_mip_position = self.utils.mip_scaler.transform(X_mip_position.reshape(-1, 1)).reshape(X_mip_position.shape)\n",
        "      #X_rad_position = self.utils.rad_scaler.transform(X_rad_position.reshape(-1, 1)).reshape(X_rad_position.shape)\n",
        "\n",
        "      len_proton = np.asarray(X_proton_candidates, dtype = object).shape[1]\n",
        "\n",
        "\n",
        "      # Prepare the outputs\n",
        "      y = np.array([info.particleType  for info in particle_info])\n",
        "      # if pdg == 211 -- pion 321 kaon 2212 proton else \"other\"\n",
        "\n",
        "\n",
        "      # Convert the outputs to one-hot encoded vectors\n",
        "      lb = LabelBinarizer()\n",
        "      y = lb.fit_transform(y)\n",
        "\n",
        "      # Split the data into train and test sets\n",
        "      map_size = 40\n",
        "\n",
        "      # ef :TODO fix this back again\n",
        "      X_map_pion = X_pion_candidates#extract_neighborhood_map(candidate_positions = X_pion_candidates, mip_positions = X_mip_position, neighborhood_size = 40, map_size = map_size)\n",
        "      X_map_kaon = X_kaon_candidates#extract_neighborhood_map(candidate_positions = X_kaon_candidates, mip_positions = X_mip_position, neighborhood_size = 40, map_size = map_size)\n",
        "      X_map_proton = X_proton_candidates# extract_neighborhood_map(candidate_positions = X_proton_candidates, mip_positions = X_mip_position, neighborhood_size = 40, map_size = map_size)\n",
        "\n",
        "\n",
        "\n",
        "      X_train_mCluCharge, X_test_mCluCharge,  X_train_mCluSize, X_test_mCluSize = train_test_split(X_mCluCharge, X_mCluSize, random_state=42)\n",
        "\n",
        "\n",
        "      X_train_pion_candidates, X_test_pion_candidates, X_train_kaon_candidates, X_test_kaon_candidates, \\\n",
        "      X_train_proton_candidates, X_test_proton_candidates, X_train_momentum, X_test_momentum, \\\n",
        "      X_train_refractive_index, X_test_refractive_index, X_train_phi, X_test_phi, X_train_theta, \\\n",
        "      X_test_theta, X_train_mip_position, X_test_mip_position, \\\n",
        "      X_train_rad_position, X_test_rad_position, \\\n",
        "      X_train_map_pion, X_test_map_pion, X_train_map_kaon, X_test_map_kaon, X_train_map_proton, X_test_map_proton, \\\n",
        "      y_train, y_test, = \\\n",
        "      train_test_split(X_pion_candidates, X_kaon_candidates, X_proton_candidates, X_momentum, X_refractive_index, \\\n",
        "      X_phi, X_theta, X_mip_position, X_rad_position, X_map_pion, X_map_kaon, X_map_proton, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "      # X_train_dist2mip, X_test_dist2mip, X_train_map, X_test_map, X_train_momentum, X_test_momentum, X_train_refractive_index, X_test_refractive_index, \\\n",
        "      #     X_train_ckov, X_test_ckov, X_train_mip_position, X_test_mip_position, y_train, y_test = \\\n",
        "      #     train_test_split(X_dist2mip, X_map, X_momentum, X_refractive_index, X_ckov, X_mip_position, y, test_size=0.2, random_state=42)\n",
        "\n",
        "      # # Suppose X_photon_ckov_segmented is your data\n",
        "      # X_train_photon_ckov_segmented, X_test_photon_ckov_segmented = \\\n",
        "      #     train_test_split(X_photon_ckov_segmented, test_size=0.2, random_state=42)\n",
        "\n",
        "      X_train = {\n",
        "          \"X_train_momentum\": X_train_momentum,\n",
        "          \"X_train_refractive_index\": X_train_refractive_index,\n",
        "          \"X_train_phi\": X_train_phi,\n",
        "          \"X_train_theta\": X_train_theta,\n",
        "          \"X_train_mip_position\": X_train_mip_position,\n",
        "          \"X_train_rad_position\": X_train_rad_position,\n",
        "          \"X_train_pion_candidates\": X_train_pion_candidates,\n",
        "          \"X_train_kaon_candidates\": X_train_kaon_candidates,\n",
        "          \"X_train_proton_candidates\": X_train_proton_candidates,\n",
        "          \"X_train_map_pion\": X_train_map_pion,\n",
        "          \"X_train_map_kaon\": X_train_map_kaon,\n",
        "          \"X_train_map_proton\": X_train_map_proton,\n",
        "          \"X_train_mCluCharge\": X_train_mCluCharge,  # New addition\n",
        "          \"X_train_mCluSize\": X_train_mCluSize       # New addition\n",
        "      }\n",
        "\n",
        "      X_test = {\n",
        "          \"X_test_momentum\": X_test_momentum,\n",
        "          \"X_test_refractive_index\": X_test_refractive_index,\n",
        "          \"X_test_phi\": X_test_phi,\n",
        "          \"X_test_theta\": X_test_theta,\n",
        "          \"X_test_mip_position\": X_test_mip_position,\n",
        "          \"X_test_rad_position\": X_test_rad_position,\n",
        "          \"X_test_pion_candidates\": X_test_pion_candidates,\n",
        "          \"X_test_kaon_candidates\": X_test_kaon_candidates,\n",
        "          \"X_test_proton_candidates\": X_test_proton_candidates,\n",
        "          \"X_test_map_pion\": X_test_map_pion,\n",
        "          \"X_test_map_kaon\": X_test_map_kaon,\n",
        "          \"X_test_map_proton\": X_test_map_proton,\n",
        "          \"X_test_mCluCharge\": X_test_mCluCharge,  # New addition\n",
        "          \"X_test_mCluSize\": X_test_mCluSize        # New addition\n",
        "      }\n",
        "      print(f\"  return (X_train, X_test, y_train, y_test) \\n:\")\n",
        "\n",
        "      return (X_train, X_test, y_train, y_test)\n",
        "\n",
        "\n",
        "\n",
        "  def plot_hist(self, input_sequence_length=None, X_train=None, X_test=None, y_train=None, y_test=None):\n",
        "      # Extract the new data fields\n",
        "      X_train_pion_candidates = X_train[\"X_train_pion_candidates\"]\n",
        "      X_train_kaon_candidates = X_train[\"X_train_kaon_candidates\"]\n",
        "      X_train_proton_candidates = X_train[\"X_train_proton_candidates\"]\n",
        "      X_train_momentum = X_train[\"X_train_momentum\"]\n",
        "      X_train_refractive_index = X_train[\"X_train_refractive_index\"]\n",
        "      X_train_phi = X_train[\"X_train_phi\"]\n",
        "      X_train_theta = X_train[\"X_train_theta\"]\n",
        "      X_train_mip_position = X_train[\"X_train_mip_position\"]\n",
        "      X_train_rad_position = X_train[\"X_train_rad_position\"]\n",
        "\n",
        "      X_test_pion_candidates = X_test[\"X_test_pion_candidates\"]\n",
        "      X_test_kaon_candidates = X_test[\"X_test_kaon_candidates\"]\n",
        "      X_test_proton_candidates = X_test[\"X_test_proton_candidates\"]\n",
        "      X_test_momentum = X_test[\"X_test_momentum\"]\n",
        "      X_test_refractive_index = X_test[\"X_test_refractive_index\"]\n",
        "      X_test_phi = X_test[\"X_test_phi\"]\n",
        "      X_test_theta = X_test[\"X_test_theta\"]\n",
        "      X_test_mip_position = X_test[\"X_test_mip_position\"]\n",
        "      X_test_rad_position = X_test[\"X_test_rad_position\"]\n",
        "\n",
        "      # Convert the candidates data to numpy arrays\n",
        "      pion_candidates_train = np.array(X_train_pion_candidates).astype(np.float32)\n",
        "      kaon_candidates_train = np.array(X_train_kaon_candidates).astype(np.float32)\n",
        "      proton_candidates_train = np.array(X_train_proton_candidates).astype(np.float32)\n",
        "\n",
        "      pion_candidates_test = np.array(X_test_pion_candidates).astype(np.float32)\n",
        "      kaon_candidates_test = np.array(X_test_kaon_candidates).astype(np.float32)\n",
        "      proton_candidates_test = np.array(X_test_proton_candidates).astype(np.float32)\n",
        "\n",
        "      # Create subplots for the histograms and 2D map\n",
        "      fig, axs = plt.subplots(2, 5, figsize=(20, 10))\n",
        "      fig2, axs2 = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "      # For train data\n",
        "      train_variables = [X_train_refractive_index, X_train_momentum, X_train_mip_position, X_train_phi, X_train_theta]\n",
        "      train_labels = ['Train Refractive Index', 'Train Momentum', 'Train MIP Position', 'Train phi', 'Train theta']\n",
        "\n",
        "      X_train_mip_position = (np.asarray(X_train_mip_position)).reshape(len(X_train_mip_position), 2)\n",
        "      print(f\"X_train_mip_position shape = {X_train_mip_position.shape}\")\n",
        "      for i, variable in enumerate(train_variables):\n",
        "          if i < 2 or i == 3:  # For 1D data\n",
        "              axs[0, i].hist(variable, edgecolor='black')\n",
        "              axs[0, i].set_title(train_labels[i])\n",
        "          elif i == 2:  # For 2D data\n",
        "              axs2[0].scatter(X_train_mip_position[:, 0], X_train_mip_position[:, 1], marker='o', s=10)\n",
        "              axs2[0].set_title('2D Map: MIP Position')\n",
        "              axs2[0].set_xlabel('X position')\n",
        "              axs2[0].set_ylabel('Y position')\n",
        "          else:  # For 1D data\n",
        "              axs[0, i-1].hist(variable, edgecolor='black')\n",
        "              axs[0, i-1].set_title(train_labels[i])\n",
        "\n",
        "      # For test data\n",
        "      test_variables = [X_test_refractive_index, X_test_momentum, X_test_mip_position, X_test_phi, X_test_theta]\n",
        "      test_labels = ['Test Refractive Index', 'Test Momentum', 'Test MIP Position', 'Test phiP', 'Test thetaP']\n",
        "\n",
        "      X_test_mip_position = (np.asarray(X_test_mip_position)).reshape(len(X_test_mip_position), 2)\n",
        "      print(f\"X_test_mip_position shape = {X_test_mip_position.shape}\")\n",
        "\n",
        "      for i, variable in enumerate(test_variables):\n",
        "          if i < 2 or i == 3:  # For 1D data\n",
        "              axs[1, i].hist(variable, edgecolor='black')\n",
        "              axs[1, i].set_title(test_labels[i])\n",
        "          elif i == 2:  # For 2D data\n",
        "              axs2[1].scatter(X_test_mip_position[:, 0], X_test_mip_position[:, 1], marker='o', s=10)\n",
        "              axs2[1].set_title('2D Map: MIP Position')\n",
        "              axs2[1].set_xlabel('X position')\n",
        "              axs2[1].set_ylabel('Y position')\n",
        "          else:  # For 1D data\n",
        "              axs[1, i-1].hist(variable, edgecolor='black')\n",
        "              axs[1, i-1].set_title(test_labels[i])\n",
        "\n",
        "      plt.show()\n",
        "\n",
        "      # Plot 2D map for one instance of pion, kaon, and proton\n",
        "      pion_candidates_train = np.array(pion_candidates_train).astype(np.float32)\n",
        "      kaon_candidates_train = np.array(kaon_candidates_train).astype(np.float32)\n",
        "      proton_candidates_train = np.array(proton_candidates_train).astype(np.float32)\n",
        "\n",
        "      pion_instance = pion_candidates_train[0]\n",
        "      kaon_instance = kaon_candidates_train[0]\n",
        "      proton_instance = proton_candidates_train[0]\n",
        "\n",
        "\n",
        "      # ef :TODO fix this back again\n",
        "      # axs2[2].scatter(pion_instance[:, 0], pion_instance[:, 1], c='r', marker='o', label='Pion', s=20)\n",
        "      # axs2[2].scatter(kaon_instance[:, 0], kaon_instance[:, 1], c='g', marker='s', label='Kaon', s=20)\n",
        "      # axs2[2].scatter(proton_instance[:, 0], proton_instance[:, 1], c='b', marker='^', label='Proton', s=20)\n",
        "      # axs2[2].set_title('2D Map: Particle Instances')\n",
        "      # axs2[2].set_xlabel('X position')\n",
        "      # axs2[2].set_ylabel('Y position')\n",
        "      # axs2[2].legend()\n",
        "\n",
        "\n",
        "  # TODO: to this to the new data-fields :\n",
        "  def plot_hist2(self, input_sequence_length = None, X_train = None, X_test = None, y_train = None, y_test = None):\n",
        "      #map_shape = (None, None, 1)  # Variable shape for the map input\n",
        "      #X_train_photon_ckov_segmented shape : (1955, 3, 74)\n",
        "\n",
        "      # create a map with resolution to be chosen, iterate over teh filledBins vector\n",
        "      #X_map = np.array([create_map(filledBins = info.filledBins, resolution = self.resolution) for info in particle_info])\n",
        "      #filled_bins_array = np.array([info.filledBins for info in particle_info], dtype=object)\n",
        "\n",
        "      X_train_pion_candidates = X_train[\"X_train_pion_candidates\"]\n",
        "      X_train_kaon_candidates = X_train[\"X_train_kaon_candidates\"]\n",
        "      X_train_proton_candidates = X_train[\"X_train_proton_candidates\"]\n",
        "      X_train_momentum = X_train[\"X_train_momentum\"]\n",
        "      X_train_refractive_index = X_train[\"X_train_refractive_index\"]\n",
        "      X_train_phi = X_train[\"X_train_phi\"]\n",
        "      X_train_theta = X_train[\"X_train_theta\"]\n",
        "      X_train_mip_position = X_train[\"X_train_mip_position\"]\n",
        "      X_train_rad_position = X_train[\"X_train_rad_position\"]\n",
        "\n",
        "      X_test_pion_candidates = X_test[\"X_test_pion_candidates\"]\n",
        "      X_test_kaon_candidates = X_test[\"X_test_kaon_candidates\"]\n",
        "      X_test_proton_candidates = X_test[\"X_test_proton_candidates\"]\n",
        "      X_test_momentum = X_test[\"X_test_momentum\"]\n",
        "      X_test_refractive_index = X_test[\"X_test_refractive_index\"]\n",
        "      X_test_phi = X_test[\"X_test_phi\"]\n",
        "      X_test_theta = X_test[\"X_test_theta\"]\n",
        "      X_test_mip_position = X_test[\"X_test_mip_position\"]\n",
        "      X_test_rad_position = X_test[\"X_test_rad_position\"]\n",
        "\n",
        "      # Plot 2D map for one instance of pion, kaon, and proton\n",
        "      pion_candidates_train = np.array(X_train_pion_candidates).astype(np.float32)\n",
        "      kaon_candidates_train = np.array(X_train_kaon_candidates).astype(np.float32)\n",
        "      proton_candidates_train = np.array(X_train_proton_candidates).astype(np.float32)\n",
        "\n",
        "\n",
        "      # Plot 2D map for one instance of pion, kaon, and proton\n",
        "      pion_candidates_test = np.array(X_test_pion_candidates).astype(np.float32)\n",
        "      kaon_candidates_test = np.array(X_test_kaon_candidates).astype(np.float32)\n",
        "      proton_candidates_test = np.array(X_test_proton_candidates).astype(np.float32)\n",
        "\n",
        "\n",
        "\n",
        "      fig, axs = plt.subplots(2, 4, figsize=(20, 10))\n",
        "\n",
        "      train_variables = [X_train_refractive_index, X_train_momentum, X_train_mip_position, X_train_phi, X_train_theta]\n",
        "      train_labels = ['Train Refractive Index', 'Train Momentum', 'Train MIP Position', 'Train phi', 'Train theta']\n",
        "\n",
        "      X_train_mip_position = (np.asarray(X_train_mip_position)).reshape(len(X_train_mip_position), 2)\n",
        "      print(f\"X_train_mip_position shape = {X_train_mip_position.shape}\")\n",
        "      for i, variable in enumerate(train_variables):\n",
        "          if i < 3:  # variable.ndim == 1: # For 1D data\n",
        "              axs[0, i].hist(variable, edgecolor='black')\n",
        "              axs[0, i].set_title(train_labels[i])\n",
        "          elif i == 3:  # For 2D data\n",
        "              axs2[0].scatter(X_train_mip_position[:, 0], X_train_mip_position[:, 1], marker='o', s=10)\n",
        "              axs2[0].set_title('2D Map: MIP Position')\n",
        "              axs2[0].set_xlabel('X position')\n",
        "              axs2[0].set_ylabel('Y position')\n",
        "          else:  # For 1D data\n",
        "              axs[0, i].hist(variable, edgecolor='black')\n",
        "              axs[0, i].set_title(train_labels[i])\n",
        "\n",
        "      # For test data\n",
        "      test_variables = [X_test_refractive_index, X_test_momentum, X_test_mip_position, X_test_phi, X_test_theta]\n",
        "      test_labels = ['Test Refractive Index', 'Test Momentum','Test MIP Position', 'Test phiP', 'Test thetaP']\n",
        "\n",
        "\n",
        "      X_test_mip_position = (np.asarray(X_test_mip_position)).reshape(len(X_test_mip_position), 2)\n",
        "      print(f\"X_test_mip_position shape = {X_test_mip_position.shape}\")\n",
        "\n",
        "      for i, variable in enumerate(test_variables):\n",
        "          if i < 3:  # variable.ndim == 1: # For 1D data\n",
        "              axs[1, i].hist(variable, edgecolor='black')\n",
        "              axs[1, i].set_title(test_labels[i])\n",
        "          elif i == 3:  # For 2D data\n",
        "              axs2[1].scatter(X_test_mip_position[:, 0], X_test_mip_position[:, 1], marker='o', s=10)\n",
        "              axs2[1].set_title('2D Map: MIP Position')\n",
        "              axs2[1].set_xlabel('X position')\n",
        "              axs2[1].set_ylabel('Y position')\n",
        "          else:  # For 1D data\n",
        "              axs[1, i].hist(variable, edgecolor='black')\n",
        "              axs[1, i].set_title(test_labels[i])\n",
        "\n",
        "      plt.show()\n",
        "\n",
        "      len_pion = np.asarray(pion_candidates_test, dtype = object).shape[1]\n",
        "      len_kaon = np.asarray(kaon_candidates_test, dtype = object).shape[1]\n",
        "      len_proton = np.asarray(proton_candidates_train, dtype = object).shape[1]\n",
        "\n",
        "      # print(f\" pion_candidates_test lastdim = {len_pion}\")\n",
        "      # print(f\" kaon_candidates_train lastdim = {len_kaon}\")\n",
        "      # print(f\" proton_candidates_train lastdim = {len_proton}\")\n",
        "\n",
        "      max_value = np.max(pion_candidates_test)\n",
        "\n",
        "      # Find the minimum value across all dimensions\n",
        "      min_value = np.min(pion_candidates_test)\n",
        "\n",
        "      # print(\"pion_candidates_test Maximum value:\", max_value)\n",
        "      # print(\"pion_candidates_test Minimum value:\", min_value)\n",
        "\n",
        "      # print(f\" pion_candidates_test lastdim = {np.asarray(pion_candidates_test, dtype = object).shape[1]}\")\n",
        "      # print(f\" kaon_candidates_test lastdim = {np.asarray(kaon_candidates_test, dtype = object).shape[1]}\")\n",
        "      # print(f\" proton_candidates_test lastdim = {np.asarray(proton_candidates_test, dtype = object).shape[1]}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      pion_candidates_train = np.array(pion_candidates_train).astype(np.float32)\n",
        "      kaon_candidates_train = np.array(kaon_candidates_train).astype(np.float32)\n",
        "      proton_candidates_train = np.array(proton_candidates_train).astype(np.float32)\n",
        "\n",
        "\n",
        "      x_train = np.asarray([pion_candidates_train, kaon_candidates_train, proton_candidates_train], dtype = \"object\")\n",
        "      x_test = np.asarray([pion_candidates_test, kaon_candidates_test, proton_candidates_test], dtype = \"object\")\n",
        "\n",
        "\n",
        "      #x_train shape = (3, 1955, 74) pion_candidates_test shape = (489, 74) proton_candidates_train_tf shape = (1955, 74)\n",
        "      print(f\"x_train shape = {np.asarray(x_train, dtype = object).shape}\")\n",
        "      print(f\"pion_candidates_test shape = {np.asarray(pion_candidates_test, dtype = object).shape}\")\n",
        "\n",
        "\n",
        "\n",
        "      fig2, axs2 = plt.subplots(2, 3, figsize=(18, 12))\n",
        "      strings_desc = [\"pion\", \"kaon\", \"proton\"]\n",
        "      for i in range(3):\n",
        "          axs2[0, i].hist(x_train[i, :, :],  bins=np.arange(0.05, 0.8, 0.01), edgecolor='black')\n",
        "          axs2[0, i].set_title(f'Histogram for Ckov Train data: {strings_desc[i]}')\n",
        "          axs2[0, i].set_xlabel('Value')\n",
        "          axs2[0, i].set_ylabel('Frequency')\n",
        "\n",
        "          axs2[1, i].hist(x_test[i, :, :],  bins=np.arange(0.05, 0.8, 0.01), edgecolor='black')\n",
        "          axs2[1, i].set_title(f'Histogram for Ckov Test data: {strings_desc[i]}')\n",
        "          axs2[1, i].set_xlabel('Value')\n",
        "          axs2[1, i].set_ylabel('Frequency')\n",
        "\n",
        "      plt.show()\n",
        "\n",
        "      x_train_tf = np.asarray([pion_candidates_train, kaon_candidates_train, proton_candidates_train], dtype = object)\n",
        "      x_test_tf = np.asarray([pion_candidates_test, kaon_candidates_test, proton_candidates_test], dtype = object)\n",
        "\n",
        "      print(f\"x_train_tf shape = {x_train_tf.shape}\")\n",
        "\n",
        "      fig3, axs3 = plt.subplots(2, 3, figsize=(18, 12))\n",
        "      for i in range(3):\n",
        "          axs3[0, i].hist(x_train_tf[i, :, :],  bins=np.arange(0.1, 0.8, 0.01), edgecolor='black')\n",
        "          axs3[0, i].set_title(f'Histogram for TF : Ckov Train data: {strings_desc[i]}')\n",
        "          axs3[0, i].set_xlabel('Value')\n",
        "          axs3[0, i].set_ylabel('Frequency')\n",
        "\n",
        "          axs3[1, i].hist(x_test_tf[i, :, :],  bins=np.arange(0.1, 0.8, 0.01), edgecolor='black')\n",
        "          axs3[1, i].set_title(f'Histogram for TF :Ckov Test data: {strings_desc[i]}')\n",
        "          axs3[1, i].set_xlabel('Value')\n",
        "          axs3[1, i].set_ylabel('Frequency')\n",
        "\n",
        "      plt.show()\n",
        "\n",
        "      # Plot 2D map for one instance of pion, kaon, and proton\n",
        "      pion_candidates_train = np.array(pion_candidates_train).astype(np.float32)\n",
        "      kaon_candidates_train = np.array(kaon_candidates_train).astype(np.float32)\n",
        "      proton_candidates_train = np.array(proton_candidates_train).astype(np.float32)\n",
        "\n",
        "      pion_instance = pion_candidates_train[0]\n",
        "      kaon_instance = kaon_candidates_train[0]\n",
        "      proton_instance = proton_candidates_train[0]\n",
        "\n",
        "      axs2[2].scatter(pion_instance[:, 0], pion_instance[:, 1], c='r', marker='o', label='Pion', s=20)\n",
        "      axs2[2].scatter(kaon_instance[:, 0], kaon_instance[:, 1], c='g', marker='s', label='Kaon', s=20)\n",
        "      axs2[2].scatter(proton_instance[:, 0], proton_instance[:, 1], c='b', marker='^', label='Proton', s=20)\n",
        "      axs2[2].set_title('2D Map: Particle Instances')\n",
        "      axs2[2].set_xlabel('X position')\n",
        "      axs2[2].set_ylabel('Y position')\n",
        "      axs2[2].legend()\n",
        "\n",
        "\n",
        "\n",
        "  def build_model(self, input_sequence_length = None, X_train = None, X_test = None, y_train = None, y_test = None, mask = None):\n",
        "\n",
        "      #map_shape = (None, None, 1)  # Variable shape for the map input\n",
        "      #X_train_photon_ckov_segmented shape : (1955, 3, 74)\n",
        "\n",
        "      X_train_pion_candidates = X_train[\"X_train_pion_candidates\"]\n",
        "      X_train_kaon_candidates = X_train[\"X_train_kaon_candidates\"]\n",
        "      X_train_proton_candidates = X_train[\"X_train_proton_candidates\"]\n",
        "      X_train_momentum = X_train[\"X_train_momentum\"]\n",
        "      X_train_refractive_index = X_train[\"X_train_refractive_index\"]\n",
        "      X_train_phi = X_train[\"X_train_phi\"]\n",
        "      X_train_theta = X_train[\"X_train_theta\"]\n",
        "      X_train_mip_position = X_train[\"X_train_mip_position\"]\n",
        "      X_train_rad_position = X_train[\"X_train_rad_position\"]\n",
        "      X_train_map_pion = X_train[\"X_train_map_pion\"]\n",
        "      X_train_map_kaon = X_train[\"X_train_map_kaon\"]\n",
        "      X_train_map_proton = X_train[\"X_train_map_proton\"]\n",
        "\n",
        "      X_test_pion_candidates = X_test[\"X_test_pion_candidates\"]\n",
        "      X_test_kaon_candidates = X_test[\"X_test_kaon_candidates\"]\n",
        "      X_test_proton_candidates = X_test[\"X_test_proton_candidates\"]\n",
        "      X_test_momentum = X_test[\"X_test_momentum\"]\n",
        "      X_test_refractive_index = X_test[\"X_test_refractive_index\"]\n",
        "      X_test_phi = X_test[\"X_test_phi\"]\n",
        "      X_test_theta = X_test[\"X_test_theta\"]\n",
        "      X_test_mip_position = X_test[\"X_test_mip_position\"]\n",
        "      X_test_rad_position = X_test[\"X_test_rad_position\"]\n",
        "      X_test_map_pion = X_test[\"X_test_map_pion\"]\n",
        "      X_test_map_kaon = X_test[\"X_test_map_kaon\"]\n",
        "      X_test_map_proton = X_test[\"X_test_map_proton\"]\n",
        "\n",
        "      X_train_mCluCharge = X_train[\"X_train_mCluCharge\"]\n",
        "      X_train_mCluSize = X_train[\"X_train_mCluSize\"]\n",
        "      X_test_mCluCharge = X_test[\"X_test_mCluCharge\"]\n",
        "      X_test_mCluSize = X_test[\"X_test_mCluSize\"]\n",
        "\n",
        "      #plot_function(X_train_pion_candidates, X_train_kaon_candidates, X_train_proton_candidates, X_train_mip_position, X_train_rad_position, X_train_phi, X_train_theta)\n",
        "      print(\"Fields in the first vector of X_train:\")\n",
        "      print(\"X_train_pion_candidates shape:\", X_train_pion_candidates.shape)\n",
        "      print(\"X_train_kaon_candidates shape:\", X_train_kaon_candidates.shape)\n",
        "      print(\"X_train_proton_candidates shape:\", X_train_proton_candidates.shape)\n",
        "      print(\"X_train_momentum shape:\", X_train_momentum.shape)\n",
        "      print(\"X_train_refractive_index shape:\", X_train_refractive_index.shape)\n",
        "      print(\"X_train_phi shape:\", X_train_phi.shape)\n",
        "      print(\"X_train_theta shape:\", X_train_theta.shape)\n",
        "      print(\"X_train_mip_position shape:\", X_train_mip_position.shape)\n",
        "      print(\"X_train_rad_position shape:\", X_train_rad_position.shape)\n",
        "      print(\"X_train_map_pion shape:\", X_train_map_pion.shape)\n",
        "      print(\"X_train_map_kaon shape:\", X_train_map_kaon.shape)\n",
        "      print(\"X_train_map_proton shape:\", X_train_map_proton.shape)\n",
        "\n",
        "      print(\"\\nFields in the first vector of X_test:\")\n",
        "      print(\"X_test_pion_candidates shape:\", X_test_pion_candidates.shape)\n",
        "      print(\"X_test_kaon_candidates shape:\", X_test_kaon_candidates.shape)\n",
        "      print(\"X_test_proton_candidates shape:\", X_test_proton_candidates.shape)\n",
        "      print(\"X_test_momentum shape:\", X_test_momentum.shape)\n",
        "      print(\"X_test_refractive_index shape:\", X_test_refractive_index.shape)\n",
        "      print(\"X_test_phi shape:\", X_test_phi.shape)\n",
        "      print(\"X_test_theta shape:\", X_test_theta.shape)\n",
        "      print(\"X_test_mip_position shape:\", X_test_mip_position.shape)\n",
        "      print(\"X_test_rad_position shape:\", X_test_rad_position.shape)\n",
        "      print(\"X_test_map_pion shape:\", X_test_map_pion.shape)\n",
        "      print(\"X_test_map_kaon shape:\", X_test_map_kaon.shape)\n",
        "      print(\"X_test_map_proton shape:\", X_test_map_proton.shape)\n",
        "\n",
        "      # For train data\n",
        "      train_variables = [X_train_pion_candidates, X_train_kaon_candidates, X_train_proton_candidates,\n",
        "                        X_train_momentum, X_train_refractive_index, X_train_phi, X_train_theta,\n",
        "                        X_train_mip_position, X_train_rad_position,\n",
        "                         #X_train_map_pion, X_train_map_kaon, X_train_map_proton,\n",
        "                         X_train_mCluCharge, X_train_mCluSize]\n",
        "\n",
        "      train_labels = ['Train Pion Candidates', 'Train Kaon Candidates', 'Train Proton Candidates',\n",
        "                      'Train Momentum', 'Train Refractive Index', 'Train Phi', 'Train Theta',\n",
        "                      'Train MIP Position', 'Train Rad Position',\n",
        "                      #'Train Map Pion','Train Map Kaon', 'Train Map Proton',\n",
        "                      'Train mCluCharge', 'Train mCluSize']\n",
        "\n",
        "      test_variables = [X_test_pion_candidates, X_test_kaon_candidates, X_test_proton_candidates,\n",
        "                        X_test_momentum, X_test_refractive_index, X_test_phi, X_test_theta,\n",
        "                        X_test_mip_position, X_test_rad_position,\n",
        "                        #X_test_map_pion, X_test_map_kaon, X_test_map_proton,\n",
        "                        X_test_mCluCharge, X_test_mCluSize]\n",
        "\n",
        "      test_labels = ['Test Pion Candidates', 'Test Kaon Candidates', 'Test Proton Candidates',\n",
        "                    'Test Momentum', 'Test Refractive Index', 'Test Phi', 'Test Theta',\n",
        "                    'Test MIP Position', 'Test Rad Position',\n",
        "                    #'Test Map Pion','Test Map Kaon', 'Test Map Proton',\n",
        "                    'Test mCluCharge', 'Test mCluSize']\n",
        "\n",
        "\n",
        "      # TODO: evaluer hva som er best !\n",
        "      # 1. sende inn X_test_cand_status_encoded, X_test_cand_pos som det er naa\n",
        "      # 2. bare sende inn de som har status != 0\n",
        "      # 3 splitte til pion, proton, kaon candidates og sende disse inn\n",
        "\n",
        "\n",
        "      len_pion = np.asarray(X_test_pion_candidates, dtype = object).shape[1]\n",
        "      len_kaon = np.asarray(X_test_kaon_candidates, dtype = object).shape[1]\n",
        "      len_proton = np.asarray(X_test_proton_candidates, dtype = object).shape[1]\n",
        "\n",
        "\n",
        "      pion_ip = Input(shape=(len_pion, 2), name=\"pion_ip\")\n",
        "      kaon_ip = Input(shape=(len_kaon, 2), name=\"kaon_ip\")\n",
        "      proton_ip = Input(shape=(len_proton, 2), name=\"proton_ip\")\n",
        "\n",
        "\n",
        "\n",
        "      # pion_ip = Input(shape=(len_pion, 2), name=\"pion_ip\")\n",
        "      # kaon_ip = Input(shape=(len_kaon, 2), name=\"kaon_ip\")\n",
        "      # proton_ip = Input(shape=(len_proton, 2), name=\"proton_ip\")\n",
        "\n",
        "\n",
        "      pion_ip_mask = Masking(mask_value=0.)(pion_ip)\n",
        "      kaon_ip_mask = Masking(mask_value=0.)(kaon_ip)\n",
        "      proton_ip_mask = Masking(mask_value=0.)(proton_ip)\n",
        "\n",
        "      prev_pion = pion_ip_mask # NB change back to mask\n",
        "      prev_kaon = kaon_ip_mask\n",
        "      prev_proton = proton_ip_mask\n",
        "\n",
        "      # Grid search parameters\n",
        "      # Grid search parameters\n",
        "      filter_sizes = [5]  # Filter sizes to test\n",
        "      num_filters = [32]#[16, 32]  # Number of filters to test\n",
        "      strides = [(2, 2)]#[(1, 1), (2, 2)]  # Strides to test\n",
        "      pool_sizes = [(2, 2)]  # Max pooling sizes to test\n",
        "      fc1_units = [64]#, 128]  # Number of units in fc1 to test\n",
        "      fc2_units = [32]#, 32]  # Number of units in fc2 to test\n",
        "\n",
        "      dropouts = [0.3, 0.35, 0.4]\n",
        "      best_accuracy = 0\n",
        "      best_model = None\n",
        "      alphas = [0.005, 0.05, 0.1]\n",
        "\n",
        "      l1l2_weights_mip = [\n",
        "          [(0.01, 0.01, 0.01), (0.02, 0.02, 0.02), (0.03, 0.03, 0.03), (0.04, 0.04, 0.04), (0.05, 0.05, 0.05),\n",
        "          (0.06, 0.06, 0.06), (0.07, 0.07, 0.07), (0.08, 0.08, 0.08), (0.09, 0.09, 0.09), (0.1, 0.1, 0.1)]\n",
        "      ]\n",
        "      l1l2_weights_mip = [[(w1/10, w2/10, w3/10) for (w1, w2, w3) in sublist] for sublist in l1l2_weights_mip]\n",
        "\n",
        "      l1l2_weights_ref_index = [\n",
        "          [(0.01, 0.01, 0.01), (0.02, 0.02, 0.02), (0.03, 0.03, 0.03), (0.04, 0.04, 0.04), (0.05, 0.05, 0.05),\n",
        "          (0.06, 0.06, 0.06), (0.07, 0.07, 0.07), (0.08, 0.08, 0.08), (0.09, 0.09, 0.09), (0.1, 0.1, 0.1)]\n",
        "      ]\n",
        "      l1l2_weights_ref_index = [[(w1/10, w2/10, w3/10) for (w1, w2, w3) in sublist] for sublist in l1l2_weights_ref_index]\n",
        "\n",
        "\n",
        "      l1l2_weights_pos = [[(w1/10, w2/10, w3/10) for (w1, w2, w3) in sublist] for sublist in l1l2_weights_ref_index]\n",
        "\n",
        "      l1l2_weights_hadrons = [\n",
        "          [(0.1, 0.1, 0.1), (0.1, 0.1, 0.1), (0.1, 0.1, 0.1),(0.1, 0.1, 0.1),(0.1, 0.1, 0.1),(0.1, 0.1, 0.1),(0.1, 0.1, 0.1),(0.1, 0.1, 0.1),\n",
        "         (0.1, 0.1, 0.1),(0.1, 0.1, 0.1),(0.1, 0.1, 0.1),(0.1, 0.1, 0.1),(0.1, 0.1, 0.1),(0.1, 0.1, 0.1),\n",
        "          (0.1, 0.1, 0.1),(0.1, 0.1, 0.1),(0.1, 0.1, 0.1),(0.1, 0.1, 0.1),(0.1, 0.1, 0.1),(0.1, 0.1, 0.1),(0.1, 0.1, 0.1),(0.1, 0.1, 0.1),(0.1, 0.1, 0.1)]\n",
        "      ]\n",
        "      l1l2_weights_hadrons = [[(w1/10, w2/10, w3/10) for (w1, w2, w3) in sublist] for sublist in l1l2_weights_hadrons]\n",
        "\n",
        "      l1l2_weights_momentum = [\n",
        "          [(0.01, 0.01, 0.01), (0.02, 0.02, 0.02), (0.1, 0.1, 0.1), (0.1, 0.1, 0.1), (0.1, 0.1, 0.1),\n",
        "          (0.1, 0.1, 0.1), (0.1, 0.1, 0.1), (0.1, 0.1, 0.1), (0.1, 0.1, 0.), (0.1, 0.1, 0.1)]\n",
        "      ]\n",
        "      l1l2_weights_momentum = [[(w1/10, w2/10, w3/10) for (w1, w2, w3) in sublist] for sublist in l1l2_weights_momentum]\n",
        "\n",
        "\n",
        "      l1l2_weights_mip = [[(0.001*4, 0.001*2, 0.001*2) for _ in sublist] for sublist in l1l2_weights_mip]\n",
        "\n",
        "      l1l2_weights_ref_index = [[(0.001*4, 0.001*2, 0.001*2) for _ in sublist] for sublist in l1l2_weights_ref_index]\n",
        "\n",
        "      #l1l2_weights_hadrons = [[(0.001*4, 0.001*2, 0.001*2) for _ in sublist] for sublist in l1l2_weights_hadrons]\n",
        "\n",
        "      l1l2_weights_momentum = [[(0.001*4, 0.001*2, 0.001*2) for _ in sublist] for sublist in l1l2_weights_momentum]\n",
        "\n",
        "      # just assign very small first\n",
        "      l1l2_weights_rad = [[(0.001, 0.001, 0.001) for _ in sublist] for sublist in l1l2_weights_mip]\n",
        "      l1l2_weights_energy = [[(0.001, 0.001, 0.001) for _ in sublist] for sublist in l1l2_weights_mip]\n",
        "      l1l2_weights_phi = [[(0.001, 0.001, 0.001) for _ in sublist] for sublist in l1l2_weights_mip]\n",
        "      l1l2_weights_theta = [[(0.001, 0.001, 0.001) for _ in sublist] for sublist in l1l2_weights_mip]\n",
        "\n",
        "\n",
        "\n",
        "      # i need help with these two :\n",
        "      l1l2_weights_cand_pos = [[(0.0001, 0.0001, 0.0001) for _ in sublist] for sublist in l1l2_weights_mip]\n",
        "      l1l2_weights_cand_status = [[(0.0001, 0.0001, 0.0001) for _ in sublist] for sublist in l1l2_weights_mip]\n",
        "\n",
        "\n",
        "      print(f\"l1l2_weights_mip shape {np.array(l1l2_weights_mip, dtype =object).shape}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      momentum_shape = refractive_index_shape = phi_shape = theta_shape = energy_shape = (1, )\n",
        "\n",
        "      #momentum_shape = refractive_index_shape = phi_shape = theta_shape = energy_shape = (len_proton, 1)\n",
        "\n",
        "      phi_input = Input(shape=phi_shape, name=\"phi_shape\")\n",
        "      theta_input = Input(shape=theta_shape, name=\"theta_shape\")\n",
        "      momentum_input = Input(shape=momentum_shape, name=\"momentum_input\")\n",
        "      refractive_index_input = Input(shape=refractive_index_shape, name=\"refractive_index_input\")\n",
        "      mip_position_shape = rad_position_shape = (2,)\n",
        "      #mip_position_shape = rad_position_shape = (len_proton, 2)\n",
        "\n",
        "      rad_position_input = Input(shape=rad_position_shape, name=\"rad_position_input\")\n",
        "      mip_position_input = Input(shape=mip_position_shape, name=\"mip_position_input\")\n",
        "\n",
        "      print(\"mapplying inputs\")\n",
        "\n",
        "      # For training data\n",
        "      # For train data\n",
        "      train_variables = [X_train_pion_candidates, X_train_kaon_candidates, X_train_proton_candidates, X_train_momentum, X_train_refractive_index, X_train_phi, X_train_theta, X_train_mip_position, X_train_rad_position]#, X_train_map_pion, X_train_map_kaon, X_train_map_proton]\n",
        "      train_labels = ['Train Pion Candidates', 'Train Kaon Candidates', 'Train Proton Candidates', 'Train Momentum', 'Train Refractive Index', 'Train Phi', 'Train Theta', 'Train MIP Position', 'Train Rad Position']#, 'Train Map Pion', 'Train Map Kaon', 'Train Map Proton']\n",
        "\n",
        "      # For test data\n",
        "      test_variables = [X_test_pion_candidates, X_test_kaon_candidates, X_test_proton_candidates, X_test_momentum, X_test_refractive_index, X_test_phi, X_test_theta, X_test_mip_position, X_test_rad_position]#, X_test_map_pion, X_test_map_kaon, X_test_map_proton]\n",
        "      test_labels = ['Test Pion Candidates', 'Test Kaon Candidates', 'Test Proton Candidates', 'Test Momentum', 'Test Refractive Index', 'Test Phi', 'Test Theta', 'Test MIP Position', 'Test Rad Position']#, 'Test Map Pion', 'Test Map Kaon', 'Test Map Proton']\n",
        "\n",
        "\n",
        "      train_variables = [X_train_momentum, X_train_refractive_index, X_train_phi, X_train_theta, X_train_mip_position, X_train_rad_position]#, X_train_map_pion, X_train_map_kaon, X_train_map_proton]\n",
        "      train_labels = ['Train Momentum', 'Train Refractive Index', 'Train Phi', 'Train Theta', 'Train MIP Position', 'Train Rad Position']#, 'Train Map Pion', 'Train Map Kaon', 'Train Map Proton']\n",
        "\n",
        "      # For test data\n",
        "      test_variables = [ X_test_momentum, X_test_refractive_index, X_test_phi, X_test_theta, X_test_mip_position, X_test_rad_position]#, X_test_map_pion, X_test_map_kaon, X_test_map_proton]\n",
        "      test_labels = ['Test Momentum', 'Test Refractive Index', 'Test Phi', 'Test Theta', 'Test MIP Position', 'Test Rad Position']#, 'Test Map Pion', 'Test Map Kaon', 'Test Map Proton']\n",
        "\n",
        "      map_size = (40, 40)\n",
        "\n",
        "\n",
        "      print(\"now create ip shapes pion_input_shape\")\n",
        "\n",
        "\n",
        "\n",
        "      # CNN\n",
        "      # pion_input_shape = (map_size[0], map_size[1], 1)\n",
        "      # kaon_input_shape = (map_size[0], map_size[1], 1)\n",
        "      # proton_input_shape = (map_size[0], map_size[1], 1)\n",
        "\n",
        "\n",
        "      # pion_map_input = Input(shape=pion_input_shape, name='pion_map_input')\n",
        "      # kaon_map_input = Input(shape=kaon_input_shape, name='kaon_map_input')\n",
        "      # proton_map_input = Input(shape=proton_input_shape, name='proton_map_input')\n",
        "\n",
        "\n",
        "      inputs = [pion_ip_mask, kaon_ip_mask, proton_ip_mask, phi_input, theta_input, refractive_index_input, momentum_input, rad_position_input, mip_position_input]#, pion_map_input, kaon_map_input, proton_map_input]\n",
        "      inputs = [phi_input, theta_input, refractive_index_input, momentum_input, rad_position_input, mip_position_input]#, pion_map_input, kaon_map_input, proton_map_input]\n",
        "\n",
        "      d=1\n",
        "      for input_layer in inputs:\n",
        "          print(f\"input{d} = {input_layer}, input_shape = {input_layer.shape}\")\n",
        "          d = d+1\n",
        "\n",
        "\n",
        "      for dropout in dropouts:\n",
        "          for num_filter in num_filters:\n",
        "              for stride in strides:\n",
        "                  for alpha in alphas:\n",
        "                      for fc1_unit in fc1_units:\n",
        "                          for fc2_unit in fc2_units:\n",
        "                              # check for                               #nan_count = np.isnan(pion_candidates_test).sum()\n",
        "\n",
        "                              # print the number of nan\n",
        "                              #print(f\"Number of nan values in pion_candidates_test = {nan_count}\")\n",
        "                              # pion_ip_mask = Masking(mask_value=0.)(pion_ip)\n",
        "                              # kaon_ip_mask = Masking(mask_value=0.)(kaon_ip)\n",
        "                              # proton_ip_mask = Masking(mask_value=0.)(proton_ip)\n",
        "\n",
        "                              #prev_dist2mip_layer = dist2mip_masked\n",
        "\n",
        "                              units = [fc1_unit * i for i in [1, 2, 4, 8, 16, 32, 64, 128, 64, 32, 16, 8, 4, 2, 1, 0.5, 0.25]]\n",
        "                              units = [fc1_unit * i for i in [1, 2, 4, 8, 16, 32, 16, 8, 4, 2, 1, 0.5, 0.25/4]]\n",
        "                              units = [fc1_unit * i for i in [1, 2, 4, 8, 16, 8, 4, 2, 1, 0.5, 0.25]]\n",
        "                              units = [fc1_unit * i for i in [1, 2, 4, 8, 16, 32, 8, 4, 2, 1, 0.5, 0.25]]\n",
        "                              units = [fc1_unit * i for i in [32, 16, 8, 8, 4, 2, 1, 0.5, 0.25]]\n",
        "                              units = [fc1_unit * i for i in [32, 16, 8]]\n",
        "\n",
        "                              #units = [fc1_unit * i for i in [1]]\n",
        "\n",
        "\n",
        "                              for i in range(1, len(units)):\n",
        "                                  unit = int(units[i])\n",
        "\n",
        "                                  # For pion\n",
        "                                  dense_pion_i = Dense(unit, name=f\"dense_pion_{i}\",\n",
        "                                                      kernel_regularizer=regularizers.L1L2(l1l2_weights_hadrons[0][i - 1][0], l1l2_weights_hadrons[0][i - 1][1]),\n",
        "                                                      bias_regularizer=regularizers.L1(l1l2_weights_hadrons[0][i - 1][2]))(prev_pion)\n",
        "\n",
        "                                  bn_pion_i = BatchNormalization(name=f\"bn_pion_{i}\")(dense_pion_i)\n",
        "                                  leakyrelu_pion_i = LeakyReLU(alpha=alpha, name=f\"leakyrelu_pion_{i}\")(bn_pion_i)\n",
        "                                  dropout_pion_i = Dropout(dropout, name=f\"dropout_pion_{i}\")(leakyrelu_pion_i)\n",
        "                                  prev_pion = dropout_pion_i\n",
        "\n",
        "                                  # For kaon\n",
        "                                  dense_kaon_i = Dense(unit, name=f\"dense_kaon_{i}\",\n",
        "                                                      kernel_regularizer=regularizers.L1L2(l1l2_weights_hadrons[0][i - 1][0], l1l2_weights_hadrons[0][i - 1][1]),\n",
        "                                                      bias_regularizer=regularizers.L1(l1l2_weights_hadrons[0][i - 1][2]))(prev_kaon)\n",
        "\n",
        "                                  bn_kaon_i = BatchNormalization(name=f\"bn_kaon_{i}\")(dense_kaon_i)\n",
        "                                  leakyrelu_kaon_i = LeakyReLU(alpha=alpha, name=f\"leakyrelu_kaon_{i}\")(bn_kaon_i)\n",
        "                                  dropout_kaon_i = Dropout(dropout, name=f\"dropout_kaon_{i}\")(leakyrelu_kaon_i)\n",
        "                                  prev_kaon = dropout_kaon_i\n",
        "\n",
        "                                  # For proton\n",
        "                                  dense_proton_i = Dense(unit, name=f\"dense_proton_{i}\",\n",
        "                                                        kernel_regularizer=regularizers.L1L2(l1l2_weights_hadrons[0][i - 1][0], l1l2_weights_hadrons[0][i - 1][1]),\n",
        "                                                        bias_regularizer=regularizers.L1(l1l2_weights_hadrons[0][i - 1][2]))(prev_proton)\n",
        "\n",
        "                                  bn_proton_i = BatchNormalization(name=f\"bn_proton_{i}\")(dense_proton_i)\n",
        "                                  leakyrelu_proton_i = LeakyReLU(alpha=alpha, name=f\"leakyrelu_proton_{i}\")(bn_proton_i)\n",
        "                                  dropout_proton_i = Dropout(dropout, name=f\"dropout_proton_{i}\")(leakyrelu_proton_i)\n",
        "                                  prev_proton = dropout_proton_i\n",
        "\n",
        "\n",
        "                              #units2 = [fc1_unit * i for i in [1, 2, 4, 8, 16, 4, 2, 1, 0.5, 0.25/4]]\n",
        "                              units2 = [fc1_unit * i for i in [1, 2, 4, 8, 16, 4, 2, 1, 0.5, 0.25]]\n",
        "                              units2 = [fc1_unit * i for i in [16, 8, 4, 4, 2, 2, 1, 1,  0.5, 0.25]]\n",
        "                              units2 = [fc1_unit * i for i in [32, 16, 8]]\n",
        "\n",
        "                              #units2 = [fc1_unit * i for i in [1]]\n",
        "\n",
        "                              prev_momentum = momentum_input\n",
        "                              prev_momentum = tf.identity(prev_momentum, name=\"prev_momentum\")\n",
        "\n",
        "                              prev_ref_index = refractive_index_input\n",
        "                              prev_ref_index = tf.identity(prev_ref_index, name=\"prev_ref_index\")\n",
        "\n",
        "                              prev_mip = mip_position_input\n",
        "                              prev_mip = tf.identity(prev_mip, name=\"prev_mip\")\n",
        "\n",
        "                              prev_rad = rad_position_input\n",
        "                              prev_rad = tf.identity(prev_rad, name=\"prev_rad\")\n",
        "\n",
        "                              prev_phi = phi_input\n",
        "                              prev_phi = tf.identity(prev_phi, name=\"prev_phi\")\n",
        "\n",
        "                              prev_theta = theta_input\n",
        "                              prev_theta = tf.identity(prev_theta, name=\"prev_theta\")\n",
        "\n",
        "\n",
        "\n",
        "                              for i in range(1, len(units2)):\n",
        "                                  step = int(units2[i])\n",
        "\n",
        "                                  #### REFRACTIVE INDEX #####\n",
        "                                  dense_ref_index_i = Dense(step, name=f\"dense_ref_index_{i}\",\n",
        "                                                            kernel_regularizer=regularizers.L1L2(l1l2_weights_ref_index[0][i - 1][0], l1l2_weights_ref_index[0][i - 1][1]),\n",
        "                                                            bias_regularizer=regularizers.L1(l1l2_weights_ref_index[0][i - 1][2]))(prev_ref_index)\n",
        "\n",
        "                                  bn_ref_index_i = BatchNormalization(name=f\"bn_ref_index_{i}\")(dense_ref_index_i)\n",
        "                                  leakyrelu_ref_index_i = LeakyReLU(alpha=alpha, name=f\"leakyrelu_ref_index_{i}\")(bn_ref_index_i)\n",
        "                                  dropout_ref_index_i = Dropout(dropout, name=f\"dropout_ref_index_{i}\")(leakyrelu_ref_index_i)\n",
        "\n",
        "                                  #### MOMENTUM #####\n",
        "                                  dense_momentum_i = Dense(step, name=f\"dense_momentum_{i}\",\n",
        "                                                          kernel_regularizer=regularizers.L1L2(l1l2_weights_momentum[0][i - 1][0], l1l2_weights_momentum[0][i - 1][1]),\n",
        "                                                          bias_regularizer=regularizers.L1(l1l2_weights_momentum[0][i - 1][2]))(prev_momentum)\n",
        "                                  bn_momentum_i = BatchNormalization(name=f\"bn_momentum_{i}\")(dense_momentum_i)\n",
        "                                  leakyrelu_momentum_i = LeakyReLU(alpha=alpha, name=f\"leakyrelu_momentum_{i}\")(bn_momentum_i)\n",
        "                                  dropout_momentum_i = Dropout(dropout, name=f\"dropout_momentum_{i}\")(leakyrelu_momentum_i)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                                  # PHI\n",
        "                                  dense_phi_i = Dense(step, name=f\"dense_phi_{i}\",\n",
        "                                                      kernel_regularizer=regularizers.L1L2(l1l2_weights_phi[0][i - 1][0], l1l2_weights_phi[0][i - 1][1]),\n",
        "                                                      bias_regularizer=regularizers.L1(l1l2_weights_phi[0][i - 1][2]))(prev_phi)\n",
        "                                  bn_phi_i = BatchNormalization(name=f\"bn_phi_{i}\")(dense_phi_i)\n",
        "                                  leakyrelu_phi_i = LeakyReLU(alpha=alpha, name=f\"leakyrelu_phi_{i}\")(bn_phi_i)\n",
        "                                  dropout_phi_i = Dropout(dropout, name=f\"dropout_phi_{i}\")(leakyrelu_phi_i)\n",
        "\n",
        "                                  # THETA\n",
        "                                  dense_theta_i = Dense(step, name=f\"dense_theta_{i}\",\n",
        "                                                      kernel_regularizer=regularizers.L1L2(l1l2_weights_theta[0][i - 1][0], l1l2_weights_theta[0][i - 1][1]),\n",
        "                                                      bias_regularizer=regularizers.L1(l1l2_weights_theta[0][i - 1][2]))(prev_theta)\n",
        "                                  bn_theta_i = BatchNormalization(name=f\"bn_theta_{i}\")(dense_theta_i)\n",
        "                                  leakyrelu_theta_i = LeakyReLU(alpha=alpha, name=f\"leakyrelu_theta_{i}\")(bn_theta_i)\n",
        "                                  dropout_theta_i = Dropout(dropout, name=f\"dropout_theta_{i}\")(leakyrelu_theta_i)\n",
        "\n",
        "                                  # RAD_POSITION\n",
        "                                  dense_rad_position_i = Dense(step, name=f\"dense_rad_position_{i}\",\n",
        "                                                      kernel_regularizer=regularizers.L1L2(l1l2_weights_rad[0][i - 1][0], l1l2_weights_rad[0][i - 1][1]),\n",
        "                                                      bias_regularizer=regularizers.L1(l1l2_weights_rad[0][i - 1][2]))(prev_rad)\n",
        "                                  bn_rad_position_i = BatchNormalization(name=f\"bn_rad_position_{i}\")(dense_rad_position_i)\n",
        "                                  leakyrelu_rad_position_i = LeakyReLU(alpha=alpha, name=f\"leakyrelu_rad_position_{i}\")(bn_rad_position_i)\n",
        "                                  dropout_rad_position_i = Dropout(dropout, name=f\"dropout_rad_position_{i}\")(leakyrelu_rad_position_i)\n",
        "\n",
        "\n",
        "                                  #### MIP POS #####\n",
        "                                  dense_mip_pos_i = Dense(step/2, name=f\"dense_mip_pos_{i}\",\n",
        "                                                          kernel_regularizer=regularizers.L1L2(l1l2_weights_mip[0][i - 1][0], l1l2_weights_mip[0][i - 1][1]),\n",
        "                                                          bias_regularizer=regularizers.L1(l1l2_weights_mip[0][i - 1][2]))(prev_mip)\n",
        "\n",
        "                                  bn_mip_pos_i = BatchNormalization(name=f\"bn_mip_pos_{i}\")(dense_mip_pos_i)\n",
        "                                  leakyrelu_mip_pos_i = LeakyReLU(alpha=alpha, name=f\"leakyrelu_mip_pos_{i}\")(bn_mip_pos_i)\n",
        "                                  dropout_mip_pos_i = Dropout(dropout, name=f\"dropout_mip_pos_{i}\")(leakyrelu_mip_pos_i)\n",
        "\n",
        "\n",
        "                                  prev_mip = dropout_mip_pos_i\n",
        "                                  prev_rad = dropout_rad_position_i\n",
        "\n",
        "                                  prev_momentum = dropout_momentum_i\n",
        "                                  prev_ref_index = dropout_ref_index_i\n",
        "                                  prev_phi = dropout_phi_i\n",
        "                                  prev_theta = dropout_theta_i\n",
        "\n",
        "                              epochs = 400\n",
        "                              lr = create_lr_scheduler(num_epochs=epochs)\n",
        "\n",
        "                              # concat_layers should have same ordeR :\n",
        "\n",
        "                              print(\"now concat scalar 2D\")\n",
        "\n",
        "\n",
        "\n",
        "                              pre_dense_layer = tf.keras.layers.Concatenate(name=\"concat_scalars_2D\")([prev_phi, prev_theta, prev_ref_index, prev_momentum, prev_rad, prev_mip])\n",
        "                              # can i flatten something here to make the dimensions match? :\n",
        "\n",
        "                              flat_pion = Flatten(name = \"flatpion\")(prev_pion)\n",
        "                              flat_kaon = Flatten(name = \"flatkaon\")(prev_kaon)\n",
        "                              flat_proton = Flatten(name = \"flatproton\")(prev_proton)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                              print(\"now call extract_neighborhood_map\")\n",
        "\n",
        "                              # Get the CNN outputs for each particle type\n",
        "\n",
        "                              #def extract_neighborhood_map(candidate_positions, mip_positions, neighborhood_size, map_size):\n",
        "\n",
        "\n",
        "                              # map_pion = np.expand_dims(map_pion, axis=0)\n",
        "                              # map_proton = np.expand_dims(map_proton, axis=0)\n",
        "                              # map_kaon = np.expand_dims(map_kaon, axis=0)\n",
        "\n",
        "# \"                             conv1 = Conv2D(32, (5, 5))(pion_map_input)\n",
        "#                               conv1 = BatchNormalization()(conv1)\n",
        "#                               conv1 = tf.keras.activations.relu(conv1)\n",
        "#                               conv1 = MaxPooling2D((2, 2))(conv1)  # Add max pooling after conv1\\n\",\n",
        "#                               convPion = Dropout(0.2)(conv1)  # Add dropout after max pooling\\n\n",
        "#                               flat_mapPion = Flatten()(convPion)\n",
        "\n",
        "\n",
        "#                               conv1 = Conv2D(32, (5, 5))(kaon_map_input)\n",
        "#                               conv1 = BatchNormalization()(conv1)\n",
        "#                               conv1 = tf.keras.activations.relu(conv1)\n",
        "#                               conv1 = MaxPooling2D((2, 2))(conv1)  # Add max pooling after conv1\\n\",\n",
        "#                               convKaon = Dropout(0.2)(conv1)  # Add dropout after max pooling\\n\n",
        "#                               flat_mapKaon = Flatten()(convKaon)\n",
        "\n",
        "\n",
        "#                               conv1 = Conv2D(32, (5, 5))(proton_map_input)\n",
        "#                               conv1 = BatchNormalization()(conv1)\n",
        "#                               conv1 = tf.keras.activations.relu(conv1)\n",
        "#                               conv1 = MaxPooling2D((2, 2))(conv1)  # Add max pooling after conv1\\n\",\n",
        "#                               convProton = Dropout(0.2)(conv1)  # Add dropout after max pooling\\n\n",
        "#                               flat_mapProton = Flatten()(convProton)\n",
        "\n",
        "                              # kommenter over vekk\n",
        "\n",
        "                              # print(\"now create_cnn_model\")\n",
        "                              # pion_input_shape = proton_input_shape = kaon_input_shape = ()\n",
        "                              # cnn_pion_model = create_cnn_model(map_proton.shape, name = \"pion__\")\n",
        "                              # cnn_kaon_model = create_cnn_model(map_proton.shape, name = \"kaon__\")\n",
        "                              # cnn_proton_model = create_cnn_model(map_proton.shape, name = \"proton__\")\n",
        "\n",
        "                              print(\"now assembple ip\")\n",
        "                              # Inputs for the model\n",
        "\n",
        "\n",
        "\n",
        "                              print(\"now cnn_pion_model call W IP\")\n",
        "\n",
        "\n",
        "                              # map_pion = np.expand_dims(map_pion, axis=0)\n",
        "                              # map_proton = np.expand_dims(map_proton, axis=0)\n",
        "                              # map_kaon = np.expand_dims(map_kaon, axis=0)\n",
        "\n",
        "\n",
        "\n",
        "                              # cnn_pion_output = cnn_pion_model(map_pion)\n",
        "                              # cnn_kaon_output = cnn_kaon_model(map_kaon)\n",
        "                              # cnn_proton_output = cnn_proton_model(map_proton)\n",
        "\n",
        "\n",
        "\n",
        "                              # dense_pion = Dense(32, activation='relu', name = \"dense Pion\")(cnn_pion_output)\n",
        "                              # dense_kaon = Dense(32, activation='relu', name = \"dense Kaon\")(cnn_kaon_output)\n",
        "                              # dense_proton = Dense(32, activation='relu', name = \"dense Proton\")(cnn_proton_output)\n",
        "\n",
        "\n",
        "                              # flat_pionconv = Flatten(name = \"flatpiondensepioj\")(dense_pion)\n",
        "                              # flat_kaonconv = Flatten(name = \"flatpiondense_kaon\")(dense_kaon)\n",
        "                              # flat_protonconv = Flatten(name = \"flatpiondense_proton\")(dense_proton)\n",
        "\n",
        "\n",
        "\n",
        "                              # concat_cnn = tf.keras.layers.Concatenate(name=\"Concat CNNs\")([flat_pionconv, flat_kaonconv, flat_protonconv])\n",
        "\n",
        "                              #concat_cnn = tf.keras.layers.Concatenate(name=\"Concat_CNN\")([flat_mapPion, flat_mapKaon, flat_mapProton])\n",
        "\n",
        "\n",
        "                              print(\"now concat_cnn done \")\n",
        "\n",
        "                              dense_comb = Dense(16, activation='relu')(pre_dense_layer)\n",
        "                              dropout_comb = Dropout(dropout)(dense_comb)\n",
        "\n",
        "                              #for pd in pre_dense_layer:\n",
        "                              #    print(f\"concat_layers{d} : layer = {layer}, output_shape = {layer.shape}, layer_name = {layer.name}\")\n",
        "\n",
        "                              print(\"now concat all\")\n",
        "                              concat_layers = [flat_pion, flat_kaon, flat_proton]\n",
        "\n",
        "                              final_concat_layer = tf.keras.layers.Concatenate(name=\"concat_scalars_all\")(concat_layers + [pre_dense_layer])# + [concat_cnn])\n",
        "                              final_concat_layer = tf.keras.layers.Concatenate(name=\"concat_scalars_all\")([pre_dense_layer])# + [concat_cnn])\n",
        "\n",
        "                              dense_comb = Dense(8, activation='relu')(final_concat_layer)\n",
        "                              final_concat_layer = Dropout(dropout)(dense_comb)\n",
        "\n",
        "\n",
        "                              d = 1\n",
        "                              for layer in concat_layers:\n",
        "                                  print(f\"concat_layers{d} : layer = {layer}, output_shape = {layer.shape}, layer_name = {layer.name}\")\n",
        "                                  d = d + 1\n",
        "                              print(\"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "                              # for kun å evaluere en hvis type IPs\n",
        "                              #inputs, concat_layers, x_input_test, x_input_train = [], [], [], []\n",
        "                              #x_input_train_ev = [pion_candidates_train_tf, kaon_candidates_train_tf, proton_candidates_train_tf, X_train[\"X_train_refractive_index\"],X_train[\"X_train_momentum\"], X_train[\"X_train_mip_position\"]]\n",
        "                              #x_input_test_ev = [pion_candidates_test_tf, kaon_candidates_test_tf, proton_candidates_test_tf, X_test[\"X_test_refractive_index\"],X_test[\"X_test_momentum\"], X_test[\"X_test_mip_position\"]]\n",
        "\n",
        "\n",
        "                              # senere : for å kun ta med en hvis type parametere :\n",
        "                              # for index in range(len(inputs_ev)):\n",
        "                              #   if mask[index] == 1:\n",
        "                              #     inputs.append(inputs_ev[index])\n",
        "                              #     concat_layers.append(concat_layers_ev[index])\n",
        "                              #     x_input_train.append(x_input_train_ev[index])\n",
        "                              #     x_input_test.append(x_input_test_ev[index])\n",
        "\n",
        "                              print(\"Concatenating layers by : concat = concatenate(concat_layers)\")\n",
        "                              concat = concatenate(concat_layers)\n",
        "\n",
        "\n",
        "\n",
        "                              ## number of op typs : other pion kaon proton\n",
        "                              # changed to 3 outputs -- ie no noncandidate\n",
        "                              output = Dense(3, activation='softmax')(final_concat_layer) # NB LEGG MERKE TIL AT DENNE ER ENDRET FRA FC2\n",
        "\n",
        "                              #x_input_train = [pion_candidates_train_tf, kaon_candidates_train_tf, proton_candidates_train_tf]\n",
        "                              #x_input_test = [pion_candidates_test_tf, kaon_candidates_test_tf, proton_candidates_test_tf]\n",
        "\n",
        "                              x_input_train = train_variables\n",
        "                              x_input_test = test_variables\n",
        "                              # for i, array in enumerate(x_input_train):\n",
        "                              #     if np.isnan(array).any():\n",
        "                              #         print(f\"Array at index {i} contains NaN values.\")\n",
        "                              #     else:\n",
        "                              #         print(f\"Array at index {i} is valid.\")\n",
        "                              # for i, array in enumerate(x_input_train):\n",
        "                              #     if np.any(np.isnan(array)):\n",
        "                              #         print(f\"Array at index {i} contains NaN values.\")\n",
        "                              #     else:\n",
        "                              #         print(f\"Array at index {i} does not contain NaN values.\")\n",
        "                              # d = 1\n",
        "                              # for input_layer in inputs:\n",
        "                              #     print(f\"input{d} = {input_layer}, input_shape = {input_layer.shape}, input_name = {input_layer.name}\")\n",
        "                              #     d = d + 1\n",
        "\n",
        "                              # d = 1\n",
        "                              # for layer in concat_layers:\n",
        "                              #     print(f\"concat_layers{d} : layer = {layer}, output_shape = {layer.shape}, layer_name = {layer.name}\")\n",
        "                              #     d = d + 1\n",
        "\n",
        "\n",
        "                              # for i, input in enumerate(inputs):\n",
        "                              #   print(f\"input{i} = {input} || inputname ={input.name}\")\n",
        "                              #   print(f\"output{i} = {concat_layers[i]} | output_shape = {concat_layers[i].name}\")\n",
        "                              #   try:\n",
        "                              #     print(f\"x_input_train_shape{i} = {np.asarray(np.asarray(x_input_train[i]), dtype=object).shape}\")\n",
        "                              #   except Exception as e:\n",
        "                              #     print(f\"x_input_train_shape failed w {e}\")\n",
        "\n",
        "                              model = Model(inputs = inputs,\n",
        "                                            outputs=output)\n",
        "\n",
        "                              #model = Model(inputs=[window_input], outputs=[output])\n",
        "                              model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), # NB changed from 0.0002\n",
        "                                            loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "                              model.summary()\n",
        "                              print(\"end model.summary()\")\n",
        "\n",
        "                              # later change back?\n",
        "                              # Train the model\n",
        "                              history = model.fit(\n",
        "                                  x=x_input_train, # x=[X_train_dist2mip, X_train_momentum, X_train_refractive_index, X_train_ckov, X_train_mip_position]\n",
        "                                  y=y_train,\n",
        "                                  validation_data=(\n",
        "                                      x_input_test,# X_test[\"X_test_refractive_index\"], X_test[\"X_test_momentum\"], X_test[\"X_test_mip_position\"]],\n",
        "                                      y_test\n",
        "                                  ),\n",
        "                                  batch_size=32,\n",
        "                                  epochs=epochs,\n",
        "                                  verbose=1, #NB! kommenterte ut detee\n",
        "                                  callbacks=[lr, early_stopping] # include early_stopping in the list of callbacks\n",
        "                              )\n",
        "\n",
        "                            #   # plotting the worst cases?:\n",
        "                            #   #y_pred_test = model.predict([X_test[\"X_test_map\"], X_test[\"X_test_momentum\"], X_test[\"X_test_refractive_index\"], X_test[\"X_test_mip_position\"]])\n",
        "                            #   #plot_worst(model, y_test, X_test_map, X_test_momentum, X_test_refractive_index, X_test_ckov, X_test_mip_position, y_pred):\n",
        "                            #   print(\"Shape of y_test: \", y_test.shape)\n",
        "                            #   #print(\"Shape of X_test_map: \", X_test[\"X_test_map\"].shape)\n",
        "                            # # print(\"Shape of X_test_windows: \", X_test[\"X_test_windows\"].shape)\n",
        "                            #   print(\"Shape of X_test_momentum: \", X_test[\"X_test_momentum\"].shape)\n",
        "                            #   print(\"Shape of X_test_refractive_index: \", X_test[\"X_test_refractive_index\"].shape)\n",
        "                            #   #print(\"Shape of X_test_ckov: \", X_test[\"X_test_ckov\"].shape)\n",
        "                            #   print(\"Shape of X_test_mip_position: \", X_test[\"X_test_mip_position\"].shape)\n",
        "                            #   #print(\"Shape of y_pred_test: \", y_pred_test.shape)\n",
        "                            #   # try: # NB! commented out this line\n",
        "                            #   #   #plot_worst_(model, y_test, X_test[\"X_test_map\"], X_test[\"X_test_momentum\"], X_test[\"X_test_refractive_index\"], X_test[\"X_test_ckov\"], X_test[\"X_test_mip_position\"], y_pred_test)\n",
        "                            #   #   #plot_worst_(model, y_test, X_test[\"X_test_map\"], X_test[\"X_test_momentum\"], X_test[\"X_test_refractive_index\"], X_test[\"X_test_ckov\"], X_test[\"X_test_mip_position\"], self.resolution)\n",
        "                            #   # except Exception as e:\n",
        "                            #   #   print(f\"skip plot_worst_ due to error: {e}\")\n",
        "\n",
        "                              y_pred_train = model.predict(x_input_train)\n",
        "                              # y_pred_train = model.predict([pion_candidates_train_tf, kaon_candidates_train_tf, proton_candidates_train_tf,\n",
        "                              #                               X_train[\"X_train_refractive_index\"], X_train[\"X_train_momentum\"],\n",
        "                              #                               X_train[\"X_train_mip_position\"]])\n",
        "                              y_pred_test = model.predict(x_input_test)\n",
        "                              # y_pred_test = model.predict([pion_candidates_test_tf, kaon_candidates_test_tf, proton_candidates_test_tf,\n",
        "                              #                              X_test[\"X_test_refractive_index\"], X_test[\"X_test_momentum\"],\n",
        "                              #                              X_test[\"X_test_mip_position\"]])\n",
        "\n",
        "\n",
        "                              #    def plot_training_history(self, history=None, vector_of_weights=None, vector_of_weights2=None, dropout=None, y_pred_train=None, y_pred_test=None, y_train_true=None, y_test_true=None):\n",
        "\n",
        "\n",
        "\n",
        "                              plot_training_history(history = history, vector_of_weights = units, vector_of_weights2 = units2,\n",
        "                                                    dropout = dropout, y_pred_train = y_pred_train, y_pred_test = y_pred_test, y_train_true = y_train,  y_test_true = y_test, relu_alpha = alpha)\n",
        "\n",
        "                              #self.plot_training_history(history, units, units2)\n",
        "                              #self.plot_training_history(history, units, units2)\n",
        "\n",
        "                              # Evaluate the model\n",
        "\n",
        "                              # x_ip_test = [pion_candidates_test_tf, kaon_candidates_test_tf, proton_candidates_test_tf,\n",
        "                              #         X_test[\"X_test_refractive_index\"], X_test[\"X_test_momentum\"], X_test[\"X_test_mip_position\"]]\n",
        "                              x_ip_test = x_input_test\n",
        "                              _, accuracy = model.evaluate(\n",
        "                                  x= x_ip_test,\n",
        "                                  y=y_test,\n",
        "                                  verbose=1\n",
        "                              )\n",
        "\n",
        "                              print(f\"Model Accuracy: {accuracy:.4f} (fc1 Size: {fc1_unit}, Num fc2: {fc2_unit}, dropout = {dropout}\")\n",
        "\n",
        "\n",
        "                              # Check if the current model configuration is better\n",
        "                              if accuracy > best_accuracy:\n",
        "                                  best_accuracy = accuracy\n",
        "                                  best_model = model\n",
        "\n",
        "      # Set the best model as the final model\n",
        "      self.model = best_model\n",
        "\n",
        "\n",
        "  def train_model(self, X_train, X_test, y_train, y_test):\n",
        "      # Compile the model\n",
        "\n",
        "      X_train_pion_candidates = X_train[\"X_train_pion_candidates\"]\n",
        "      X_train_kaon_candidates = X_train[\"X_train_kaon_candidates\"]\n",
        "      X_train_proton_candidates = X_train[\"X_train_proton_candidates\"]\n",
        "      X_train_momentum = X_train[\"X_train_momentum\"]\n",
        "      X_train_refractive_index = X_train[\"X_train_refractive_index\"]\n",
        "      X_train_phi = X_train[\"X_train_phi\"]\n",
        "      X_train_theta = X_train[\"X_train_theta\"]\n",
        "      X_train_mip_position = X_train[\"X_train_mip_position\"]\n",
        "      X_train_rad_position = X_train[\"X_train_rad_position\"]\n",
        "\n",
        "      X_test_pion_candidates = X_test[\"X_test_pion_candidates\"]\n",
        "      X_test_kaon_candidates = X_test[\"X_test_kaon_candidates\"]\n",
        "      X_test_proton_candidates = X_test[\"X_test_proton_candidates\"]\n",
        "      X_test_momentum = X_test[\"X_test_momentum\"]\n",
        "      X_test_refractive_index = X_test[\"X_test_refractive_index\"]\n",
        "      X_test_phi = X_test[\"X_test_phi\"]\n",
        "      X_test_theta = X_test[\"X_test_theta\"]\n",
        "      X_test_mip_position = X_test[\"X_test_mip_position\"]\n",
        "      X_test_rad_position = X_test[\"X_test_rad_position\"]\n",
        "\n",
        "      print(\"Fields in the first vector of X_train:\")\n",
        "      print(\"X_train_pion_candidates shape:\", X_train_pion_candidates.shape)\n",
        "      print(\"X_train_kaon_candidates shape:\", X_train_kaon_candidates.shape)\n",
        "      print(\"X_train_proton_candidates shape:\", X_train_proton_candidates.shape)\n",
        "      print(\"X_train_momentum shape:\", X_train_momentum.shape)\n",
        "      print(\"X_train_refractive_index shape:\", X_train_refractive_index.shape)\n",
        "      print(\"X_train_phi shape:\", X_train_phi.shape)\n",
        "      print(\"X_train_theta shape:\", X_train_theta.shape)\n",
        "      print(\"X_train_mip_position shape:\", X_train_mip_position.shape)\n",
        "      print(\"X_train_rad_position shape:\", X_train_rad_position.shape)\n",
        "\n",
        "      print(\"\\nFields in the first vector of X_test:\")\n",
        "      print(\"X_test_pion_candidates shape:\", X_test_pion_candidates.shape)\n",
        "      print(\"X_test_kaon_candidates shape:\", X_test_kaon_candidates.shape)\n",
        "      print(\"X_test_proton_candidates shape:\", X_test_proton_candidates.shape)\n",
        "      print(\"X_test_momentum shape:\", X_test_momentum.shape)\n",
        "      print(\"X_test_refractive_index shape:\", X_test_refractive_index.shape)\n",
        "      print(\"X_test_phi shape:\", X_test_phi.shape)\n",
        "      print(\"X_test_theta shape:\", X_test_theta.shape)\n",
        "      print(\"X_test_mip_position shape:\", X_test_mip_position.shape)\n",
        "      print(\"X_test_rad_position shape:\", X_test_rad_position.shape)\n",
        "\n",
        "      self.model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "\n",
        "      # Print the first element of y_train\n",
        "      print(\"First element of y_train:\", y_train[0])\n",
        "\n",
        "      # Train the model\n",
        "      history = self.model.fit(\n",
        "          x=[X_train],\n",
        "          y=y_train,\n",
        "          validation_data=(\n",
        "              [X_test],\n",
        "              y_test\n",
        "          ),\n",
        "          batch_size=16,\n",
        "          epochs=10,\n",
        "          verbose=1\n",
        "      )\n",
        "      return history\n",
        "\n",
        "  def evaluate_model(self, X_test, y_test):\n",
        "\n",
        "\n",
        "      X_test_pion_candidates = X_test[\"X_test_pion_candidates\"]\n",
        "      X_test_kaon_candidates = X_test[\"X_test_kaon_candidates\"]\n",
        "      X_test_proton_candidates = X_test[\"X_test_proton_candidates\"]\n",
        "      X_test_momentum = X_test[\"X_test_momentum\"]\n",
        "      X_test_refractive_index = X_test[\"X_test_refractive_index\"]\n",
        "      X_test_phi = X_test[\"X_test_phi\"]\n",
        "      X_test_theta = X_test[\"X_test_theta\"]\n",
        "      X_test_mip_position = X_test[\"X_test_mip_position\"]\n",
        "      X_test_rad_position = X_test[\"X_test_rad_position\"]\n",
        "      loss, accuracy = self.model.evaluate(\n",
        "          x=X_test,\n",
        "          y=y_test,\n",
        "          verbose=0\n",
        "      )\n",
        "      print(f\"Test Loss: {loss:.4f}\")\n",
        "      print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "\n",
        "  def train(self, X_train, X_test, y_train, y_test, mask):\n",
        "\n",
        "      #X_ckov_segm = X_train[\"X_train_photon_ckov_segmented\"]\n",
        "      #print(f\"X_train_photon_ckov_segmented shape = {np.asarray(X_ckov_segm, dtype = object).shape}\")\n",
        "      X_train_photon_ckov_segmented = 0# np.asarray(X_train[\"X_train_photon_ckov_segmented\"])\n",
        "\n",
        "\n",
        "\n",
        "      #print(f\" in  def train(self, filename) : X_train_photon_ckov_segmented shape : {X_train_photon_ckov_segmented.shape}\")\n",
        "      #X_train_dist2mip = X_train_dist2mip.reshape(X_train_dist2mip.shape[0], X_train_dist2mip.shape[1],, 1)\n",
        "      #for dist in X_train_dist2mip:\n",
        "      #  print(f\"dist = {dist}\")\n",
        "\n",
        "\n",
        "      input_sequence_length = 0# len(max(X_train_photon_ckov_segmented, key=len))\n",
        "\n",
        "      self.build_model(input_sequence_length = input_sequence_length, X_train = X_train, X_test = X_test, y_train = y_train, y_test = y_test, mask = mask)\n",
        "\n",
        "\n",
        "      try:\n",
        "        history = self.train_model(X_train, X_test, y_train, y_test)\n",
        "        self.evaluate_model(X_test, y_test)\n",
        "      except Exception as e:\n",
        "        print(\"    def train(self, filename) failed at history = self.train_model bc of error : {e} \")\n",
        "\n",
        "      #plot_training_history(self, history, vector_of_weights, vector_of_weights2, dropout):\n",
        "      #self.plot_training_history(history)\n",
        "\n",
        "\n",
        "\n",
        "def plot_worst(model, y_test, X_test_map, X_test_momentum, X_test_refractive_index, X_test_ckov, X_test_mip_position, y_pred):\n",
        "  # 1. Predict labels on validation data\n",
        "\n",
        "  # 2. Calculate the difference between predicted and actual labels\n",
        "  losses = tf.keras.losses.categorical_crossentropy(y_test, y_pred).numpy()\n",
        "\n",
        "  # Sort the indices of the losses from highest to lowest\n",
        "  sorted_indices = np.argsort(losses)[::-1]\n",
        "\n",
        "  # Get the indices of the worst performing 10%\n",
        "  worst_10_percent_indices = sorted_indices[:int(0.1*len(sorted_indices))]\n",
        "\n",
        "  # Create figure and axes\n",
        "  num_plots = len(worst_10_percent_indices)\n",
        "  #fig, axes = plt.subplots(num_plots, 1, figsize=(8, 20))\n",
        "  fig, axes = plt.subplots(num_plots,figsize=(8, 20))\n",
        "\n",
        "  # Define mass categories\n",
        "  mass_categories = [\"pion\", \"kaon\", \"proton\"]\n",
        "\n",
        "  # 3. Create plots for these cases, including their feature information and predicted vs actual labels\n",
        "  for i, index in enumerate(worst_10_percent_indices):\n",
        "      # Get the map and corresponding information\n",
        "      map_data = X_test_map[index, :, :]\n",
        "      actual_mass_category = mass_categories[np.argmax(y_test[index])]\n",
        "\n",
        "      print(f\"y_test[index] = {y_test[index]}\")\n",
        "\n",
        "      predicted_mass_category = mass_categories[np.argmax(y_pred[index])]\n",
        "      ckov = X_test_ckov[index]\n",
        "      mip_position = X_test_mip_position[index]\n",
        "      momentum = X_test_momentum[index]\n",
        "      refractive_index = X_test_refractive_index[index]\n",
        "\n",
        "      mass_actual = momentum * np.sqrt(refractive_index**2 * np.cos(ckov)*np.cos(ckov) - 1)\n",
        "\n",
        "      # Check if the value is NaN (invalid Cherenkov angle)\n",
        "      if np.isnan(mass_actual):\n",
        "          mass_actual = \"Invalid\"\n",
        "\n",
        "      # Plot the map\n",
        "      axes[i].imshow(map_data, cmap='gray')\n",
        "\n",
        "      # Add a red dot at the MIP position\n",
        "      axes[i].plot(mip_position[0], mip_position[1], 'ro')\n",
        "\n",
        "      # Set the title with the information\n",
        "      axes[i].set_title(f\"Actual Mass\")#: {actual_mass_category}, Predicted Mass: {predicted_mass_category},\\nMass: {mass_actual}, Mass_prob = {y_pred[index]} \\nCKOV: {ckov}, MIP Position: {mip_position}, \\nMomentum: {momentum}, Refractive Index: {refractive_index}\")\n",
        "      #\n",
        "      axes[i].set_title(f\"Actual Mass: {actual_mass_category}, Predicted Mass: {predicted_mass_category},\\nMass: {mass_actual}, Mass_prob = {y_pred[index]} \\nCKOV: {ckov}, MIP Position: {mip_position}, \\nMomentum: {momentum}, Refractive Index: {refractive_index}\")\n",
        "\n",
        "      #axes[i].set_title(f\"Actual Mass: {actual_mass_category}, Predicted Mass: {predicted_mass_category}, Mass: {mass_actual}\\nCKOV: {ckov}, MIP Position: {mip_position}, Momentum: {momentum}, Refractive Index: {refractive_index}\")\n",
        "      axes[i].axis('off')\n",
        "\n",
        "      print(\"\\n\")\n",
        "      print(f\"  Actual Mass: {actual_mass_category}, Predicted Mass: {predicted_mass_category},\\n Mass: {mass_actual}, Mass_prob = {y_pred[index]} \\n CKOV: {ckov}, MIP Position: {mip_position}, \\n  Momentum: {momentum}, Refractive Index: {refractive_index}\")\n",
        "  # Adjust the spacing between subplots\n",
        "  plt.tight_layout()\n",
        "\n",
        "  # Show the plot\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Usage"
      ],
      "metadata": {
        "id": "PptqTnJ_2iyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Usage example\n",
        "\n",
        "\n",
        "#these three have .008 in stddev\n",
        "# 3 percent occupancy:\n",
        "print(\"classifier = MassClassifier(percentage_to_read = 100, resolution = 4) # pass percentage of dataset to read\")\n",
        "classifier = MassClassifier(percentage_to_read = 100, resolution = 4) # pass percentage of dataset to read\n",
        "\n",
        "# L whole range; 4 std\n",
        "#file_to_read = \"test/ParticleInfoProton.h5\" , Prtoon\n",
        "#file_to_read = \"test/ParticleInfo20k.h5\"\n",
        "\n",
        "# Proton, 3 std, L 0.1 1.4\n",
        "#file_to_read = \"test/diffLandSegm/ParticleInfo.h5\" proton proton\n",
        "\n",
        "#file_to_read = \"test/diffLandSegm/ParticleInfoPb.h5\" # PbPB 2STD 2k\n",
        "file_to_read = \"test/diffLandSegm/ParticleInfoPb20k.h5\" # PbPB 2STD, 20k particles\n",
        "\n",
        "\n",
        "file_to_read = \"ParticleInfo2.h5\" # PbPB 2STD, 20k particles\n",
        "\n",
        "\n",
        "#file_to_read = \"test/diffLandSegm/ParticleInfo8kPP.h5\" # PP 2STD, 8k particles\n",
        "\n",
        "#file_to_read = \"test/diffLandSegm/ParticleInfo18k01.h5\" # 0.1 2STD, 18k particle\n",
        "\n",
        "print(f\"classifier.load_data{file_to_read}\")\n",
        "\n",
        "\n",
        "file_to_read = \"ParticleInfo211.h5\" # PbPB 2STD, 20k particles\n",
        "classifier.load_data(file_to_read)\n",
        "\n",
        "file_to_read = \"ParticleInfo321.h5\" # PbPB 2STD, 20k particles\n",
        "classifier.load_data(file_to_read)\n",
        "\n",
        "\n",
        "file_to_read = \"ParticleInfo2212.h5\" # PbPB 2STD, 20k particles\n",
        "classifier.load_data(file_to_read)\n",
        "\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = classifier.preprocess_data()\n",
        "#print(\"X_train, X_test, y_train, y_test, maps, positions = classifier.preprocess_data()\")\n",
        "\n",
        "#classifier.train(X_train = X_train, X_test = X_test, y_train = y_train, y_test = y_test)\n",
        "\n",
        "#classifier.build_model(file_to_read)\n",
        "\n",
        "#classifier.train(file_to_read)\n",
        "\n",
        "\n",
        "# 3 percent occupancy:\n",
        "#classifier = MassClassifier(percentage_to_read = 25, resolution = 4) # pass percentage of dataset to read\n",
        "#classifier.train(\"ParticleInfoProton.h5\")\n"
      ],
      "metadata": {
        "id": "dP4hLxCiN9SP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdeed09f-ff85-4fec-a5b7-eb70e37e6af5"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classifier = MassClassifier(percentage_to_read = 100, resolution = 4) # pass percentage of dataset to read\n",
            "classifier.load_dataParticleInfo2.h5\n",
            "MassClassifier __ load_data\n",
            " i 0 max_length_nested 49\n",
            " i 14 max_length_nested 96\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:2009: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  result = asarray(a).shape\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3156: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  return asarray(a).ndim\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/shape_base.py:591: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  a = asanyarray(a)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_values_data Type of sequences: <class 'list'>, shape of sequences: (221,)\n",
            "Type of sequences: <class 'list'>, shape of sequences: (221,)\n",
            "Type of sequences: <class 'list'>, shape of sequences: (221,)\n",
            "Type of sequences: <class 'list'>, shape of sequences: (221,)\n",
            "Type of sequences: <class 'list'>, shape of sequences: (221,)\n",
            "<class 'list'> (221,)\n",
            "Created scaler for scalars\n",
            "self.particle_vector is None\n",
            "Number of particles: 221\n",
            "MassClassifier __ load_data\n",
            " i 0 max_length_nested 29\n",
            " i 2 max_length_nested 44\n",
            " i 6 max_length_nested 55\n",
            " i 15 max_length_nested 62\n",
            " i 56 max_length_nested 83\n",
            " i 114 max_length_nested 94\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:2009: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  result = asarray(a).shape\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3156: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  return asarray(a).ndim\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/shape_base.py:591: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  a = asanyarray(a)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_values_data Type of sequences: <class 'list'>, shape of sequences: (241,)\n",
            "Type of sequences: <class 'list'>, shape of sequences: (241,)\n",
            "Type of sequences: <class 'list'>, shape of sequences: (241,)\n",
            "Type of sequences: <class 'list'>, shape of sequences: (241,)\n",
            "Type of sequences: <class 'list'>, shape of sequences: (241,)\n",
            "<class 'list'> (241,)\n",
            "Created scaler for scalars\n",
            "self.particle_vector is not None\n",
            "Number of particles: 462\n",
            "MassClassifier __ load_data\n",
            " i 0 max_length_nested 24\n",
            " i 4 max_length_nested 27\n",
            " i 5 max_length_nested 36\n",
            " i 10 max_length_nested 47\n",
            " i 33 max_length_nested 59\n",
            " i 55 max_length_nested 60\n",
            " i 132 max_length_nested 84\n",
            "x_values_data Type of sequences: <class 'list'>, shape of sequences: (206,)\n",
            "Type of sequences: <class 'list'>, shape of sequences: (206,)\n",
            "Type of sequences: <class 'list'>, shape of sequences: (206,)\n",
            "Type of sequences: <class 'list'>, shape of sequences: (206,)\n",
            "Type of sequences: <class 'list'>, shape of sequences: (206,)\n",
            "<class 'list'> (206,)\n",
            "Created scaler for scalars\n",
            "self.particle_vector is not None\n",
            "Number of particles: 668\n",
            "Fields in the first vector of X_train:\n",
            "pion_candidates_list shape: (206, 84, 7)\n",
            "kaon_candidates_list shape: (206, 84, 7)\n",
            "proton_candidates_list shape: (206, 84, 7)\n",
            "PADDED \n",
            ":\n",
            "X_pion_candidates shape: (206, 84, 7)\n",
            "X_kaon_candidates shape: (206, 84, 7)\n",
            "X_proton_candidates shape: (206, 84, 7)\n",
            "PADDED \n",
            ":\n",
            "  return (X_train, X_test, y_train, y_test) \n",
            ":\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:2009: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  result = asarray(a).shape\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3156: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  return asarray(a).ndim\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/shape_base.py:591: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  a = asanyarray(a)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plotting histograms of Training Versus Test Sets"
      ],
      "metadata": {
        "id": "VRrcGqJzL0yO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#classifier.plot_hist(X_train = X_train, X_test = X_test, y_train = y_train, y_test = y_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "yodihp5E2k9X"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X_train_pion_candidates = X_train[\"X_train_pion_candidates\"]\n",
        "X_train_kaon_candidates = X_train[\"X_train_kaon_candidates\"]\n",
        "X_train_proton_candidates = X_train[\"X_train_proton_candidates\"]\n",
        "X_train_momentum = X_train[\"X_train_momentum\"]\n",
        "X_train_refractive_index = X_train[\"X_train_refractive_index\"]\n",
        "X_train_phi = X_train[\"X_train_phi\"]\n",
        "X_train_theta = X_train[\"X_train_theta\"]\n",
        "X_train_mip_position = X_train[\"X_train_mip_position\"]\n",
        "X_train_rad_position = X_train[\"X_train_rad_position\"]\n",
        "\n",
        "X_test_pion_candidates = X_test[\"X_test_pion_candidates\"]\n",
        "X_test_kaon_candidates = X_test[\"X_test_kaon_candidates\"]\n",
        "X_test_proton_candidates = X_test[\"X_test_proton_candidates\"]\n",
        "X_test_momentum = X_test[\"X_test_momentum\"]\n",
        "X_test_refractive_index = X_test[\"X_test_refractive_index\"]\n",
        "X_test_phi = X_test[\"X_test_phi\"]\n",
        "X_test_theta = X_test[\"X_test_theta\"]\n",
        "X_test_mip_position = X_test[\"X_test_mip_position\"]\n",
        "X_test_rad_position = X_test[\"X_test_rad_position\"]\n",
        "\n",
        "\n",
        "# ef : TODO fix this again\n",
        "#plot_first_instance(X_train_pion_candidates, X_train_kaon_candidates, X_train_proton_candidates, X_train_mip_position, X_train_rad_position, X_train_phi, X_train_theta)"
      ],
      "metadata": {
        "id": "e5rA7r6VJzF9"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the model\n"
      ],
      "metadata": {
        "id": "82LnNwwvL7Ys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# map_pion = extract_neighborhood_map(candidate_positions = X_test[\"X_test_pion_candidates\"], mip_positions = X_test[\"X_test_mip_position\"], neighborhood_size = 40, map_size = 40)\n",
        "\n",
        "\n",
        "# # map_sample = map_pion[0]\n",
        "\n",
        "# # # Plot the map\n",
        "# # plt.imshow(map_sample, cmap='gray')\n",
        "# # plt.colorbar()\n",
        "# # plt.title(\"Neighborhood Map for num_samples = 1\")\n",
        "# # plt.xlabel(\"X-axis\")\n",
        "# # plt.ylabel(\"Y-axis\")\n",
        "# # plt.show()\n",
        "\n",
        "# map_pion2 = extract_neighborhood_map(candidate_positions = X_test[\"X_test_kaon_candidates\"], mip_positions = X_test[\"X_test_mip_position\"], neighborhood_size = 40, map_size = 40)\n",
        "\n",
        "\n",
        "# # #map_sample = map_pion[0]\n",
        "\n",
        "# # # Plot the map\n",
        "# # plt.imshow(map_sample, cmap='gray')\n",
        "# # plt.colorbar()\n",
        "# # plt.title(\"Neighborhood Map for num_samples = 1\")\n",
        "# # plt.xlabel(\"X-axis\")\n",
        "# # plt.ylabel(\"Y-axis\")\n",
        "# # plt.show()\n",
        "# map_pion = extract_neighborhood_map(candidate_positions = X_test[\"X_test_proton_candidates\"], mip_positions = X_test[\"X_test_mip_position\"], neighborhood_size = 40, map_size = 40)\n",
        "\n",
        "\n",
        "# #map_sample = map_pion[0]\n",
        "mask = [1,1,1,1]\n",
        "classifier.train(X_train = X_train, X_test = X_test, y_train = y_train, y_test = y_test, mask = mask)\n"
      ],
      "metadata": {
        "id": "X4MLTLJX2mA1",
        "outputId": "789c1c3a-d948-4639-add6-df18728a354b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fields in the first vector of X_train:\n",
            "X_train_pion_candidates shape: (164, 84, 7)\n",
            "X_train_kaon_candidates shape: (164, 84, 7)\n",
            "X_train_proton_candidates shape: (164, 84, 7)\n",
            "X_train_momentum shape: (164,)\n",
            "X_train_refractive_index shape: (164,)\n",
            "X_train_phi shape: (164,)\n",
            "X_train_theta shape: (164,)\n",
            "X_train_mip_position shape: (164, 2)\n",
            "X_train_rad_position shape: (164, 2)\n",
            "X_train_map_pion shape: (164, 84, 7)\n",
            "X_train_map_kaon shape: (164, 84, 7)\n",
            "X_train_map_proton shape: (164, 84, 7)\n",
            "\n",
            "Fields in the first vector of X_test:\n",
            "X_test_pion_candidates shape: (42, 84, 7)\n",
            "X_test_kaon_candidates shape: (42, 84, 7)\n",
            "X_test_proton_candidates shape: (42, 84, 7)\n",
            "X_test_momentum shape: (42,)\n",
            "X_test_refractive_index shape: (42,)\n",
            "X_test_phi shape: (42,)\n",
            "X_test_theta shape: (42,)\n",
            "X_test_mip_position shape: (42, 2)\n",
            "X_test_rad_position shape: (42, 2)\n",
            "X_test_map_pion shape: (42, 84, 7)\n",
            "X_test_map_kaon shape: (42, 84, 7)\n",
            "X_test_map_proton shape: (42, 84, 7)\n",
            "l1l2_weights_mip shape (1, 10, 3)\n",
            "mapplying inputs\n",
            "now create ip shapes pion_input_shape\n",
            "input1 = KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name='phi_shape'), name='phi_shape', description=\"created by layer 'phi_shape'\"), input_shape = (None, 1)\n",
            "input2 = KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name='theta_shape'), name='theta_shape', description=\"created by layer 'theta_shape'\"), input_shape = (None, 1)\n",
            "input3 = KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name='refractive_index_input'), name='refractive_index_input', description=\"created by layer 'refractive_index_input'\"), input_shape = (None, 1)\n",
            "input4 = KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name='momentum_input'), name='momentum_input', description=\"created by layer 'momentum_input'\"), input_shape = (None, 1)\n",
            "input5 = KerasTensor(type_spec=TensorSpec(shape=(None, 2), dtype=tf.float32, name='rad_position_input'), name='rad_position_input', description=\"created by layer 'rad_position_input'\"), input_shape = (None, 2)\n",
            "input6 = KerasTensor(type_spec=TensorSpec(shape=(None, 2), dtype=tf.float32, name='mip_position_input'), name='mip_position_input', description=\"created by layer 'mip_position_input'\"), input_shape = (None, 2)\n",
            "now concat scalar 2D\n",
            "now call extract_neighborhood_map\n",
            "now assembple ip\n",
            "now cnn_pion_model call W IP\n",
            "now concat_cnn done \n",
            "now concat all\n",
            "concat_layers1 : layer = KerasTensor(type_spec=TensorSpec(shape=(None, 43008), dtype=tf.float32, name=None), name='flatpion/Reshape:0', description=\"created by layer 'flatpion'\"), output_shape = (None, 43008), layer_name = flatpion/Reshape:0\n",
            "concat_layers2 : layer = KerasTensor(type_spec=TensorSpec(shape=(None, 43008), dtype=tf.float32, name=None), name='flatkaon/Reshape:0', description=\"created by layer 'flatkaon'\"), output_shape = (None, 43008), layer_name = flatkaon/Reshape:0\n",
            "concat_layers3 : layer = KerasTensor(type_spec=TensorSpec(shape=(None, 43008), dtype=tf.float32, name=None), name='flatproton/Reshape:0', description=\"created by layer 'flatproton'\"), output_shape = (None, 43008), layer_name = flatproton/Reshape:0\n",
            "\n",
            "\n",
            "Concatenating layers by : concat = concatenate(concat_layers)\n",
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " phi_shape (InputLayer)         [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " theta_shape (InputLayer)       [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " refractive_index_input (InputL  [(None, 1)]         0           []                               \n",
            " ayer)                                                                                            \n",
            "                                                                                                  \n",
            " momentum_input (InputLayer)    [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " rad_position_input (InputLayer  [(None, 2)]         0           []                               \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " mip_position_input (InputLayer  [(None, 2)]         0           []                               \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " tf.identity_34 (TFOpLambda)    (None, 1)            0           ['phi_shape[0][0]']              \n",
            "                                                                                                  \n",
            " tf.identity_35 (TFOpLambda)    (None, 1)            0           ['theta_shape[0][0]']            \n",
            "                                                                                                  \n",
            " tf.identity_31 (TFOpLambda)    (None, 1)            0           ['refractive_index_input[0][0]'] \n",
            "                                                                                                  \n",
            " tf.identity_30 (TFOpLambda)    (None, 1)            0           ['momentum_input[0][0]']         \n",
            "                                                                                                  \n",
            " tf.identity_33 (TFOpLambda)    (None, 2)            0           ['rad_position_input[0][0]']     \n",
            "                                                                                                  \n",
            " tf.identity_32 (TFOpLambda)    (None, 2)            0           ['mip_position_input[0][0]']     \n",
            "                                                                                                  \n",
            " dense_phi_1 (Dense)            (None, 1024)         2048        ['tf.identity_34[0][0]']         \n",
            "                                                                                                  \n",
            " dense_theta_1 (Dense)          (None, 1024)         2048        ['tf.identity_35[0][0]']         \n",
            "                                                                                                  \n",
            " dense_ref_index_1 (Dense)      (None, 1024)         2048        ['tf.identity_31[0][0]']         \n",
            "                                                                                                  \n",
            " dense_momentum_1 (Dense)       (None, 1024)         2048        ['tf.identity_30[0][0]']         \n",
            "                                                                                                  \n",
            " dense_rad_position_1 (Dense)   (None, 1024)         3072        ['tf.identity_33[0][0]']         \n",
            "                                                                                                  \n",
            " dense_mip_pos_1 (Dense)        (None, 512)          1536        ['tf.identity_32[0][0]']         \n",
            "                                                                                                  \n",
            " bn_phi_1 (BatchNormalization)  (None, 1024)         4096        ['dense_phi_1[0][0]']            \n",
            "                                                                                                  \n",
            " bn_theta_1 (BatchNormalization  (None, 1024)        4096        ['dense_theta_1[0][0]']          \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " bn_ref_index_1 (BatchNormaliza  (None, 1024)        4096        ['dense_ref_index_1[0][0]']      \n",
            " tion)                                                                                            \n",
            "                                                                                                  \n",
            " bn_momentum_1 (BatchNormalizat  (None, 1024)        4096        ['dense_momentum_1[0][0]']       \n",
            " ion)                                                                                             \n",
            "                                                                                                  \n",
            " bn_rad_position_1 (BatchNormal  (None, 1024)        4096        ['dense_rad_position_1[0][0]']   \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " bn_mip_pos_1 (BatchNormalizati  (None, 512)         2048        ['dense_mip_pos_1[0][0]']        \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " leakyrelu_phi_1 (LeakyReLU)    (None, 1024)         0           ['bn_phi_1[0][0]']               \n",
            "                                                                                                  \n",
            " leakyrelu_theta_1 (LeakyReLU)  (None, 1024)         0           ['bn_theta_1[0][0]']             \n",
            "                                                                                                  \n",
            " leakyrelu_ref_index_1 (LeakyRe  (None, 1024)        0           ['bn_ref_index_1[0][0]']         \n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " leakyrelu_momentum_1 (LeakyReL  (None, 1024)        0           ['bn_momentum_1[0][0]']          \n",
            " U)                                                                                               \n",
            "                                                                                                  \n",
            " leakyrelu_rad_position_1 (Leak  (None, 1024)        0           ['bn_rad_position_1[0][0]']      \n",
            " yReLU)                                                                                           \n",
            "                                                                                                  \n",
            " leakyrelu_mip_pos_1 (LeakyReLU  (None, 512)         0           ['bn_mip_pos_1[0][0]']           \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " dropout_phi_1 (Dropout)        (None, 1024)         0           ['leakyrelu_phi_1[0][0]']        \n",
            "                                                                                                  \n",
            " dropout_theta_1 (Dropout)      (None, 1024)         0           ['leakyrelu_theta_1[0][0]']      \n",
            "                                                                                                  \n",
            " dropout_ref_index_1 (Dropout)  (None, 1024)         0           ['leakyrelu_ref_index_1[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_momentum_1 (Dropout)   (None, 1024)         0           ['leakyrelu_momentum_1[0][0]']   \n",
            "                                                                                                  \n",
            " dropout_rad_position_1 (Dropou  (None, 1024)        0           ['leakyrelu_rad_position_1[0][0]'\n",
            " t)                                                              ]                                \n",
            "                                                                                                  \n",
            " dropout_mip_pos_1 (Dropout)    (None, 512)          0           ['leakyrelu_mip_pos_1[0][0]']    \n",
            "                                                                                                  \n",
            " dense_phi_2 (Dense)            (None, 512)          524800      ['dropout_phi_1[0][0]']          \n",
            "                                                                                                  \n",
            " dense_theta_2 (Dense)          (None, 512)          524800      ['dropout_theta_1[0][0]']        \n",
            "                                                                                                  \n",
            " dense_ref_index_2 (Dense)      (None, 512)          524800      ['dropout_ref_index_1[0][0]']    \n",
            "                                                                                                  \n",
            " dense_momentum_2 (Dense)       (None, 512)          524800      ['dropout_momentum_1[0][0]']     \n",
            "                                                                                                  \n",
            " dense_rad_position_2 (Dense)   (None, 512)          524800      ['dropout_rad_position_1[0][0]'] \n",
            "                                                                                                  \n",
            " dense_mip_pos_2 (Dense)        (None, 256)          131328      ['dropout_mip_pos_1[0][0]']      \n",
            "                                                                                                  \n",
            " bn_phi_2 (BatchNormalization)  (None, 512)          2048        ['dense_phi_2[0][0]']            \n",
            "                                                                                                  \n",
            " bn_theta_2 (BatchNormalization  (None, 512)         2048        ['dense_theta_2[0][0]']          \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " bn_ref_index_2 (BatchNormaliza  (None, 512)         2048        ['dense_ref_index_2[0][0]']      \n",
            " tion)                                                                                            \n",
            "                                                                                                  \n",
            " bn_momentum_2 (BatchNormalizat  (None, 512)         2048        ['dense_momentum_2[0][0]']       \n",
            " ion)                                                                                             \n",
            "                                                                                                  \n",
            " bn_rad_position_2 (BatchNormal  (None, 512)         2048        ['dense_rad_position_2[0][0]']   \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " bn_mip_pos_2 (BatchNormalizati  (None, 256)         1024        ['dense_mip_pos_2[0][0]']        \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " leakyrelu_phi_2 (LeakyReLU)    (None, 512)          0           ['bn_phi_2[0][0]']               \n",
            "                                                                                                  \n",
            " leakyrelu_theta_2 (LeakyReLU)  (None, 512)          0           ['bn_theta_2[0][0]']             \n",
            "                                                                                                  \n",
            " leakyrelu_ref_index_2 (LeakyRe  (None, 512)         0           ['bn_ref_index_2[0][0]']         \n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " leakyrelu_momentum_2 (LeakyReL  (None, 512)         0           ['bn_momentum_2[0][0]']          \n",
            " U)                                                                                               \n",
            "                                                                                                  \n",
            " leakyrelu_rad_position_2 (Leak  (None, 512)         0           ['bn_rad_position_2[0][0]']      \n",
            " yReLU)                                                                                           \n",
            "                                                                                                  \n",
            " leakyrelu_mip_pos_2 (LeakyReLU  (None, 256)         0           ['bn_mip_pos_2[0][0]']           \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " dropout_phi_2 (Dropout)        (None, 512)          0           ['leakyrelu_phi_2[0][0]']        \n",
            "                                                                                                  \n",
            " dropout_theta_2 (Dropout)      (None, 512)          0           ['leakyrelu_theta_2[0][0]']      \n",
            "                                                                                                  \n",
            " dropout_ref_index_2 (Dropout)  (None, 512)          0           ['leakyrelu_ref_index_2[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_momentum_2 (Dropout)   (None, 512)          0           ['leakyrelu_momentum_2[0][0]']   \n",
            "                                                                                                  \n",
            " dropout_rad_position_2 (Dropou  (None, 512)         0           ['leakyrelu_rad_position_2[0][0]'\n",
            " t)                                                              ]                                \n",
            "                                                                                                  \n",
            " dropout_mip_pos_2 (Dropout)    (None, 256)          0           ['leakyrelu_mip_pos_2[0][0]']    \n",
            "                                                                                                  \n",
            " concat_scalars_2D (Concatenate  (None, 2816)        0           ['dropout_phi_2[0][0]',          \n",
            " )                                                                'dropout_theta_2[0][0]',        \n",
            "                                                                  'dropout_ref_index_2[0][0]',    \n",
            "                                                                  'dropout_momentum_2[0][0]',     \n",
            "                                                                  'dropout_rad_position_2[0][0]', \n",
            "                                                                  'dropout_mip_pos_2[0][0]']      \n",
            "                                                                                                  \n",
            " concat_scalars_all (Concatenat  (None, 2816)        0           ['concat_scalars_2D[0][0]']      \n",
            " e)                                                                                               \n",
            "                                                                                                  \n",
            " dense_16 (Dense)               (None, 8)            22536       ['concat_scalars_all[0][0]']     \n",
            "                                                                                                  \n",
            " dropout_11 (Dropout)           (None, 8)            0           ['dense_16[0][0]']               \n",
            "                                                                                                  \n",
            " dense_17 (Dense)               (None, 3)            27          ['dropout_11[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,824,483\n",
            "Trainable params: 2,807,587\n",
            "Non-trainable params: 16,896\n",
            "__________________________________________________________________________________________________\n",
            "end model.summary()\n",
            "Epoch 1/400\n",
            "6/6 [==============================] - 15s 357ms/step - loss: 211.5081 - accuracy: 0.1524 - val_loss: 212.7413 - val_accuracy: 0.1667 - lr: 0.0000e+00\n",
            "Epoch 2/400\n",
            "6/6 [==============================] - 1s 102ms/step - loss: 211.4371 - accuracy: 0.1463 - val_loss: 212.1215 - val_accuracy: 0.1667 - lr: 3.4014e-06\n",
            "Epoch 3/400\n",
            "6/6 [==============================] - 1s 87ms/step - loss: 211.2089 - accuracy: 0.2073 - val_loss: 211.5939 - val_accuracy: 0.1667 - lr: 6.8027e-06\n",
            "Epoch 4/400\n",
            "6/6 [==============================] - 1s 86ms/step - loss: 210.9794 - accuracy: 0.1829 - val_loss: 211.1081 - val_accuracy: 0.1667 - lr: 1.0204e-05\n",
            "Epoch 5/400\n",
            "6/6 [==============================] - 1s 86ms/step - loss: 210.5437 - accuracy: 0.2378 - val_loss: 210.5462 - val_accuracy: 0.1667 - lr: 1.3605e-05\n",
            "Epoch 6/400\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 210.1797 - accuracy: 0.2134 - val_loss: 209.9339 - val_accuracy: 0.1667 - lr: 1.7007e-05\n",
            "Epoch 7/400\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 209.5249 - accuracy: 0.2500 - val_loss: 209.2633 - val_accuracy: 0.1667 - lr: 2.0408e-05\n",
            "Epoch 8/400\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 208.8421 - accuracy: 0.3415 - val_loss: 208.5271 - val_accuracy: 0.1667 - lr: 2.3810e-05\n",
            "Epoch 9/400\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 208.0309 - accuracy: 0.3415 - val_loss: 207.6847 - val_accuracy: 0.1905 - lr: 2.7211e-05\n",
            "Epoch 10/400\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 207.2780 - accuracy: 0.3841 - val_loss: 206.7366 - val_accuracy: 0.3095 - lr: 3.0612e-05\n",
            "Epoch 11/400\n",
            "6/6 [==============================] - 1s 99ms/step - loss: 206.2263 - accuracy: 0.4695 - val_loss: 205.7073 - val_accuracy: 0.2857 - lr: 3.4014e-05\n",
            "Epoch 12/400\n",
            "6/6 [==============================] - 1s 88ms/step - loss: 205.1602 - accuracy: 0.5671 - val_loss: 204.5945 - val_accuracy: 0.4286 - lr: 3.7415e-05\n",
            "Epoch 13/400\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 204.0658 - accuracy: 0.5854 - val_loss: 203.4099 - val_accuracy: 0.5714 - lr: 4.0816e-05\n",
            "Epoch 14/400\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 202.8975 - accuracy: 0.6098 - val_loss: 202.1349 - val_accuracy: 0.6667 - lr: 4.4218e-05\n",
            "Epoch 15/400\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 201.5096 - accuracy: 0.6951 - val_loss: 200.7696 - val_accuracy: 0.7143 - lr: 4.7619e-05\n",
            "Epoch 16/400\n",
            "6/6 [==============================] - 1s 95ms/step - loss: 200.1411 - accuracy: 0.6402 - val_loss: 199.3207 - val_accuracy: 0.7381 - lr: 5.1020e-05\n",
            "Epoch 17/400\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 198.6198 - accuracy: 0.7500 - val_loss: 197.7825 - val_accuracy: 0.7381 - lr: 5.4422e-05\n",
            "Epoch 18/400\n",
            "6/6 [==============================] - 1s 87ms/step - loss: 197.0717 - accuracy: 0.7561 - val_loss: 196.1479 - val_accuracy: 0.7857 - lr: 5.7823e-05\n",
            "Epoch 19/400\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 195.3708 - accuracy: 0.8354 - val_loss: 194.4189 - val_accuracy: 0.7857 - lr: 6.1224e-05\n",
            "Epoch 20/400\n",
            "6/6 [==============================] - 1s 100ms/step - loss: 193.6142 - accuracy: 0.8110 - val_loss: 192.6011 - val_accuracy: 0.7857 - lr: 6.4626e-05\n",
            "Epoch 21/400\n",
            "6/6 [==============================] - 1s 133ms/step - loss: 191.6601 - accuracy: 0.8415 - val_loss: 190.6833 - val_accuracy: 0.7857 - lr: 6.8027e-05\n",
            "Epoch 22/400\n",
            "6/6 [==============================] - 1s 135ms/step - loss: 189.7371 - accuracy: 0.8659 - val_loss: 188.6753 - val_accuracy: 0.7857 - lr: 7.1429e-05\n",
            "Epoch 23/400\n",
            "6/6 [==============================] - 1s 141ms/step - loss: 187.7293 - accuracy: 0.8232 - val_loss: 186.5877 - val_accuracy: 0.7857 - lr: 7.4830e-05\n",
            "Epoch 24/400\n",
            "6/6 [==============================] - 1s 145ms/step - loss: 185.5773 - accuracy: 0.8232 - val_loss: 184.4125 - val_accuracy: 0.7857 - lr: 7.8231e-05\n",
            "Epoch 25/400\n",
            "6/6 [==============================] - 1s 142ms/step - loss: 183.3845 - accuracy: 0.8476 - val_loss: 182.1507 - val_accuracy: 0.7857 - lr: 8.1633e-05\n",
            "Epoch 26/400\n",
            "6/6 [==============================] - 1s 123ms/step - loss: 181.1514 - accuracy: 0.8232 - val_loss: 179.8018 - val_accuracy: 0.7857 - lr: 8.5034e-05\n",
            "Epoch 27/400\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 178.7691 - accuracy: 0.8598 - val_loss: 177.3751 - val_accuracy: 0.7857 - lr: 8.8435e-05\n",
            "Epoch 28/400\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 176.2851 - accuracy: 0.8476 - val_loss: 174.8788 - val_accuracy: 0.7857 - lr: 9.1837e-05\n",
            "Epoch 29/400\n",
            "6/6 [==============================] - 1s 103ms/step - loss: 173.8211 - accuracy: 0.8415 - val_loss: 172.2998 - val_accuracy: 0.7857 - lr: 9.5238e-05\n",
            "Epoch 30/400\n",
            "6/6 [==============================] - 1s 97ms/step - loss: 171.0927 - accuracy: 0.8720 - val_loss: 169.6395 - val_accuracy: 0.7857 - lr: 9.8639e-05\n",
            "Epoch 31/400\n",
            "6/6 [==============================] - 1s 104ms/step - loss: 168.4764 - accuracy: 0.8598 - val_loss: 166.9029 - val_accuracy: 0.7857 - lr: 1.0204e-04\n",
            "Epoch 32/400\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 165.6801 - accuracy: 0.8476 - val_loss: 164.0940 - val_accuracy: 0.7857 - lr: 1.0544e-04\n",
            "Epoch 33/400\n",
            "6/6 [==============================] - 1s 101ms/step - loss: 162.8338 - accuracy: 0.8537 - val_loss: 161.2156 - val_accuracy: 0.7857 - lr: 1.0884e-04\n",
            "Epoch 34/400\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 159.9374 - accuracy: 0.8780 - val_loss: 158.2786 - val_accuracy: 0.7857 - lr: 1.1224e-04\n",
            "Epoch 35/400\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 156.9286 - accuracy: 0.8598 - val_loss: 155.2740 - val_accuracy: 0.7857 - lr: 1.1565e-04\n",
            "Epoch 36/400\n",
            "6/6 [==============================] - 1s 98ms/step - loss: 153.9716 - accuracy: 0.8598 - val_loss: 152.2033 - val_accuracy: 0.7857 - lr: 1.1905e-04\n",
            "Epoch 37/400\n",
            "6/6 [==============================] - 1s 96ms/step - loss: 150.8539 - accuracy: 0.8415 - val_loss: 149.0757 - val_accuracy: 0.7857 - lr: 1.2245e-04\n",
            "Epoch 38/400\n",
            "6/6 [==============================] - 1s 95ms/step - loss: 147.6648 - accuracy: 0.8598 - val_loss: 145.8900 - val_accuracy: 0.7857 - lr: 1.2585e-04\n",
            "Epoch 39/400\n",
            "6/6 [==============================] - 1s 93ms/step - loss: 144.4778 - accuracy: 0.8537 - val_loss: 142.6534 - val_accuracy: 0.7857 - lr: 1.2925e-04\n",
            "Epoch 40/400\n",
            "6/6 [==============================] - 1s 93ms/step - loss: 141.2760 - accuracy: 0.8537 - val_loss: 139.3683 - val_accuracy: 0.7857 - lr: 1.3265e-04\n",
            "Epoch 41/400\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 137.8887 - accuracy: 0.8720 - val_loss: 136.0375 - val_accuracy: 0.7857 - lr: 1.3605e-04\n",
            "Epoch 42/400\n",
            "6/6 [==============================] - 1s 102ms/step - loss: 134.6098 - accuracy: 0.8293 - val_loss: 132.6662 - val_accuracy: 0.7857 - lr: 1.3946e-04\n",
            "Epoch 43/400\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 131.2085 - accuracy: 0.8537 - val_loss: 129.2695 - val_accuracy: 0.7857 - lr: 1.4286e-04\n",
            "Epoch 44/400\n",
            "6/6 [==============================] - 1s 139ms/step - loss: 127.7843 - accuracy: 0.8293 - val_loss: 125.8540 - val_accuracy: 0.7857 - lr: 1.4626e-04\n",
            "Epoch 45/400\n",
            "6/6 [==============================] - 1s 143ms/step - loss: 124.3042 - accuracy: 0.8598 - val_loss: 122.3934 - val_accuracy: 0.7857 - lr: 1.4966e-04\n",
            "Epoch 46/400\n",
            "6/6 [==============================] - 1s 131ms/step - loss: 120.8388 - accuracy: 0.8780 - val_loss: 118.9005 - val_accuracy: 0.7857 - lr: 1.5306e-04\n",
            "Epoch 47/400\n",
            "6/6 [==============================] - 1s 142ms/step - loss: 117.3769 - accuracy: 0.8598 - val_loss: 115.3883 - val_accuracy: 0.7857 - lr: 1.5646e-04\n",
            "Epoch 48/400\n",
            "6/6 [==============================] - 1s 139ms/step - loss: 113.8365 - accuracy: 0.8598 - val_loss: 111.8627 - val_accuracy: 0.7857 - lr: 1.5986e-04\n",
            "Epoch 49/400\n",
            "6/6 [==============================] - 1s 122ms/step - loss: 110.2858 - accuracy: 0.8720 - val_loss: 108.3211 - val_accuracy: 0.7857 - lr: 1.6327e-04\n",
            "Epoch 50/400\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 106.7981 - accuracy: 0.8354 - val_loss: 104.7774 - val_accuracy: 0.7857 - lr: 1.6667e-04\n",
            "Epoch 51/400\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 103.2593 - accuracy: 0.8780 - val_loss: 101.3219 - val_accuracy: 0.7857 - lr: 1.6667e-04\n",
            "Epoch 52/400\n",
            "6/6 [==============================] - 1s 101ms/step - loss: 99.8685 - accuracy: 0.8293 - val_loss: 97.9978 - val_accuracy: 0.7857 - lr: 1.6501e-04\n",
            "Epoch 53/400\n",
            "6/6 [==============================] - 1s 88ms/step - loss: 96.5578 - accuracy: 0.8415 - val_loss: 94.7260 - val_accuracy: 0.7857 - lr: 1.6336e-04\n",
            "Epoch 54/400\n",
            "6/6 [==============================] - 1s 93ms/step - loss: 93.3213 - accuracy: 0.8415 - val_loss: 91.5468 - val_accuracy: 0.7857 - lr: 1.6173e-04\n",
            "Epoch 55/400\n",
            "6/6 [==============================] - 1s 86ms/step - loss: 90.1755 - accuracy: 0.8537 - val_loss: 88.4738 - val_accuracy: 0.7857 - lr: 1.6012e-04\n",
            "Epoch 56/400\n",
            "6/6 [==============================] - 1s 100ms/step - loss: 87.1376 - accuracy: 0.8293 - val_loss: 85.4871 - val_accuracy: 0.7857 - lr: 1.5852e-04\n",
            "Epoch 57/400\n",
            "6/6 [==============================] - 1s 99ms/step - loss: 84.1635 - accuracy: 0.8598 - val_loss: 82.6034 - val_accuracy: 0.7857 - lr: 1.5694e-04\n",
            "Epoch 58/400\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 81.3004 - accuracy: 0.8659 - val_loss: 79.8249 - val_accuracy: 0.7857 - lr: 1.5538e-04\n",
            "Epoch 59/400\n",
            "6/6 [==============================] - 1s 95ms/step - loss: 78.5510 - accuracy: 0.8415 - val_loss: 77.1143 - val_accuracy: 0.7857 - lr: 1.5383e-04\n",
            "Epoch 60/400\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 75.8269 - accuracy: 0.8780 - val_loss: 74.4858 - val_accuracy: 0.7857 - lr: 1.5230e-04\n",
            "Epoch 61/400\n",
            "6/6 [==============================] - 1s 100ms/step - loss: 73.2326 - accuracy: 0.8659 - val_loss: 71.9289 - val_accuracy: 0.7857 - lr: 1.5078e-04\n",
            "Epoch 62/400\n",
            "6/6 [==============================] - 1s 99ms/step - loss: 70.7292 - accuracy: 0.8415 - val_loss: 69.4673 - val_accuracy: 0.7857 - lr: 1.4927e-04\n",
            "Epoch 63/400\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 68.3476 - accuracy: 0.8293 - val_loss: 67.1481 - val_accuracy: 0.7857 - lr: 1.4779e-04\n",
            "Epoch 64/400\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 66.0664 - accuracy: 0.8232 - val_loss: 64.8856 - val_accuracy: 0.7857 - lr: 1.4631e-04\n",
            "Epoch 65/400\n",
            "6/6 [==============================] - 1s 102ms/step - loss: 63.7466 - accuracy: 0.8780 - val_loss: 62.6573 - val_accuracy: 0.7857 - lr: 1.4486e-04\n",
            "Epoch 66/400\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 61.5891 - accuracy: 0.8598 - val_loss: 60.5309 - val_accuracy: 0.7619 - lr: 1.4341e-04\n",
            "Epoch 67/400\n",
            "6/6 [==============================] - 1s 108ms/step - loss: 59.4323 - accuracy: 0.8720 - val_loss: 58.4427 - val_accuracy: 0.7857 - lr: 1.4198e-04\n",
            "Epoch 68/400\n",
            "6/6 [==============================] - 1s 137ms/step - loss: 57.3132 - accuracy: 0.8963 - val_loss: 56.4129 - val_accuracy: 0.7857 - lr: 1.4057e-04\n",
            "Epoch 69/400\n",
            "6/6 [==============================] - 1s 133ms/step - loss: 55.3816 - accuracy: 0.8659 - val_loss: 54.4634 - val_accuracy: 0.7857 - lr: 1.3916e-04\n",
            "Epoch 70/400\n",
            "6/6 [==============================] - 1s 136ms/step - loss: 53.3744 - accuracy: 0.8963 - val_loss: 52.5725 - val_accuracy: 0.7857 - lr: 1.3778e-04\n",
            "Epoch 71/400\n",
            "6/6 [==============================] - 1s 138ms/step - loss: 51.5138 - accuracy: 0.8963 - val_loss: 50.7499 - val_accuracy: 0.7857 - lr: 1.3640e-04\n",
            "Epoch 72/400\n",
            "6/6 [==============================] - 1s 146ms/step - loss: 49.8462 - accuracy: 0.8476 - val_loss: 48.9988 - val_accuracy: 0.7857 - lr: 1.3504e-04\n",
            "Epoch 73/400\n",
            "6/6 [==============================] - 1s 107ms/step - loss: 48.0632 - accuracy: 0.8659 - val_loss: 47.3177 - val_accuracy: 0.7857 - lr: 1.3370e-04\n",
            "Epoch 74/400\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 46.4083 - accuracy: 0.8293 - val_loss: 45.7358 - val_accuracy: 0.7857 - lr: 1.3237e-04\n",
            "Epoch 75/400\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 44.8464 - accuracy: 0.8720 - val_loss: 44.1699 - val_accuracy: 0.7857 - lr: 1.3105e-04\n",
            "Epoch 76/400\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 43.2832 - accuracy: 0.8841 - val_loss: 42.6299 - val_accuracy: 0.7857 - lr: 1.2974e-04\n",
            "Epoch 77/400\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 41.7439 - accuracy: 0.8598 - val_loss: 41.1441 - val_accuracy: 0.7857 - lr: 1.2845e-04\n",
            "Epoch 78/400\n",
            "6/6 [==============================] - 1s 95ms/step - loss: 40.3260 - accuracy: 0.8293 - val_loss: 39.7264 - val_accuracy: 0.7857 - lr: 1.2717e-04\n",
            "Epoch 79/400\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 38.9072 - accuracy: 0.8841 - val_loss: 38.3626 - val_accuracy: 0.7857 - lr: 1.2590e-04\n",
            "Epoch 80/400\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 37.5643 - accuracy: 0.8780 - val_loss: 37.0596 - val_accuracy: 0.7857 - lr: 1.2464e-04\n",
            "Epoch 81/400\n",
            "6/6 [==============================] - 1s 93ms/step - loss: 36.2809 - accuracy: 0.8902 - val_loss: 35.8512 - val_accuracy: 0.7857 - lr: 1.2340e-04\n",
            "Epoch 82/400\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 35.1006 - accuracy: 0.8476 - val_loss: 34.6468 - val_accuracy: 0.7857 - lr: 1.2217e-04\n",
            "Epoch 83/400\n",
            "6/6 [==============================] - 1s 93ms/step - loss: 33.8635 - accuracy: 0.8598 - val_loss: 33.4781 - val_accuracy: 0.7857 - lr: 1.2095e-04\n",
            "Epoch 84/400\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 32.7312 - accuracy: 0.8598 - val_loss: 32.3592 - val_accuracy: 0.7857 - lr: 1.1975e-04\n",
            "Epoch 85/400\n",
            "6/6 [==============================] - 1s 98ms/step - loss: 31.6368 - accuracy: 0.8780 - val_loss: 31.3156 - val_accuracy: 0.7857 - lr: 1.1855e-04\n",
            "Epoch 86/400\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 30.5902 - accuracy: 0.8659 - val_loss: 30.3632 - val_accuracy: 0.7857 - lr: 1.1737e-04\n",
            "Epoch 87/400\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 29.6489 - accuracy: 0.8354 - val_loss: 29.3779 - val_accuracy: 0.7857 - lr: 1.1620e-04\n",
            "Epoch 88/400\n",
            "6/6 [==============================] - 1s 93ms/step - loss: 28.6718 - accuracy: 0.8537 - val_loss: 28.4211 - val_accuracy: 0.7619 - lr: 1.1504e-04\n",
            "Epoch 89/400\n",
            "6/6 [==============================] - 1s 95ms/step - loss: 27.7528 - accuracy: 0.8354 - val_loss: 27.5195 - val_accuracy: 0.7619 - lr: 1.1390e-04\n",
            "Epoch 90/400\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 26.8164 - accuracy: 0.8780 - val_loss: 26.6396 - val_accuracy: 0.7857 - lr: 1.1276e-04\n",
            "Epoch 91/400\n",
            "6/6 [==============================] - 1s 111ms/step - loss: 26.0113 - accuracy: 0.8415 - val_loss: 25.8280 - val_accuracy: 0.7857 - lr: 1.1164e-04\n",
            "Epoch 92/400\n",
            "6/6 [==============================] - 1s 144ms/step - loss: 25.2044 - accuracy: 0.8780 - val_loss: 25.1212 - val_accuracy: 0.7857 - lr: 1.1052e-04\n",
            "Epoch 93/400\n",
            "6/6 [==============================] - 1s 133ms/step - loss: 24.4507 - accuracy: 0.8780 - val_loss: 24.4390 - val_accuracy: 0.7857 - lr: 1.0942e-04\n",
            "Epoch 94/400\n",
            "6/6 [==============================] - 1s 129ms/step - loss: 23.8462 - accuracy: 0.8232 - val_loss: 23.7143 - val_accuracy: 0.7857 - lr: 1.0833e-04\n",
            "Epoch 95/400\n",
            "6/6 [==============================] - 1s 140ms/step - loss: 23.1000 - accuracy: 0.8354 - val_loss: 23.0294 - val_accuracy: 0.8095 - lr: 1.0725e-04\n",
            "Epoch 96/400\n",
            "6/6 [==============================] - 1s 134ms/step - loss: 22.4129 - accuracy: 0.8780 - val_loss: 22.3874 - val_accuracy: 0.7857 - lr: 1.0618e-04\n",
            "Epoch 97/400\n",
            "6/6 [==============================] - 1s 122ms/step - loss: 21.8252 - accuracy: 0.8110 - val_loss: 21.8021 - val_accuracy: 0.7857 - lr: 1.0512e-04\n",
            "Epoch 98/400\n",
            "6/6 [==============================] - 1s 94ms/step - loss: 21.2329 - accuracy: 0.8476 - val_loss: 21.2233 - val_accuracy: 0.7857 - lr: 1.0408e-04\n",
            "Epoch 99/400\n",
            "6/6 [==============================] - 1s 93ms/step - loss: 20.6734 - accuracy: 0.8720 - val_loss: 20.6811 - val_accuracy: 0.7857 - lr: 1.0304e-04\n",
            "Epoch 100/400\n",
            "6/6 [==============================] - 1s 98ms/step - loss: 20.1110 - accuracy: 0.8720 - val_loss: 20.1590 - val_accuracy: 0.7857 - lr: 1.0201e-04\n",
            "Epoch 101/400\n",
            "6/6 [==============================] - 1s 94ms/step - loss: 19.6121 - accuracy: 0.8720 - val_loss: 19.6706 - val_accuracy: 0.7857 - lr: 1.0099e-04\n",
            "Epoch 102/400\n",
            "6/6 [==============================] - 1s 87ms/step - loss: 19.0912 - accuracy: 0.9085 - val_loss: 19.1854 - val_accuracy: 0.7857 - lr: 9.9987e-05\n",
            "Epoch 103/400\n",
            "6/6 [==============================] - 1s 97ms/step - loss: 18.6474 - accuracy: 0.8720 - val_loss: 18.7321 - val_accuracy: 0.7857 - lr: 9.8990e-05\n",
            "Epoch 104/400\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 18.1871 - accuracy: 0.8659 - val_loss: 18.3013 - val_accuracy: 0.7857 - lr: 9.8003e-05\n",
            "Epoch 105/400\n",
            "6/6 [==============================] - 1s 105ms/step - loss: 17.6861 - accuracy: 0.8780 - val_loss: 17.8621 - val_accuracy: 0.7857 - lr: 9.7026e-05\n",
            "Epoch 106/400\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 17.2863 - accuracy: 0.8780 - val_loss: 17.4729 - val_accuracy: 0.7857 - lr: 9.6059e-05\n",
            "Epoch 107/400\n",
            "6/6 [==============================] - 1s 105ms/step - loss: 16.9283 - accuracy: 0.8476 - val_loss: 17.1858 - val_accuracy: 0.7857 - lr: 9.5102e-05\n",
            "Epoch 108/400\n",
            "6/6 [==============================] - 1s 93ms/step - loss: 16.6637 - accuracy: 0.8963 - val_loss: 16.8639 - val_accuracy: 0.7857 - lr: 9.4154e-05\n",
            "Epoch 109/400\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 16.2832 - accuracy: 0.8659 - val_loss: 16.4898 - val_accuracy: 0.7857 - lr: 9.3215e-05\n",
            "Epoch 110/400\n",
            "6/6 [==============================] - 1s 94ms/step - loss: 15.9622 - accuracy: 0.8720 - val_loss: 16.1501 - val_accuracy: 0.7857 - lr: 9.2286e-05\n",
            "Epoch 111/400\n",
            "6/6 [==============================] - 1s 94ms/step - loss: 15.6847 - accuracy: 0.8293 - val_loss: 15.8865 - val_accuracy: 0.7857 - lr: 9.1366e-05\n",
            "Epoch 112/400\n",
            "6/6 [==============================] - 1s 98ms/step - loss: 15.4137 - accuracy: 0.8598 - val_loss: 15.6186 - val_accuracy: 0.7857 - lr: 9.0455e-05\n",
            "Epoch 113/400\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 15.0653 - accuracy: 0.8598 - val_loss: 15.2743 - val_accuracy: 0.7857 - lr: 8.9553e-05\n",
            "Epoch 114/400\n",
            "6/6 [==============================] - 1s 96ms/step - loss: 14.7440 - accuracy: 0.8841 - val_loss: 14.9209 - val_accuracy: 0.7857 - lr: 8.8661e-05\n",
            "Epoch 115/400\n",
            "6/6 [==============================] - 1s 127ms/step - loss: 14.4093 - accuracy: 0.8598 - val_loss: 14.6017 - val_accuracy: 0.7857 - lr: 8.7777e-05\n",
            "Epoch 116/400\n",
            "6/6 [==============================] - 1s 135ms/step - loss: 14.1384 - accuracy: 0.8476 - val_loss: 14.3218 - val_accuracy: 0.7857 - lr: 8.6902e-05\n",
            "Epoch 117/400\n",
            "6/6 [==============================] - 1s 134ms/step - loss: 13.8095 - accuracy: 0.9024 - val_loss: 14.0634 - val_accuracy: 0.7857 - lr: 8.6035e-05\n",
            "Epoch 118/400\n",
            "6/6 [==============================] - 1s 132ms/step - loss: 13.5537 - accuracy: 0.8720 - val_loss: 13.7853 - val_accuracy: 0.7857 - lr: 8.5178e-05\n",
            "Epoch 119/400\n",
            "6/6 [==============================] - 1s 136ms/step - loss: 13.3172 - accuracy: 0.8659 - val_loss: 13.5187 - val_accuracy: 0.7857 - lr: 8.4329e-05\n",
            "Epoch 120/400\n",
            "6/6 [==============================] - 1s 143ms/step - loss: 13.0464 - accuracy: 0.8720 - val_loss: 13.2597 - val_accuracy: 0.7857 - lr: 8.3488e-05\n",
            "Epoch 121/400\n",
            "6/6 [==============================] - 1s 108ms/step - loss: 12.7775 - accuracy: 0.8780 - val_loss: 13.0179 - val_accuracy: 0.7857 - lr: 8.2656e-05\n",
            "Epoch 122/400\n",
            "6/6 [==============================] - 1s 100ms/step - loss: 12.6035 - accuracy: 0.8659 - val_loss: 12.8222 - val_accuracy: 0.7857 - lr: 8.1832e-05\n",
            "Epoch 123/400\n",
            "6/6 [==============================] - 1s 94ms/step - loss: 12.3407 - accuracy: 0.8537 - val_loss: 12.6441 - val_accuracy: 0.7857 - lr: 8.1016e-05\n",
            "Epoch 124/400\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 12.1999 - accuracy: 0.8354 - val_loss: 12.5363 - val_accuracy: 0.7857 - lr: 8.0208e-05\n",
            "Epoch 125/400\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 12.0444 - accuracy: 0.8415 - val_loss: 12.3730 - val_accuracy: 0.7857 - lr: 7.9409e-05\n",
            "Epoch 126/400\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 11.8810 - accuracy: 0.8659 - val_loss: 12.2084 - val_accuracy: 0.7857 - lr: 7.8617e-05\n",
            "Epoch 127/400\n",
            "6/6 [==============================] - 1s 99ms/step - loss: 11.7226 - accuracy: 0.8780 - val_loss: 12.0365 - val_accuracy: 0.7857 - lr: 7.7833e-05\n",
            "Epoch 128/400\n",
            "6/6 [==============================] - 1s 108ms/step - loss: 11.4497 - accuracy: 0.8720 - val_loss: 11.7950 - val_accuracy: 0.7857 - lr: 7.7058e-05\n",
            "Epoch 129/400\n",
            "6/6 [==============================] - 1s 102ms/step - loss: 11.2318 - accuracy: 0.8902 - val_loss: 11.5586 - val_accuracy: 0.7857 - lr: 7.6289e-05\n",
            "Epoch 130/400\n",
            "6/6 [==============================] - 1s 95ms/step - loss: 11.0020 - accuracy: 0.8780 - val_loss: 11.3589 - val_accuracy: 0.7857 - lr: 7.5529e-05\n",
            "Epoch 131/400\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 10.9311 - accuracy: 0.8537 - val_loss: 11.2475 - val_accuracy: 0.7857 - lr: 7.4776e-05\n",
            "Epoch 132/400\n",
            "6/6 [==============================] - 1s 94ms/step - loss: 10.7589 - accuracy: 0.8232 - val_loss: 11.0627 - val_accuracy: 0.7857 - lr: 7.4031e-05\n",
            "Epoch 133/400\n",
            "6/6 [==============================] - 1s 132ms/step - loss: 10.5338 - accuracy: 0.8720 - val_loss: 10.8484 - val_accuracy: 0.7857 - lr: 7.3293e-05\n",
            "Epoch 134/400\n",
            "6/6 [==============================] - 1s 152ms/step - loss: 10.3511 - accuracy: 0.8780 - val_loss: 10.7103 - val_accuracy: 0.7857 - lr: 7.2562e-05\n",
            "Epoch 135/400\n",
            "6/6 [==============================] - 1s 184ms/step - loss: 10.2025 - accuracy: 0.8720 - val_loss: 10.6334 - val_accuracy: 0.7857 - lr: 7.1839e-05\n",
            "Epoch 136/400\n",
            "6/6 [==============================] - 1s 113ms/step - loss: 10.1237 - accuracy: 0.8415 - val_loss: 10.5633 - val_accuracy: 0.7857 - lr: 7.1122e-05\n",
            "Epoch 137/400\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 9.9756 - accuracy: 0.8720 - val_loss: 10.3712 - val_accuracy: 0.7857 - lr: 7.0413e-05\n",
            "Epoch 138/400\n",
            "6/6 [==============================] - 1s 133ms/step - loss: 9.8291 - accuracy: 0.8720 - val_loss: 10.2001 - val_accuracy: 0.7857 - lr: 6.9712e-05\n",
            "Epoch 139/400\n",
            "6/6 [==============================] - 1s 135ms/step - loss: 9.6944 - accuracy: 0.8476 - val_loss: 10.0742 - val_accuracy: 0.7857 - lr: 6.9017e-05\n",
            "Epoch 140/400\n",
            "6/6 [==============================] - 1s 127ms/step - loss: 9.5834 - accuracy: 0.8780 - val_loss: 9.9515 - val_accuracy: 0.7857 - lr: 6.8329e-05\n",
            "Epoch 141/400\n",
            "6/6 [==============================] - 1s 138ms/step - loss: 9.4293 - accuracy: 0.8659 - val_loss: 9.7761 - val_accuracy: 0.7857 - lr: 6.7647e-05\n",
            "Epoch 142/400\n",
            "6/6 [==============================] - 1s 95ms/step - loss: 9.3100 - accuracy: 0.8841 - val_loss: 9.6359 - val_accuracy: 0.7857 - lr: 6.6973e-05\n",
            "Epoch 143/400\n",
            "6/6 [==============================] - 1s 94ms/step - loss: 9.1256 - accuracy: 0.8841 - val_loss: 9.5120 - val_accuracy: 0.7857 - lr: 6.6305e-05\n",
            "Epoch 144/400\n",
            "6/6 [==============================] - 1s 94ms/step - loss: 9.0416 - accuracy: 0.8720 - val_loss: 9.4971 - val_accuracy: 0.7619 - lr: 6.5644e-05\n",
            "Epoch 145/400\n",
            "6/6 [==============================] - 1s 97ms/step - loss: 8.9599 - accuracy: 0.8415 - val_loss: 9.4412 - val_accuracy: 0.7857 - lr: 6.4990e-05\n",
            "Epoch 146/400\n",
            "6/6 [==============================] - 1s 95ms/step - loss: 8.8967 - accuracy: 0.8171 - val_loss: 9.3388 - val_accuracy: 0.7857 - lr: 6.4342e-05\n",
            "Epoch 147/400\n",
            "6/6 [==============================] - 1s 100ms/step - loss: 8.7583 - accuracy: 0.8598 - val_loss: 9.1865 - val_accuracy: 0.7857 - lr: 6.3701e-05\n",
            "Epoch 148/400\n",
            "6/6 [==============================] - 1s 103ms/step - loss: 8.6498 - accuracy: 0.8659 - val_loss: 9.0253 - val_accuracy: 0.7857 - lr: 6.3066e-05\n",
            "Epoch 149/400\n",
            "6/6 [==============================] - 1s 96ms/step - loss: 8.4877 - accuracy: 0.8476 - val_loss: 8.8934 - val_accuracy: 0.7857 - lr: 6.2437e-05\n",
            "Epoch 150/400\n",
            "6/6 [==============================] - 1s 94ms/step - loss: 8.3357 - accuracy: 0.8780 - val_loss: 8.7648 - val_accuracy: 0.7857 - lr: 6.1815e-05\n",
            "Epoch 151/400\n",
            "6/6 [==============================] - 1s 93ms/step - loss: 8.2044 - accuracy: 0.8841 - val_loss: 8.6458 - val_accuracy: 0.7857 - lr: 6.1198e-05\n",
            "Epoch 152/400\n",
            "6/6 [==============================] - 1s 96ms/step - loss: 8.1725 - accuracy: 0.8841 - val_loss: 8.5879 - val_accuracy: 0.7857 - lr: 6.0588e-05\n",
            "Epoch 153/400\n",
            "6/6 [==============================] - 1s 95ms/step - loss: 8.0968 - accuracy: 0.8476 - val_loss: 8.4822 - val_accuracy: 0.7857 - lr: 5.9984e-05\n",
            "Epoch 154/400\n",
            "6/6 [==============================] - 1s 103ms/step - loss: 7.9902 - accuracy: 0.8598 - val_loss: 8.3643 - val_accuracy: 0.7857 - lr: 5.9386e-05\n",
            "Epoch 155/400\n",
            "6/6 [==============================] - 1s 100ms/step - loss: 7.8685 - accuracy: 0.8476 - val_loss: 8.2968 - val_accuracy: 0.7857 - lr: 5.8794e-05\n",
            "Epoch 156/400\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 7.8109 - accuracy: 0.8720 - val_loss: 8.3420 - val_accuracy: 0.7857 - lr: 5.8208e-05\n",
            "Epoch 157/400\n",
            "6/6 [==============================] - 1s 95ms/step - loss: 7.8249 - accuracy: 0.8659 - val_loss: 8.2568 - val_accuracy: 0.7857 - lr: 5.7628e-05\n",
            "Epoch 158/400\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 7.7465 - accuracy: 0.8598 - val_loss: 8.1841 - val_accuracy: 0.7857 - lr: 5.7054e-05\n",
            "Epoch 159/400\n",
            "6/6 [==============================] - 1s 112ms/step - loss: 7.6752 - accuracy: 0.8841 - val_loss: 8.0844 - val_accuracy: 0.7857 - lr: 5.6485e-05\n",
            "Epoch 160/400\n",
            "6/6 [==============================] - 1s 134ms/step - loss: 7.5764 - accuracy: 0.8598 - val_loss: 7.9596 - val_accuracy: 0.7857 - lr: 5.5922e-05\n",
            "Epoch 161/400\n",
            "6/6 [==============================] - 1s 138ms/step - loss: 7.5406 - accuracy: 0.8780 - val_loss: 7.9419 - val_accuracy: 0.7857 - lr: 5.5364e-05\n",
            "Epoch 162/400\n",
            "6/6 [==============================] - 1s 137ms/step - loss: 7.4647 - accuracy: 0.8780 - val_loss: 7.9442 - val_accuracy: 0.7381 - lr: 5.4812e-05\n",
            "Epoch 163/400\n",
            "6/6 [==============================] - 1s 134ms/step - loss: 7.4422 - accuracy: 0.8415 - val_loss: 7.8301 - val_accuracy: 0.7857 - lr: 5.4266e-05\n",
            "Epoch 164/400\n",
            "6/6 [==============================] - 1s 140ms/step - loss: 7.3354 - accuracy: 0.8720 - val_loss: 7.7644 - val_accuracy: 0.7857 - lr: 5.3725e-05\n",
            "Epoch 165/400\n",
            "6/6 [==============================] - 1s 107ms/step - loss: 7.2452 - accuracy: 0.8659 - val_loss: 7.6884 - val_accuracy: 0.7857 - lr: 5.3189e-05\n",
            "Epoch 166/400\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 7.1460 - accuracy: 0.8598 - val_loss: 7.5823 - val_accuracy: 0.7857 - lr: 5.2659e-05\n",
            "Epoch 167/400\n",
            "6/6 [==============================] - 1s 100ms/step - loss: 7.0459 - accuracy: 0.8780 - val_loss: 7.4720 - val_accuracy: 0.7857 - lr: 5.2134e-05\n",
            "Epoch 168/400\n",
            "6/6 [==============================] - 1s 95ms/step - loss: 6.9398 - accuracy: 0.8659 - val_loss: 7.4234 - val_accuracy: 0.7857 - lr: 5.1615e-05\n",
            "Epoch 169/400\n",
            "6/6 [==============================] - 1s 98ms/step - loss: 6.8966 - accuracy: 0.8598 - val_loss: 7.3533 - val_accuracy: 0.7857 - lr: 5.1100e-05\n",
            "Epoch 170/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 6.7325 - accuracy: 0.9062"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bjFn4oJgEJnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoauWOieMnWE"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/eflatlan/CNN_PID/models_sacved/helper_functions.py\n",
        "from helper_functions import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvEulDjrHxkx"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/eflatlan/CNN_PID/models_sacved/plot_helper_functions.py\n",
        "from plot_helper_functions import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YFRCFeJn9kw3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfCwNWON7Qr3K876T6EvOX",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}