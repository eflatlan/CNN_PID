{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eflatlan/CNN_PID/blob/populateAll/newArc_segment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrPLFO_92Cvr",
        "outputId": "0ff87c56-ced3-4063-d931-a4671d1c15da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.22.4)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "!pip install h5py numpy\n",
        "\n",
        "import os\n",
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "_clF1C9YOo3R"
      },
      "outputs": [],
      "source": [
        "#!wget https://raw.githubusercontent.com/eflatlan/CNN_PID/dev_floatmap/helper_functions.py\n",
        "#from helper_functions.py import print_points, plot_mapsm"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ui9UZvaI8ri6"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "Y9ZOOuC_Dhay"
      },
      "outputs": [],
      "source": [
        "#@title Default title text\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy.linalg import norm\n",
        "import os\n",
        "import h5py\n",
        "import tensorflow as tf\n",
        "\n",
        "# to check the impact of resolution in the 2d-map;\n",
        "# print the difference between the filledBins vector versus the map (map is restricted by resolution)\n",
        "def print_points(filled_bins_array = None, map_array = None, mip_position_array = None, resolution = 10):\n",
        "\n",
        "    length = map_array.shape[0]\n",
        "    distances_bins_list = []\n",
        "    distances_map_list = []\n",
        "\n",
        "    print(f\"filled_bins_array shape = {filled_bins_array.shape}\")\n",
        "    print(f\"map_array shape = {map_array.shape}\")\n",
        "    print(f\"mip_position_array shape = {mip_position_array.shape}\")\n",
        "\n",
        "\n",
        "    for i in range (1, length):\n",
        "\n",
        "        filled_bins = np.array(filled_bins_array[i])\n",
        "        map = np.array(map_array[i, :,:])\n",
        "        mip_pos = np.array(mip_position_array[i, :])\n",
        "\n",
        "        #print(f\"filled_bins shape = {filled_bins.shape}\")\n",
        "        #print(f\"map shape = {map.shape}\")\n",
        "        #print(f\"mip_pos shape = {mip_pos.shape}\")\n",
        "\n",
        "        _mip_position = []\n",
        "        #_mip_position.append(mip_position_array[])\n",
        "        distances2 = []\n",
        "\n",
        "        distances_bins = [norm(np.array(pos) - mip_pos) for pos in filled_bins]\n",
        "\n",
        "        distances_map = []\n",
        "        for y in range(map.shape[0]):\n",
        "            for x in range(map.shape[1]):\n",
        "                if map[y, x] == 1:\n",
        "                    point = (x, y)\n",
        "                    distance = np.linalg.norm(np.array(point) - mip_pos*resolution)\n",
        "                    distances_map.append(distance)\n",
        "\n",
        "\n",
        "\n",
        "        distances_bins_list.append(distances_bins)\n",
        "        distances_map_list.append(distances_map)\n",
        "\n",
        "\n",
        "    # Print the distances for each element in map_data_list\n",
        "    print(f\"Element {i+1} distances:\")\n",
        "    for j, (distances_bins, distances_map) in enumerate(zip(distances_bins_list, distances_map_list)):\n",
        "        print(f\"  Point {j+1}: Distance bins: {distances_bins}\\n, Distance map: {distances_map}\")\n",
        "    print()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def plot_maps(filled_bins_array=None, map_array=None, mip_position_array=None, X_momentum=None, X_refractive_index=None, X_ckov=None, percentage_to_plot=5, resolution = 10):\n",
        "  \"\"\"\n",
        "  Args : filled_bins_array : array that holds the vectors of filled pads\n",
        "         map_array : 2d  map with a determined resolution (the points in the filled_bins_array element, just restricted by the resolution)\n",
        "         mip_position_array : array of the MIP {x, y} positions\n",
        "\n",
        "         TODO : add mass_category and actual mass?\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  #percentage_to_plot = 0.05 / 10\n",
        "\n",
        "  # Calculate the starting index of the samples to plot\n",
        "  num_samples = map_array.shape[0]\n",
        "  start_index = -num_samples\n",
        "\n",
        "  # Create a subplot with the number of rows based on the number of samples\n",
        "  fig, axes = plt.subplots(nrows=5, ncols=1, figsize=(8, 20))\n",
        "\n",
        "  # Iterate over the samples and plot each map with information\n",
        "  for i, ax in enumerate(axes):\n",
        "      # Get the map and corresponding information\n",
        "      map_data = map_array[start_index + i, :, :]\n",
        "      #mass_category = particle_vector[start_index + i].mass_category\n",
        "      ckov = X_ckov[start_index + i]\n",
        "      mip_position = mip_position_array[start_index + i]\n",
        "      momentum = X_momentum[start_index + i]\n",
        "      refractive_index = X_refractive_index[start_index + i]\n",
        "\n",
        "      # Plot the map\n",
        "      ax.imshow(map_data, cmap='gray')\n",
        "\n",
        "\n",
        "\n",
        "      #try :\n",
        "      # Add a red dot at the MIP position\n",
        "      ax.plot(mip_position[0]*resolution, mip_position[1]*resolution, 'ro')\n",
        "      #Except exception as e :\n",
        "      #  print(\"caught non mip pos \")\n",
        "      # Set the title with the information\n",
        "      #ax.set_title(f\"Mass: {mass_category}, CKOV: {ckov}, MIP Position: {mip_position}, Momentum: {momentum},  refractive_index: {refractive_index}\")\n",
        "      ax.set_title(f\"CKOV: {ckov}, MIP Position: {mip_position}, Momentum: {momentum},  refractive_index: {refractive_index}\")\n",
        "\n",
        "      ax.axis('off')\n",
        "\n",
        "  # Adjust the spacing between subplots\n",
        "  plt.tight_layout()\n",
        "\n",
        "  # Show the plot\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcY6LVGpxZQI",
        "outputId": "a54b1b15-c697-41eb-e443-b65595237814"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-14 16:13:00--  https://raw.githubusercontent.com/eflatlan/CNN_PID/models_sacved/plot_helper_functions.py?token=GHSAT0AAAAAACC6VFJHW7BBRRHNGQZPMV3OZFROC5A\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2023-07-14 16:13:00 ERROR 404: Not Found.\n",
            "\n",
            "mv: cannot stat 'plot_helper_functions.py?token=GHSAT0AAAAAACC6VFJHW7BBRRHNGQZPMV3OZFROC5A': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "def create_lr_scheduler(num_epochs = 10):\n",
        "\n",
        "  start_lr = 0.1\n",
        "  end_lr = 5e-6\n",
        "  exp_decay = -np.log(end_lr/start_lr) / num_epochs # Calculate decay rate based on start and end learning rate\n",
        "\n",
        "  lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: start_lr * np.exp(-exp_decay * epoch))\n",
        "  return lr_scheduler\n",
        "\n",
        "\n",
        "\n",
        "def plot_lr(num_epochs = 10, history = None):\n",
        "  div = num_epochs/4\n",
        "  lrs = 1e-4 * (10 ** (np.arange(num_epochs)/div))\n",
        "  plt.figure(figsize=(10, 7))\n",
        "  plt.semilogx(lrs, history.history[\"loss\"]) # we want the x-axis (learning rate) to be log scale\n",
        "  plt.xlabel(\"Learning Rate\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "\n",
        "\n",
        "  plt.title(\"Learning rate vs. loss\");\n",
        "\n",
        "#z!wget https://raw.githubusercontent.com/eflatlan/CNN_PID/models_sacved/helper_functions.py\n",
        "!wget https://raw.githubusercontent.com/eflatlan/CNN_PID/models_sacved/plot_helper_functions.py?token=GHSAT0AAAAAACC6VFJHW7BBRRHNGQZPMV3OZFROC5A\n",
        "!mv plot_helper_functions.py?token=GHSAT0AAAAAACC6VFJHW7BBRRHNGQZPMV3OZFROC5A plot_helper_functions.py\n",
        "\n",
        "#from plot_helper_functions import plot_training_history, plot_dist2mip_histograms\n",
        "from plot_helper_functions import plot_training_history#, plot_dist2mip_histograms, plot_maps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "VRoviFHFMvUa"
      },
      "outputs": [],
      "source": [
        "#def plot_worst_(model, y_test, X_test_map, X_test_momentum, X_test_refractive_index, X_test_ckov, X_test_mip_position, y_pred):\n",
        "def plot_maps(filled_bins_array=None, map_array=None, mip_position_array=None, X_momentum=None, X_refractive_index=None, X_ckov=None, percentage_to_plot=5, resolution = 10):\n",
        "  #  print(\"Shape of y_pred: \", y_pred.shape)\n",
        "  # 1. Predict labels on validation data\n",
        "  #plot_worst(model, y_test, X_test[\"X_test_map\"], X_test[\"X_test_momentum\"], X_test[\"X_test_refractive_index\"], X_test[\"X_test_ckov\"], X_test[\"X_test_mip_position\"], y_pred_test)\n",
        "\n",
        "  # 2. Calculate the difference between predicted and actual labels\n",
        "  losses = tf.keras.losses.categorical_crossentropy(y_test, y_pred).numpy()\n",
        "\n",
        "  # Sort the indices of the losses from highest to lowest\n",
        "  sorted_indices = np.argsort(losses)[::-1]\n",
        "\n",
        "  # Get the indices of the worst performing 10%\n",
        "  worst_10_percent_indices = sorted_indices[:int(0.1*len(sorted_indices))]\n",
        "\n",
        "  # Create figure and axes\n",
        "  num_plots = len(worst_10_percent_indices)\n",
        "  #fig, axes = plt.subplots(num_plots, 1, figsize=(8, 20))\n",
        "  fig, axes = plt.subplots(num_plots,figsize=(8, 20))\n",
        "\n",
        "  # Define mass categories\n",
        "  mass_categories = [\"pion\", \"kaon\", \"proton\"]\n",
        "\n",
        "  # 3. Create plots for these cases, including their feature information and predicted vs actual labels\n",
        "  for i, index in enumerate(worst_10_percent_indices):\n",
        "      # Get the map and corresponding information\n",
        "      map_data = map_array[index, :, :]\n",
        "      actual_mass_category = mass_categories[np.argmax(y_test[index])]\n",
        "\n",
        "      print(f\"y_test[index] = {y_test[index]}\")\n",
        "\n",
        "      predicted_mass_category = mass_categories[np.argmax(y_pred[index])]\n",
        "      ckov = X_test_ckov[index]\n",
        "      mip_position = X_test_mip_position[index]\n",
        "      momentum = X_test_momentum[index]\n",
        "      refractive_index = X_test_refractive_index[index]\n",
        "\n",
        "      mass_actual = momentum * np.sqrt(refractive_index**2 * np.cos(ckov)*np.cos(ckov) - 1)\n",
        "\n",
        "      # Check if the value is NaN (invalid Cherenkov angle)\n",
        "      if np.isnan(mass_actual):\n",
        "          mass_actual = \"Invalid\"\n",
        "\n",
        "      # Plot the map\n",
        "      axes[i].imshow(map_data, cmap='gray')\n",
        "\n",
        "      # Add a red dot at the MIP position\n",
        "      axes[i].plot(mip_position[0]*resolution, mip_position[1]*resolution, 'ro')\n",
        "\n",
        "      # Set the title with the information\n",
        "      axes[i].set_title(f\"Actual Mass\")#: {actual_mass_category}, Predicted Mass: {predicted_mass_category},\\nMass: {mass_actual}, Mass_prob = {y_pred[index]} \\nCKOV: {ckov}, MIP Position: {mip_position}, \\nMomentum: {momentum}, Refractive Index: {refractive_index}\")\n",
        "      #\n",
        "      axes[i].set_title(f\"Actual Mass: {actual_mass_category}, Predicted Mass: {predicted_mass_category},\\nMass: {mass_actual}, Mass_prob = {y_pred[index]} \\nCKOV: {ckov}, MIP Position: {mip_position}, \\nMomentum: {momentum}, Refractive Index: {refractive_index}\")\n",
        "\n",
        "      #axes[i].set_title(f\"Actual Mass: {actual_mass_category}, Predicted Mass: {predicted_mass_category}, Mass: {mass_actual}\\nCKOV: {ckov}, MIP Position: {mip_position}, Momentum: {momentum}, Refractive Index: {refractive_index}\")\n",
        "      axes[i].axis('off')\n",
        "\n",
        "      print(\"\\n\")\n",
        "      print(f\"  Actual Mass: {actual_mass_category}, Predicted Mass: {predicted_mass_category},\\n Mass: {mass_actual}, Mass_prob = {y_pred[index]} \\n CKOV: {ckov}, MIP Position: {mip_position}, \\n  Momentum: {momentum}, Refractive Index: {refractive_index}\")\n",
        "  # Adjust the spacing between subplots\n",
        "  plt.tight_layout()\n",
        "\n",
        "  # Show the plot\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helping class and imports\n"
      ],
      "metadata": {
        "id": "oXEUp3H8NoS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#resolution = 4\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import cycle\n",
        "\n",
        "import sys\n",
        "\n",
        "print(sys.getrecursionlimit()) # Prints 1000\n",
        "\n",
        "print_vals = False\n",
        "from numpy.linalg import norm\n",
        "from tensorflow.keras.backend import expand_dims\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import precision_recall_curve, confusion_matrix\n",
        "\n",
        "from scipy.signal import find_peaks\n",
        "\n",
        "from __future__ import print_function\n",
        "import os\n",
        "import h5py\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Activation, Input, Conv2D, Lambda, Flatten, Dense, concatenate, BatchNormalization, MaxPooling2D, Dropout, LeakyReLU, Masking, Embedding\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class Constants:\n",
        "    PION_MASS = 0.1396\n",
        "    KAON_MASS = 0.4937\n",
        "    PROTON_MASS = 0.938\n",
        "\n",
        "np.set_printoptions(precision=4)\n",
        "\n",
        "@staticmethod\n",
        "def calculate_mass(momentum, refractiveIndex, ckov):\n",
        "    \"\"\" args : momentum, refractiveIndex, ckov\n",
        "        returns : mass\n",
        "    \"\"\"\n",
        "    mass = momentum * np.sqrt((refractiveIndex * np.cos(ckov))**2 - 1)\n",
        "    return mass\n",
        "\n",
        "\n",
        "class ParticleDataUtils:\n",
        "    class Candidate2:\n",
        "        def __init__(self, x, y, candStatus):\n",
        "            self.x = x\n",
        "            self.y = y\n",
        "            self.candStatus = candStatus\n",
        "\n",
        "    class ParticleInfo: # pls help me understand if any of these fields are missing from the X_train :\n",
        "        def __init__(self, momentum, mass, energy, refractiveIndex, ckov, xRad, yRad, xPC, yPC, thetaP, phiP, arrayInfo, candsCombined):\n",
        "            self.momentum = momentum # this dhould be with\n",
        "            self.mass = mass    # not with\n",
        "            self.energy = energy # with\n",
        "            self.refractiveIndex = refractiveIndex # with\n",
        "            self.ckov = ckov # not with\n",
        "            self.xRad = xRad # with\n",
        "            self.yRad = yRad # with\n",
        "            self.xPC = xPC# with\n",
        "            self.yPC = yPC# with\n",
        "            self.thetaP = thetaP# with\n",
        "            self.phiP = phiP# with\n",
        "            self.arrayInfo = arrayInfo # do not include this to model\n",
        "            self.candsCombined = candsCombined # with the field candStatus is a int that is 0..7, please make it categorical\n",
        "            self.mass_category = self.infer_mass_category(mass)\n",
        "            self.mip_position = [xPC, yPC]\n",
        "            self.rad_position = [xRad, yRad]\n",
        "\n",
        "        @staticmethod\n",
        "        def infer_mass_category_from_ckov(momentum, refractiveIndex, ckov):\n",
        "            mass = momentum * np.sqrt((refractiveIndex * np.cos(ckov))**2 - 1)\n",
        "\n",
        "            mass_category = \"unknown\"\n",
        "            if abs(mass - Constants.PION_MASS) < 1e-4:\n",
        "                mass_category = \"pion\"\n",
        "            elif abs(mass - Constants.KAON_MASS) < 1e-4:\n",
        "                mass_category = \"kaon\"\n",
        "            elif abs(mass - Constants.PROTON_MASS) < 1e-4:\n",
        "                mass_category = \"proton\"\n",
        "            if print_vals:\n",
        "              print(f\"\\ninfer_mass_category_from_ckov :  momentum = {momentum}|  mass_calc = {mass} |  mass_category={mass_category} | refractiveIndex = {refractiveIndex} | ckov = {ckov}\")\n",
        "            return mass_category\n",
        "\n",
        "        @staticmethod\n",
        "        def infer_mass_category(mass):\n",
        "            if abs(mass - Constants.PION_MASS) < 1e-6:\n",
        "                return \"pion\"\n",
        "            elif abs(mass - Constants.KAON_MASS) < 1e-6:\n",
        "                return \"kaon\"\n",
        "            elif abs(mass - Constants.PROTON_MASS) < 1e-6:\n",
        "                return \"proton\"\n",
        "            else:\n",
        "                return \"unknown\"\n",
        "\n",
        "        def __str__(self):\n",
        "            if print_vals:\n",
        "              return (f\"ParticleInfo(momentum={self.momentum} | mass={self.mass} |  mass_category={self.mass_category} | \"\n",
        "                      f\"refractiveIndex={self.refractiveIndex} | ckov={self.ckov} | rad_position={len(self.rad_position)}, \"\n",
        "                      f\"mip_position={self.mip_position})\")\n",
        "\n",
        "    #  ''' def calculate_distances_to_mip(self):\n",
        "    #       \"\"\"Calculate Euclidean distances from all filled bins to MIP position\"\"\"\n",
        "    #       filledBins_np = np.array(self.filledBins)\n",
        "    #       mip_position_np = np.array(self.mip_position)\n",
        "\n",
        "    #       distances = np.linalg.norm(filledBins_np - mip_position_np, axis=1)\n",
        "    #       return distances'''\n",
        "\n",
        "\n",
        "    def __init__(self, filename = \"default_filename.h5\", percentage_to_read = 100):\n",
        "        self.filename = filename\n",
        "        self.percentage_to_read = percentage_to_read\n",
        "        self.particle_vector = self.load_data(filename)\n",
        "\n",
        "        #\n",
        "        self.particle_info = self.process_data(self.particle_vector, self.percentage_to_read)\n",
        "        self.num_particles = len(self.particle_info)\n",
        "\n",
        "\n",
        "        # new scalers to be created :\n",
        "        self.momentum_scaler, self.momentum_stats = self.create_scaler(\"momentum\")\n",
        "        self.refractive_index_scaler, self.refractive_index_stats = self.create_scaler(\"refractiveIndex\")\n",
        "        self.ckov_scaler, self.ckov_stats = self.create_scaler(\"ckov\")\n",
        "        #self.distances_scaler, self.distances_stats = self.create_scaler(\"distances\")\n",
        "\n",
        "\n",
        "    def load_data(self, filename):\n",
        "        drive_path = '/content/drive/MyDrive/Colab Notebooks/CERN_ML/CNN_PID/'  # Update the path to your Google Drive folder\n",
        "        file_path = os.path.join(drive_path, filename)\n",
        "        particle_vector = []\n",
        "\n",
        "        print(f\"load_data : reading file {filename}\")\n",
        "        print(f\"load_data : location {drive_path}\")\n",
        "\n",
        "        with h5py.File(file_path, 'r') as file:\n",
        "            #file.visititems(print_hdf5_items)\n",
        "            for i, group_name in enumerate(file):\n",
        "                group = file[group_name]\n",
        "\n",
        "                # Read scalar values\n",
        "                momentum = group.attrs['Momentum']\n",
        "                mass = group.attrs['Mass']\n",
        "                energy = group.attrs['Energy']\n",
        "                refractiveIndex = group.attrs['RefractiveIndex']\n",
        "                ckov = group.attrs['Ckov']\n",
        "\n",
        "                xRad = group.attrs['xRad']\n",
        "                yRad = group.attrs['yRad']\n",
        "                xPC = group.attrs['xPC']\n",
        "                yPC = group.attrs['yPC']\n",
        "\n",
        "                thetaP = group.attrs['ThetaP']\n",
        "                phiP = group.attrs['PhiP']\n",
        "\n",
        "                arrayInfo = [group.attrs['ArrayInfo0'], group.attrs['ArrayInfo1'], group.attrs['ArrayInfo2'], group.attrs['ArrayInfo3']]\n",
        "\n",
        "\n",
        "                candsCombined_dataset = group['candsCombined']\n",
        "\n",
        "                candsCombined_data = candsCombined_dataset[...]\n",
        "                #candsCombinedVec = [ParticleDataUtils.Candidate2(x['x'], x['y']) for x in candsCombined_data]\n",
        "                # candsCombinedStatVec = [ParticleDataUtils.Candidate2(x['candStatus']) for x in candsCombined_data]\n",
        "                #candsCombinedStatVec.to_list()\n",
        "                #candsCombinedVec.to_list()\n",
        "\n",
        "                candsCombined = [ParticleDataUtils.Candidate2(x['x'], x['y'], x['candStatus']) for x in candsCombined_data]\n",
        "\n",
        "\n",
        "                # # Read filledBins\n",
        "                # filled_bins_dataset = group['FilledBins']\n",
        "                # filled_bins_data = filled_bins_dataset[...]  # Retrieve the data as a numpy array\n",
        "\n",
        "                # filled_bins = filled_bins_data.tolist()  # Convert the numpy array to a list\n",
        "\n",
        "                # # get window :\n",
        "                # calculator = CkovCalculator()\n",
        "                # results = calculator.calcCkovFromMass(momentum, refractive_index)\n",
        "                # radiuses = []\n",
        "                # for particle, radius in results.items():\n",
        "                #     #print(f\"{particle} Radius: {radius}\")\n",
        "                #     radiuses.append(radius)\n",
        "\n",
        "                # the biggest radius should then extract a window around the MIP\n",
        "\n",
        "\n",
        "                # Extract a window around the MIP using the biggest radius\n",
        "                #window_size = int(max_radius * 2)  # Adjust the size as needed\n",
        "                # NB!!! add resolution here? Or multiply elswhere..\n",
        "\n",
        "                particle_info = ParticleDataUtils.ParticleInfo(\n",
        "                    momentum, mass, energy, refractiveIndex, ckov, xRad, yRad, xPC, yPC, thetaP, phiP, arrayInfo = arrayInfo, candsCombined=candsCombined)\n",
        "\n",
        "                if print_vals == True:\n",
        "                  print(particle_info)  # This will use the __str__() method of ParticleInfo\n",
        "\n",
        "                particle_vector.append(particle_info)\n",
        "\n",
        "        return particle_vector\n",
        "\n",
        "    def process_data(self, particle_vector, percentage):\n",
        "\n",
        "        # Calculate the number of particles based on the percentage\n",
        "        num_particles = int(len(particle_vector) * (percentage / 100.0))\n",
        "\n",
        "        # Slice the particle_vector to the desired percentage\n",
        "        particle_vector = particle_vector[:num_particles]\n",
        "        return particle_vector\n",
        "\n",
        "\n",
        "\n",
        "    # TODO: add more scalers here!\n",
        "    def create_scaler(self, feature):\n",
        "        if feature == \"momentum\":\n",
        "            values = np.array([info.momentum for info in self.particle_info]).reshape(-1, 1)\n",
        "        elif feature == \"refractiveIndex\":\n",
        "            values = np.array([info.refractiveIndex for info in self.particle_info]).reshape(-1, 1)\n",
        "        elif feature == \"ckov\":\n",
        "            values = np.array([info.ckov for info in self.particle_info]).reshape(-1, 1)\n",
        "        elif feature == \"distances\":\n",
        "            distances = []\n",
        "            for info in self.particle_info:\n",
        "                distances.extend(info.distances_to_mip)\n",
        "            values = np.array(distances).reshape(-1, 1)\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid feature: {feature}\")\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        scaled_values = scaler.fit_transform(values)\n",
        "        stats = {\n",
        "            \"mean\": scaler.mean_[0],\n",
        "            \"std\": scaler.scale_[0]\n",
        "        }\n",
        "        return scaler, stats\n",
        "\n",
        "\n",
        "# TODO : denne skal mulgiens fjernes helt ?\n",
        "# create a map, the resolution is the \"inverse\"\n",
        "def create_map(filledBins=None, resolution=4):\n",
        "    # Add an offset to your map shape calculation to handle edge cases\n",
        "    offset = 0\n",
        "    map_shape = (int(144 * resolution + offset), int(160 * resolution + offset))\n",
        "    map_data = np.zeros(map_shape, dtype=np.int32)\n",
        "    if filledBins is not None:\n",
        "        filledBins_np = np.array(filledBins)\n",
        "        indices = (filledBins_np * resolution).astype(int)\n",
        "        #print(f\"create_map : indices shape : {np.array(indices).shape}\")\n",
        "\n",
        "        map_data[indices[:, 1], indices[:, 0]] = 1\n",
        "\n",
        "        ind = np.where(map_data == 1)\n",
        "        #print(f\"create_map : ind shape : {np.array(ind).shape}\")\n",
        "    return map_data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "es3UneK1Nmof",
        "outputId": "b016b268-beee-4adb-97ac-d717b93cade7"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "8U_5qogtNs15"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "IjrxNzz0LMqU"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MassClassifier:\n",
        "  def __init__(self, percentage_to_read = 10, resolution = 4):\n",
        "      self.model = None\n",
        "      self.utils = None\n",
        "      self.percentage_to_read = percentage_to_read\n",
        "      self.resolution = resolution\n",
        "\n",
        "\n",
        "  def load_data(self, filename):\n",
        "      self.utils = ParticleDataUtils(filename, percentage_to_read = self.percentage_to_read) # specify percentage of particles to read..\n",
        "      print(f\"Number of particles: {self.utils.num_particles}\")\n",
        "\n",
        "  def preprocess_data(self):\n",
        "      particle_info = self.utils.particle_info\n",
        "\n",
        "      # Prepare the inputs\n",
        "      # create a map with resolution to be chosen, iterate over teh filledBins vector\n",
        "      offset = 0\n",
        "      X_map = np.zeros((len(particle_info), 144*4 + offset, 160*4 + offset))\n",
        "\n",
        "\n",
        "      # TODO : change this to accept new vector of points w candStatus\n",
        "      # ind = 0\n",
        "      # for info in particle_info:\n",
        "      #    X_map[ind, :, :] = create_map(filledBins = info.filledBins, resolution = self.resolution)\n",
        "      #    ind = ind + 1\n",
        "\n",
        "      #X_map = np.array([create_map(filledBins = info.filledBins, resolution = self.resolution) for info in particle_info])\n",
        "      #X_map = X_map.reshape((len(particle_info), 144*self.resolution + 1, 160*self.resolution + 1))\n",
        "      #print(f\"X_map shape {X_map.shape}\")\n",
        "      #filled_bins_array = np.array([info.filledBins for info in particle_info], dtype=object)\n",
        "\n",
        "\n",
        "\n",
        "      #[info.candCombined.x, info.candCombined.y ]\n",
        "      # Hær?\n",
        "      # candidates\n",
        "      X_cand_pos = np.array([[[cand.x, cand.y] for cand in info.candsCombined] for info in particle_info], dtype = object)\n",
        "\n",
        "      X_cand_status = np.array([[candCombined.candStatus for candCombined in info.candsCombined] for info in particle_info], dtype = object)\n",
        "      from sklearn.preprocessing import MultiLabelBinarizer\n",
        "      mlb = MultiLabelBinarizer()\n",
        "      X_cand_status_encoded = mlb.fit_transform(X_cand_status)\n",
        "\n",
        "      print(\"Before padding : \")\n",
        "      print(f\" X_cand_pos_shape = {X_cand_pos.shape}\")\n",
        "      print(f\" len_candStatus_shape = {X_cand_status.shape}\")\n",
        "\n",
        "\n",
        "      #print(f\" X_cand_pos dimensions : = {np.asarray(X_cand_pos, dtype = object).shape[0]}, {np.asarray(X_cand_pos, dtype = object).shape[1], {np.asarray(X_cand_pos, dtype = object).shape[2]}}\")\n",
        "      #print(f\" len_candStatus dimensions : = {np.asarray(X_cand_status_encoded, dtype = object).shape[0]}, {np.asarray(X_cand_status_encoded, dtype = object).shape[1], {np.asarray(X_cand_status_encoded, dtype = object).shape[2]}}\")\n",
        "\n",
        "\n",
        "      # pad the variable sized arrays :\n",
        "      # sannsynligvis ikke riktig dimension\n",
        "      X_cand_status_encoded = pad_sequences(X_cand_status_encoded, padding='post')\n",
        "      X_cand_pos = pad_sequences(X_cand_pos, padding='post')\n",
        "\n",
        "      print(\"After padding : \")\n",
        "      print(f\" X_cand_pos_shape = {X_cand_pos.shape}\")\n",
        "      print(f\" len_candStatus_shape = {X_cand_status.shape}\")\n",
        "\n",
        "\n",
        "      #print(f\" X_cand_pos dimensions : = {np.asarray(X_cand_pos, dtype = object).shape[0]}, {np.asarray(X_cand_pos, dtype = object).shape[1], {np.asarray(X_cand_pos, dtype = object).shape[2]}}\")\n",
        "      #print(f\" len_candStatus dimensions : = {np.asarray(X_cand_status_encoded, dtype = object).shape[0]}, {np.asarray(X_cand_status_encoded, dtype = object).shape[1], {np.asarray(X_cand_status_encoded, dtype = object).shape[2]}}\")\n",
        "\n",
        "\n",
        "\n",
        "      # scalars\n",
        "      X_momentum = np.array([info.momentum for info in particle_info])#.reshape(-1, 32, 32, 1)\n",
        "      X_refractive_index = np.array([info.refractiveIndex for info in particle_info])#.reshape(-1, 32, 32, 1)\n",
        "\n",
        "      # this should not be included? :\n",
        "      X_ckov = np.array([info.ckov for info in particle_info])#.reshape(-1, 32, 32, 1) ??\n",
        "\n",
        "      X_phi = np.array([particle.phiP for particle in particle_info])      # new\n",
        "      X_theta = np.array([particle.thetaP for particle in particle_info])  # new\n",
        "      X_energy = np.array([particle.energy for particle in particle_info]) # new\n",
        "\n",
        "\n",
        "      # x,y pairs (2,1) :\n",
        "      X_mip_position = np.array([info.mip_position for info in particle_info]) # was already\n",
        "      X_rad_position = np.array([info.rad_position for info in particle_info]) # new\n",
        "\n",
        "\n",
        "      # X_cand_status_encoded, X_cand_pos, X_momentum, X_refractive_index, X_phi, X_theta, X_energy, X_mip_position, X_rad_position\n",
        "\n",
        "      # calculate radiuses (radius of f(m_i) where m_i = [m_pion, m_kaon, m_proton],  and n, p, + later {xRad, yRad, theta, phi} fixed for a given track)\n",
        "      # for mass-hypothesis\n",
        "\n",
        "      # not used anymore : (make new w inclinations?)\n",
        "      #calc_dist2mip(maps = None,  mip_positions = None, resolution = 10):\n",
        "\n",
        "\n",
        "      # TODO: this w new map\n",
        "      # try:\n",
        "      #   plot_maps(filled_bins_array=filled_bins_array, map_array=X_map, mip_position_array=X_mip_position, X_momentum=X_momentum, X_refractive_index=X_refractive_index, X_ckov=X_ckov, percentage_to_plot=5, resolution = self.resolution)\n",
        "      # except Exception as e:\n",
        "      #   print(f\"plot_maps failed due to error : {e}\")\n",
        "\n",
        "      # change shape to be num_samples, pad_sequences, 3:\n",
        "\n",
        "      #X_momentum = np.asarray(X_momentum)\n",
        "      #X_refractive_index = np.asarray(X_refractive_index)\n",
        "\n",
        "\n",
        "      ckov_hyp = np.zeros((len(particle_info), 3))\n",
        "      #print(f\"ckov_hyp shape : {ckov_hyp.shape}\")\n",
        "\n",
        "      #ckov_hyp = calc_ckov_hyp(p = X_momentum, n = X_refractive_index)\n",
        "      #print(f\"ckov_hyp shape : {ckov_hyp.shape}\")\n",
        "\n",
        "      #ckov_hyp = ckov_hyp.reshape((len(particle_info), 3))\n",
        "      #print(f\"ckov_hyp shape : {ckov_hyp.shape}\")\n",
        "\n",
        "      #X_dist2mip = pad_sequences(X_dist2mip, padding='post') commented out\n",
        "\n",
        "\n",
        "\n",
        "      # mp = np.array(X_map[0,:,:])\n",
        "      # #print(f\"build_model = {mp}\")\n",
        "\n",
        "      # #X_dist2mip, X_map_segmented = calc_dist2mip(maps = X_map.copy(), mip_positions = X_mip_position, resolution = self.resolution, ckov_hyp = ckov_hyp)\n",
        "\n",
        "      # #\n",
        "      # calculator2 = CkovCalculator()\n",
        "\n",
        "      # # Segment the regions :\n",
        "\n",
        "      # # num_samplesx3 array holding the ckovangles from teh masshypotheses:\n",
        "\n",
        "\n",
        "\n",
        "      #cherenkov_photon_candidates = np.concatenate(cherenkov_photon_candidates)\n",
        "      #ckov_hyp = np.concatenate(ckov_hyp)\n",
        "\n",
        "\n",
        "      # createa a num_samplesx3x<var_length> vector\n",
        "      # X_dist2mip = pad_sequences(X_dist2mip, padding='post')\n",
        "\n",
        "      # X_photon_ckov = calculator2.get_ckov_from_radius(radius = X_dist2mip, resolution = self.resolution)\n",
        "\n",
        "      # Create a num_samplesx3x<var_length> vector holding for each num_samples the segmented photon-angles, in each of the regions\n",
        "      # theta_ckov in {ckov_pion +- std.dev, ckov_kaon +- std.dev, ckov_proton +- std.dev}\n",
        "      #X_photon_ckov_segmented = segment_photons(X_photon_ckov, ckov_hyp)\n",
        "      #X_photon_ckov_segmented = pad_sequences(X_photon_ckov_segmented, padding='post')\n",
        "\n",
        "\n",
        "      # Normalize the inputs NB commented out scaling !!!\n",
        "      #X_momentum = self.utils.momentum_scaler.transform(X_momentum.reshape(-1, 1))#.reshape(-1, 32, 32, 1)\n",
        "      #X_refractive_index = self.utils.refractive_index_scaler.transform(X_refractive_index.reshape(-1, 1))#.reshape(-1, 32, 32, 1)\n",
        "      #X_ckov = self.utils.ckov_scaler.transform(X_ckov.reshape(-1, 1))#.reshape(-1, 32, 32, 1)\n",
        "\n",
        "\n",
        "      # NB, commented these out, leads to problem?\n",
        "      #X_mip_position[0,:] = self.utils.distances_scaler.transform(X_mip_position[0,:])\n",
        "      #X_mip_position[1,:] = self.utils.distances_scaler.transform(X_mip_position[1,:])\n",
        "\n",
        "\n",
        "\n",
        "      # Prepare the outputs\n",
        "      y = np.array([info.mass_category for info in particle_info])\n",
        "\n",
        "      # Convert the outputs to one-hot encoded vectors\n",
        "      lb = LabelBinarizer()\n",
        "      y = lb.fit_transform(y)\n",
        "\n",
        "      # Split the data into train and test sets\n",
        "\n",
        "\n",
        "\n",
        "      X_train_cand_status_encoded, X_test_cand_status_encoded,  X_train_cand_pos, X_test_cand_pos, \\\n",
        "      X_train_momentum, X_test_momentum, X_train_refractive_index, X_test_refractive_index, \\\n",
        "      X_train_phi, X_test_phi, X_train_theta, X_test_theta, X_train_energy, X_test_energy, \\\n",
        "      X_train_mip_position, X_test_mip_position, X_train_rad_position, X_test_rad_position, \\\n",
        "      y_train, y_test = \\\n",
        "      train_test_split(X_cand_status_encoded, X_cand_pos, X_momentum, X_refractive_index, X_phi, X_theta, X_energy, X_mip_position, X_rad_position, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "      # X_train_dist2mip, X_test_dist2mip, X_train_map, X_test_map, X_train_momentum, X_test_momentum, X_train_refractive_index, X_test_refractive_index, \\\n",
        "      #     X_train_ckov, X_test_ckov, X_train_mip_position, X_test_mip_position, y_train, y_test = \\\n",
        "      #     train_test_split(X_dist2mip, X_map, X_momentum, X_refractive_index, X_ckov, X_mip_position, y, test_size=0.2, random_state=42)\n",
        "\n",
        "      # # Suppose X_photon_ckov_segmented is your data\n",
        "      # X_train_photon_ckov_segmented, X_test_photon_ckov_segmented = \\\n",
        "      #     train_test_split(X_photon_ckov_segmented, test_size=0.2, random_state=42)\n",
        "\n",
        "      X_train = {\n",
        "          \"X_train_cand_status_encoded\": X_train_cand_status_encoded,\n",
        "          \"X_train_cand_pos\": X_train_cand_pos,\n",
        "          \"X_train_momentum\": X_train_momentum,\n",
        "          \"X_train_refractive_index\": X_train_refractive_index,\n",
        "          \"X_train_phi\": X_train_phi,\n",
        "          \"X_train_theta\": X_train_theta,\n",
        "          \"X_train_energy\": X_train_energy,\n",
        "          \"X_train_mip_position\": X_train_mip_position,\n",
        "          \"X_train_rad_position\": X_train_rad_position\n",
        "      }\n",
        "\n",
        "      X_test = {\n",
        "          \"X_test_cand_status_encoded\": X_test_cand_status_encoded,\n",
        "          \"X_test_cand_pos\": X_test_cand_pos,\n",
        "          \"X_test_momentum\": X_test_momentum,\n",
        "          \"X_test_refractive_index\": X_test_refractive_index,\n",
        "          \"X_test_phi\": X_test_phi,\n",
        "          \"X_test_theta\": X_test_theta,\n",
        "          \"X_test_energy\": X_test_energy,\n",
        "          \"X_test_mip_position\": X_test_mip_position,\n",
        "          \"X_test_rad_position\": X_test_rad_position\n",
        "      }\n",
        "\n",
        "\n",
        "      return (X_train, X_test, y_train, y_test, X_map, X_cand_pos)\n",
        "\n",
        "\n",
        "\n",
        "  # TODO: to this to the new data-fields :\n",
        "  def plot_hist(self, input_sequence_length = None, X_train = None, X_test = None, y_train = None, y_test = None):\n",
        "      #map_shape = (None, None, 1)  # Variable shape for the map input\n",
        "      #X_train_photon_ckov_segmented shape : (1955, 3, 74)\n",
        "\n",
        "      # create a map with resolution to be chosen, iterate over teh filledBins vector\n",
        "      #X_map = np.array([create_map(filledBins = info.filledBins, resolution = self.resolution) for info in particle_info])\n",
        "      #filled_bins_array = np.array([info.filledBins for info in particle_info], dtype=object)\n",
        "\n",
        "\n",
        "      X_train_cand_status_encoded = X_train[\"X_train_cand_status_encoded\"]\n",
        "      X_train_cand_pos = X_train[\"X_train_cand_pos\"]\n",
        "      X_train_momentum = X_train[\"X_train_momentum\"]\n",
        "      X_train_refractive_index = X_train[\"X_train_refractive_index\"]\n",
        "      X_train_phi = X_train[\"X_train_phi\"]\n",
        "      X_train_theta = X_train[\"X_train_theta\"]\n",
        "      X_train_energy = X_train[\"X_train_energy\"]\n",
        "      X_train_mip_position = X_train[\"X_train_mip_position\"]\n",
        "      X_train_rad_position = X_train[\"X_train_rad_position\"]\n",
        "\n",
        "      X_test_cand_status_encoded = X_test[\"X_test_cand_status_encoded\"]\n",
        "      X_test_cand_pos = X_test[\"X_test_cand_pos\"]\n",
        "      X_test_momentum = X_test[\"X_test_momentum\"]\n",
        "      X_test_refractive_index = X_test[\"X_test_refractive_index\"]\n",
        "      X_test_phi = X_test[\"X_test_phi\"]\n",
        "      X_test_theta = X_test[\"X_test_theta\"]\n",
        "      X_test_energy = X_test[\"X_test_energy\"]\n",
        "      X_test_mip_position = X_test[\"X_test_mip_position\"]\n",
        "      X_test_rad_position = X_test[\"X_test_rad_position\"]\n",
        "\n",
        "\n",
        "    # fig, axs = plt.subplots(2, 4, figsize=(20, 10))\n",
        "\n",
        "    # # For train data\n",
        "    # train_variables = [X_train_refractive_index, X_train_momentum, X_train_ckov, X_train_mip_position]\n",
        "    # train_labels = ['Train Refractive Index', 'Train Momentum', 'Train CKOV', 'Train MIP Position']\n",
        "\n",
        "    # X_train_mip_position = (np.asarray(X_train_mip_position)).reshape(len(X_train_mip_position), 2)\n",
        "    # print(f\"X_train_mip_position shape = {X_train_mip_position.shape}\")\n",
        "    # for i, variable in enumerate(train_variables):\n",
        "    #     if i < 3:#variable.ndim == 1: # For 1D data\n",
        "    #         axs[0, i].hist(variable, edgecolor='black')\n",
        "    #         axs[0, i].set_title(train_labels[i])\n",
        "    #     else: # For 2D data\n",
        "    #         axs[0, i].hist(X_train_mip_position[:, 0], alpha=0.5, bins = \"auto\", edgecolor='black', label='Dimension 1')\n",
        "    #         axs[0, i].hist(X_train_mip_position[:, 1], alpha=0.5, bins = \"auto\", edgecolor='black', label='Dimension 2')\n",
        "    #         axs[0, i].set_title(train_labels[i])\n",
        "    #         axs[0, i].legend()\n",
        "\n",
        "    # # For test data\n",
        "    # test_variables = [X_test_refractive_index, X_test_momentum, X_test_ckov, X_test_mip_position]\n",
        "    # test_labels = ['Test Refractive Index', 'Test Momentum', 'Test CKOV', 'Test MIP Position']\n",
        "\n",
        "\n",
        "    # X_test_mip_position = (np.asarray(X_test_mip_position)).reshape(len(X_test_mip_position), 2)\n",
        "    # print(f\"X_test_mip_position shape = {X_test_mip_position.shape}\")\n",
        "\n",
        "    # for i, variable in enumerate(test_variables):\n",
        "    #     if i < 3:#variable.ndim == 1: # For 1D data\n",
        "    #         axs[1, i].hist(variable, edgecolor='black')\n",
        "    #         axs[1, i].set_title(test_labels[i])\n",
        "    #     else: # For 2D data\n",
        "    #         axs[1, i].hist(X_test_mip_position[:, 0], alpha=0.5, edgecolor='black', bins = \"auto\", label='Dimension 1')\n",
        "    #         axs[1, i].hist(X_test_mip_position[:, 1], alpha=0.5, edgecolor='black', bins = \"auto\", label='Dimension 2')\n",
        "    #         axs[1, i].set_title(test_labels[i])\n",
        "    #         axs[1, i].legend()\n",
        "\n",
        "    # plt.show()\n",
        "\n",
        "    # X_train_photon_ckov_segmented = np.asarray(X_train[\"X_train_photon_ckov_segmented\"], dtype = object)\n",
        "    # X_test_photon_ckov_segmented = np.asarray(X_test[\"X_test_photon_ckov_segmented\"], dtype = object)\n",
        "\n",
        "    # pion_candidates_train = X_train_photon_ckov_segmented[:,0,:]\n",
        "    # kaon_candidates_train = X_train_photon_ckov_segmented[:,1,:]\n",
        "    # proton_candidates_train = X_train_photon_ckov_segmented[:,2,:]\n",
        "\n",
        "    # # print(f\"proton_candidates_train shape = {proton_candidates_train.shape}\")\n",
        "    # # print(f\"X_train_photon_ckov_segmented shape = {X_train_photon_ckov_segmented.shape}\")\n",
        "\n",
        "\n",
        "    # pion_candidates_test = X_test_photon_ckov_segmented[:,0,:]\n",
        "    # kaon_candidates_test = X_test_photon_ckov_segmented[:,1,:]\n",
        "    # proton_candidates_test = X_test_photon_ckov_segmented[:,2,:]\n",
        "\n",
        "\n",
        "    # len_pion = np.asarray(pion_candidates_test, dtype = object).shape[1]\n",
        "    # len_kaon = np.asarray(kaon_candidates_train, dtype = object).shape[1]\n",
        "    # len_proton = np.asarray(proton_candidates_train, dtype = object).shape[1]\n",
        "\n",
        "    # # print(f\" pion_candidates_test lastdim = {len_pion}\")\n",
        "    # # print(f\" kaon_candidates_train lastdim = {len_kaon}\")\n",
        "    # # print(f\" proton_candidates_train lastdim = {len_proton}\")\n",
        "\n",
        "    # max_value = np.max(pion_candidates_test)\n",
        "\n",
        "    # # Find the minimum value across all dimensions\n",
        "    # min_value = np.min(pion_candidates_test)\n",
        "\n",
        "    # # print(\"pion_candidates_test Maximum value:\", max_value)\n",
        "    # # print(\"pion_candidates_test Minimum value:\", min_value)\n",
        "\n",
        "    # # print(f\" pion_candidates_test lastdim = {np.asarray(pion_candidates_test, dtype = object).shape[1]}\")\n",
        "    # # print(f\" kaon_candidates_test lastdim = {np.asarray(kaon_candidates_test, dtype = object).shape[1]}\")\n",
        "    # # print(f\" proton_candidates_test lastdim = {np.asarray(proton_candidates_test, dtype = object).shape[1]}\")\n",
        "\n",
        "\n",
        "\n",
        "    # pion_candidates_test = X_test_photon_ckov_segmented[:,0,:]\n",
        "    # kaon_candidates_test = X_test_photon_ckov_segmented[:,1,:]\n",
        "    # proton_candidates_test = X_test_photon_ckov_segmented[:,2,:]\n",
        "\n",
        "    # pion_candidates_train = np.array(pion_candidates_train).astype(np.float32)\n",
        "    # kaon_candidates_train = np.array(kaon_candidates_train).astype(np.float32)\n",
        "    # proton_candidates_train = np.array(proton_candidates_train).astype(np.float32)\n",
        "\n",
        "    # # Convert the test candidates to tensors\n",
        "    # pion_candidates_test = np.array(pion_candidates_test).astype(np.float32)\n",
        "    # kaon_candidates_test = np.array(kaon_candidates_test).astype(np.float32)\n",
        "    # proton_candidates_test = np.array(proton_candidates_test).astype(np.float32)\n",
        "\n",
        "    # x_train = np.asarray([pion_candidates_train, kaon_candidates_train, proton_candidates_train], dtype = \"object\")\n",
        "    # x_test = np.asarray([pion_candidates_test, kaon_candidates_test, proton_candidates_test], dtype = \"object\")\n",
        "\n",
        "\n",
        "    # # Convert to TensorFlow tensors\n",
        "    # pion_candidates_train_tf = tf.convert_to_tensor(pion_candidates_train)\n",
        "    # kaon_candidates_train_tf = tf.convert_to_tensor(kaon_candidates_train)\n",
        "    # proton_candidates_train_tf = tf.convert_to_tensor(proton_candidates_train)\n",
        "\n",
        "\n",
        "    # # Convert to TensorFlow tensors\n",
        "    # pion_candidates_test_tf = tf.convert_to_tensor(pion_candidates_test)\n",
        "    # kaon_candidates_test_tf = tf.convert_to_tensor(kaon_candidates_test)\n",
        "    # proton_candidates_test_tf = tf.convert_to_tensor(proton_candidates_test)\n",
        "\n",
        "\n",
        "    # #x_train shape = (3, 1955, 74) pion_candidates_test shape = (489, 74) proton_candidates_train_tf shape = (1955, 74)\n",
        "    # print(f\"x_train shape = {np.asarray(x_train, dtype = object).shape}\")\n",
        "    # print(f\"pion_candidates_test shape = {np.asarray(pion_candidates_test, dtype = object).shape}\")\n",
        "    # print(f\"proton_candidates_train_tf shape = {np.asarray(proton_candidates_train_tf, dtype = object).shape}\")\n",
        "\n",
        "\n",
        "\n",
        "    # fig2, axs2 = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    # strings_desc = [\"pion\", \"kaon\", \"proton\"]\n",
        "    # for i in range(3):\n",
        "    #     axs2[0, i].hist(x_train[i, :, :],  bins=np.arange(0.05, 0.8, 0.01), edgecolor='black')\n",
        "    #     axs2[0, i].set_title(f'Histogram for Ckov Train data: {strings_desc[i]}')\n",
        "    #     axs2[0, i].set_xlabel('Value')\n",
        "    #     axs2[0, i].set_ylabel('Frequency')\n",
        "\n",
        "    #     axs2[1, i].hist(x_test[i, :, :],  bins=np.arange(0.05, 0.8, 0.01), edgecolor='black')\n",
        "    #     axs2[1, i].set_title(f'Histogram for Ckov Test data: {strings_desc[i]}')\n",
        "    #     axs2[1, i].set_xlabel('Value')\n",
        "    #     axs2[1, i].set_ylabel('Frequency')\n",
        "\n",
        "    # plt.show()\n",
        "\n",
        "    # x_train_tf = np.asarray([pion_candidates_train, kaon_candidates_train, proton_candidates_train], dtype = object)\n",
        "    # x_test_tf = np.asarray([pion_candidates_test, kaon_candidates_test, proton_candidates_test], dtype = object)\n",
        "\n",
        "    # print(\"Contains NaN:\", tf.reduce_any(tf.math.is_nan(pion_candidates_test_tf)).numpy())\n",
        "    # print(\"Contains inf:\", tf.reduce_any(tf.math.is_inf(pion_candidates_test_tf)).numpy())\n",
        "    # print(\"Min value:\", tf.reduce_min(pion_candidates_test_tf).numpy())\n",
        "    # print(\"Max value:\", tf.reduce_max(pion_candidates_test_tf).numpy())\n",
        "\n",
        "    # print(f\"x_train_tf shape = {x_train_tf.shape}\")\n",
        "\n",
        "    # fig3, axs3 = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    # for i in range(3):\n",
        "    #     axs3[0, i].hist(x_train_tf[i, :, :],  bins=np.arange(0.1, 0.8, 0.01), edgecolor='black')\n",
        "    #     axs3[0, i].set_title(f'Histogram for TF : Ckov Train data: {strings_desc[i]}')\n",
        "    #     axs3[0, i].set_xlabel('Value')\n",
        "    #     axs3[0, i].set_ylabel('Frequency')\n",
        "\n",
        "    #     axs3[1, i].hist(x_test_tf[i, :, :],  bins=np.arange(0.1, 0.8, 0.01), edgecolor='black')\n",
        "    #     axs3[1, i].set_title(f'Histogram for TF :Ckov Test data: {strings_desc[i]}')\n",
        "    #     axs3[1, i].set_xlabel('Value')\n",
        "    #     axs3[1, i].set_ylabel('Frequency')\n",
        "\n",
        "    # plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def build_model(self, input_sequence_length = None, X_train = None, X_test = None, y_train = None, y_test = None, mask = None):\n",
        "\n",
        "      #map_shape = (None, None, 1)  # Variable shape for the map input\n",
        "      #X_train_photon_ckov_segmented shape : (1955, 3, 74)\n",
        "\n",
        "      X_train_cand_status_encoded = X_train[\"X_train_cand_status_encoded\"] # this is categorical\n",
        "      X_train_cand_pos = X_train[\"X_train_cand_pos\"]  # vectors of x, y pairs, the pair is described as a type of candidate in the previous X_train_cand_status_encoded\n",
        "      X_train_momentum = X_train[\"X_train_momentum\"]\n",
        "      X_train_refractive_index = X_train[\"X_train_refractive_index\"]\n",
        "      X_train_phi = X_train[\"X_train_phi\"]\n",
        "      X_train_theta = X_train[\"X_train_theta\"]\n",
        "      X_train_energy = X_train[\"X_train_energy\"]\n",
        "      X_train_mip_position = X_train[\"X_train_mip_position\"]\n",
        "      X_train_rad_position = X_train[\"X_train_rad_position\"]\n",
        "\n",
        "      X_test_cand_status_encoded = X_test[\"X_test_cand_status_encoded\"]\n",
        "      X_test_cand_pos = X_test[\"X_test_cand_pos\"]\n",
        "      X_test_momentum = X_test[\"X_test_momentum\"]\n",
        "      X_test_refractive_index = X_test[\"X_test_refractive_index\"]\n",
        "      X_test_phi = X_test[\"X_test_phi\"]\n",
        "      X_test_theta = X_test[\"X_test_theta\"]\n",
        "      X_test_energy = X_test[\"X_test_energy\"]\n",
        "      X_test_mip_position = X_test[\"X_test_mip_position\"]\n",
        "      X_test_rad_position = X_test[\"X_test_rad_position\"]\n",
        "\n",
        "\n",
        "\n",
        "      #fig, axs = plt.subplots(2, 4, figsize=(20, 10))\n",
        "\n",
        "      # reshape 1D arrays to 2D :\n",
        "      X_train_mip_position = (np.asarray(X_train_mip_position)).reshape(len(X_train_mip_position), 2)\n",
        "      X_test_mip_position = (np.asarray(X_test_mip_position)).reshape(len(X_test_mip_position), 2)\n",
        "      X_train_rad_position = (np.asarray(X_train_rad_position)).reshape(len(X_train_rad_position), 2)\n",
        "      X_test_rad_position = (np.asarray(X_test_rad_position)).reshape(len(X_test_rad_position), 2)\n",
        "\n",
        "\n",
        "      train_variables = [X_train_cand_status_encoded, X_train_cand_pos, X_train_momentum, X_train_refractive_index, X_train_phi, X_train_theta, X_train_energy, X_train_mip_position, X_train_rad_position]\n",
        "      train_labels = ['Train Candidate Status Encoded', 'Train Candidate Position', 'Train Momentum', 'Train Refractive Index', 'Train Phi', 'Train Theta', 'Train Energy', 'Train MIP Position', 'Train Rad Position']\n",
        "\n",
        "      # If X_train_mip_position is a 1D array and needs to be reshaped to 2D\n",
        "      # X_train_mip_position = (np.asarray(X_train_mip_position)).reshape(len(X_train_mip_position), 2)\n",
        "\n",
        "      # For test data\n",
        "      test_variables = [X_test_cand_status_encoded, X_test_cand_pos, X_test_momentum, X_test_refractive_index, X_test_phi, X_test_theta, X_test_energy, X_test_mip_position, X_test_rad_position]\n",
        "      test_labels = ['Test Candidate Status Encoded', 'Test Candidate Position', 'Test Momentum', 'Test Refractive Index', 'Test Phi', 'Test Theta', 'Test Energy', 'Test MIP Position', 'Test Rad Position']\n",
        "\n",
        "\n",
        "      # TODO: evaluer hva som er best !\n",
        "      # 1. sende inn X_test_cand_status_encoded, X_test_cand_pos som det er naa\n",
        "      # 2. bare sende inn de som har status != 0\n",
        "      # 3 splitte til pion, proton, kaon candidates og sende disse inn\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      # Grid search parameters\n",
        "      # Grid search parameters\n",
        "      filter_sizes = [5]  # Filter sizes to test\n",
        "      num_filters = [32]#[16, 32]  # Number of filters to test\n",
        "      strides = [(2, 2)]#[(1, 1), (2, 2)]  # Strides to test\n",
        "      pool_sizes = [(2, 2)]  # Max pooling sizes to test\n",
        "      fc1_units = [64]#, 128]  # Number of units in fc1 to test\n",
        "      fc2_units = [32]#, 32]  # Number of units in fc2 to test\n",
        "\n",
        "      dropouts = [0.2, 0.3, 0.4]\n",
        "      best_accuracy = 0\n",
        "      best_model = None\n",
        "      alphas = [0.005, 0.05, 0.1]\n",
        "\n",
        "      l1l2_weights_mip = [\n",
        "          [(0.01, 0.01, 0.01), (0.02, 0.02, 0.02), (0.03, 0.03, 0.03), (0.04, 0.04, 0.04), (0.05, 0.05, 0.05),\n",
        "          (0.06, 0.06, 0.06), (0.07, 0.07, 0.07), (0.08, 0.08, 0.08), (0.09, 0.09, 0.09), (0.1, 0.1, 0.1)]\n",
        "      ]\n",
        "      l1l2_weights_mip = [[(w1/10, w2/10, w3/10) for (w1, w2, w3) in sublist] for sublist in l1l2_weights_mip]\n",
        "\n",
        "      l1l2_weights_ref_index = [\n",
        "          [(0.01, 0.01, 0.01), (0.02, 0.02, 0.02), (0.03, 0.03, 0.03), (0.04, 0.04, 0.04), (0.05, 0.05, 0.05),\n",
        "          (0.06, 0.06, 0.06), (0.07, 0.07, 0.07), (0.08, 0.08, 0.08), (0.09, 0.09, 0.09), (0.1, 0.1, 0.1)]\n",
        "      ]\n",
        "      l1l2_weights_ref_index = [[(w1/10, w2/10, w3/10) for (w1, w2, w3) in sublist] for sublist in l1l2_weights_ref_index]\n",
        "\n",
        "\n",
        "      l1l2_weights_pos = [[(w1/10, w2/10, w3/10) for (w1, w2, w3) in sublist] for sublist in l1l2_weights_ref_index]\n",
        "\n",
        "      # #l1l2_weights_hadrons = [\n",
        "      #     [(0.01, 0.01, 0.01), (0.02, 0.02, 0.02), (0.03, 0.03, 0.03), (0.04, 0.04, 0.04), (0.05, 0.05, 0.05),\n",
        "      #     (0.06, 0.06, 0.06), (0.07, 0.07, 0.07), (0.08, 0.08, 0.08), (0.09, 0.09, 0.09), (0.1, 0.1, 0.1),(0.1, 0.1, 0.1),\n",
        "      #     (0.1, 0.1, 0.1),(0.1, 0.1, 0.1),(0.1, 0.1, 0.1),(0.1, 0.1, 0.1),(0.1, 0.1, 0.1),(0.1, 0.1, 0.1),(0.1, 0.1, 0.1),(0.1, 0.1, 0.1),(0.1, 0.1, 0.1)]\n",
        "      # #]\n",
        "      #l1l2_weights_hadrons = [[(w1/10, w2/10, w3/10) for (w1, w2, w3) in sublist] for sublist in l1l2_weights_hadrons]\n",
        "\n",
        "      l1l2_weights_momentum = [\n",
        "          [(0.01, 0.01, 0.01), (0.02, 0.02, 0.02), (0.03, 0.03, 0.03), (0.04, 0.04, 0.04), (0.05, 0.05, 0.05),\n",
        "          (0.06, 0.06, 0.06), (0.07, 0.07, 0.07), (0.08, 0.08, 0.08), (0.09, 0.09, 0.09), (0.1, 0.1, 0.1)]\n",
        "      ]\n",
        "      l1l2_weights_momentum = [[(w1/10, w2/10, w3/10) for (w1, w2, w3) in sublist] for sublist in l1l2_weights_momentum]\n",
        "\n",
        "\n",
        "      l1l2_weights_mip = [[(0.0001, 0.0001, 0.0001) for _ in sublist] for sublist in l1l2_weights_mip]\n",
        "\n",
        "      l1l2_weights_ref_index = [[(0.0001, 0.0001, 0.0001) for _ in sublist] for sublist in l1l2_weights_ref_index]\n",
        "\n",
        "      l1l2_weights_hadrons = [[(0.0001, 0.0001, 0.0001) for _ in sublist] for sublist in l1l2_weights_hadrons]\n",
        "\n",
        "      l1l2_weights_momentum = [[(0.0001, 0.0001, 0.0001) for _ in sublist] for sublist in l1l2_weights_momentum]\n",
        "\n",
        "\n",
        "\n",
        "      # just assign very small first\n",
        "      l1l2_weights_rad = [[(0.0001, 0.0001, 0.0001) for _ in sublist] for sublist in l1l2_weights_mip]\n",
        "      l1l2_weights_energy = [[(0.0001, 0.0001, 0.0001) for _ in sublist] for sublist in l1l2_weights_mip]\n",
        "      l1l2_weights_phi = [[(0.0001, 0.0001, 0.0001) for _ in sublist] for sublist in l1l2_weights_mip]\n",
        "      l1l2_weights_theta = [[(0.0001, 0.0001, 0.0001) for _ in sublist] for sublist in l1l2_weights_mip]\n",
        "\n",
        "\n",
        "\n",
        "      # i need help with these two :\n",
        "      l1l2_weights_cand_pos = [[(0.0001, 0.0001, 0.0001) for _ in sublist] for sublist in l1l2_weights_mip]\n",
        "      l1l2_weights_cand_status = [[(0.0001, 0.0001, 0.0001) for _ in sublist] for sublist in l1l2_weights_mip]\n",
        "\n",
        "\n",
        "      print(f\"l1l2_weights_mip shape {np.array(l1l2_weights_mip, dtype =object).shape}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      # Define inputs\n",
        "\n",
        "      # Define maximum lengths\n",
        "      max_length_status = np.asarray(X_test_cand_status_encoded, dtype=object).shape[1]\n",
        "      max_length_pos = np.asarray(X_test_cand_pos, dtype=object).shape[1]\n",
        "\n",
        "      max_length_status_train = np.asarray(X_train_cand_status_encoded, dtype=object).shape[1]\n",
        "      max_length_pos_train = np.asarray(X_train_cand_pos, dtype=object).shape[1]\n",
        "\n",
        "      print(f\"Test : max_length_status {max_length_status} | max_length_pos {max_length_pos} \")\n",
        "      print(f\"Train : max_length_status_train {max_length_status_train} | max_length_pos_train {max_length_pos_train} \")\n",
        "\n",
        "      print(f\"Train : statShape {X_train_cand_status_encoded.shape} | posShape {X_train_pos.shape} \")\n",
        "      print(f\"Test : statShape {X_test_cand_status_encoded.shape} | posShape {X_test_pos.shape} \")\n",
        "\n",
        "\n",
        "      if max_length_status_train != max_length_status or max_length_pos_train != max_length_pos:\n",
        "          raise Exception(\"Dimensions not the same!!!\")\n",
        "\n",
        "      momentum_shape = refractive_index_shape = phi_shape = theta_shape = energy_shape = (1,)\n",
        "      mip_position_shape = rad_position_shape = (2,)\n",
        "      input_cand_status = Input(shape=(max_length_status, 1), name=\"cand_status_input\")\n",
        "      input_cand_pos = Input(shape=(max_length_pos,2), name=\"cand_pos_input\")          # this has NumSamples x maxLength x 2?\n",
        "\n",
        "      # new :\n",
        "      energy_input = Input(shape=energy_shape, name=\"energy_input\")\n",
        "      phi_input = Input(shape=phi_shape, name=\"phi_shape\")\n",
        "      theta_input = Input(shape=theta_shape, name=\"theta_shape\")\n",
        "\n",
        "\n",
        "      momentum_input = Input(shape=momentum_shape, name=\"momentum_input\")\n",
        "      refractive_index_input = Input(shape=refractive_index_shape, name=\"refractive_index_input\")\n",
        "\n",
        "      rad_position_input = Input(shape=rad_position_shape, name=\"rad_position_input\")\n",
        "      mip_position_input = Input(shape=mip_position_shape, name=\"mip_position_input\")\n",
        "\n",
        "\n",
        "\n",
        "      ######\n",
        "\n",
        "      # Merging the position and categorical status values :\n",
        "      ######\n",
        "\n",
        "\n",
        "      # Embedding layer for categorical data\n",
        "\n",
        "      n_categories = 8 # number of possible statuses the candidate can have from mass-hypothesis\n",
        "      embedding_dim = 50 # each category 0..7 is mapped into a unique 50-dimensionality vector\n",
        "      embedding = Embedding(input_dim=n_categories, output_dim=embedding_dim)(input_cand_status)\n",
        "      flatten_status = Flatten()(embedding)\n",
        "\n",
        "      dense_cand_pos = Dense(unit, name=f\"dense_cand_pos_init\")#,kernel_regularizer=regularizers.L1L2(l1l2_weights_cand_pos[0][i - 1][0], l1l2_weights_cand_pos[0][i - 1][1]), bias_regularizer=regularizers.L1(l1l2_weights_cand_pos[0][i - 1][2]))(prev_cand_pos)\n",
        "\n",
        "      bn_cand_pos = BatchNormalization(name=f\"bn_cand_pos_init\")(dense_cand_pos)\n",
        "      leakyrelu_cand_pos = LeakyReLU(alpha=alpha, name=f\"leakyrelu_cand_pos_init\")(bn_cand_pos)\n",
        "      dropout_cand_pos = Dropout(dropout, name=f\"dropout_cand_pos_init\")(leakyrelu_cand_pos)\n",
        "      prev_cand_pos = dropout_cand_pos\n",
        "\n",
        "\n",
        "      # Merge\n",
        "      merged_candidate_input = concatenate([flatten_status, prev_cand_pos])\n",
        "\n",
        "      # to be sent to candidate merged loop for first : merged_candidate_input\n",
        "      prev_cand_merged = merged_candidate_input\n",
        "\n",
        "\n",
        "      inputs = [merged_candidate_input, input_cand_pos, energy_input, phi_input, theta_input, refractive_index_input, momentum_input, rad_position_input, mip_position_input]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      for dropout in dropouts:\n",
        "          for num_filter in num_filters:\n",
        "              for stride in strides:\n",
        "                  for alpha in alphas:\n",
        "                      for fc1_unit in fc1_units:\n",
        "                          for fc2_unit in fc2_units:\n",
        "                              # check for                               #nan_count = np.isnan(pion_candidates_test).sum()\n",
        "\n",
        "                              # print the number of nan\n",
        "                              #print(f\"Number of nan values in pion_candidates_test = {nan_count}\")\n",
        "                              # pion_ip_mask = Masking(mask_value=0.)(pion_ip)\n",
        "                              # kaon_ip_mask = Masking(mask_value=0.)(kaon_ip)\n",
        "                              # proton_ip_mask = Masking(mask_value=0.)(proton_ip)\n",
        "\n",
        "                              #prev_dist2mip_layer = dist2mip_masked\n",
        "\n",
        "                              units = [fc1_unit * i for i in [1, 2, 4, 8, 16, 32, 64, 128, 64, 32, 16, 8, 4, 2, 1, 0.5, 0.25]]\n",
        "                              units = [fc1_unit * i for i in [1, 2, 4, 8, 16, 32, 16, 8, 4, 2, 1, 0.5, 0.25]]\n",
        "                              units = [fc1_unit * i for i in [1, 2, 4, 8, 16, 8, 4, 2, 1, 0.5, 0.25]]\n",
        "\n",
        "\n",
        "                              #prev_cand_status = input_cand_status # NB needs masking?\n",
        "                              #prev_cand_pos = input_cand_pos # NB needs mask\n",
        "\n",
        "\n",
        "\n",
        "                              # merge before here and send merege here ? :\n",
        "                              # need to combine the cand_pos w the cand_status here :!!!\n",
        "                              for i in range(1, len(units)):\n",
        "                                  unit = int(units[i])\n",
        "\n",
        "                                  # For pion\n",
        "                                  dense_cand_merged_i = Dense(unit, name=f\"dense_cand_merged_{i}\")(prev_cand_pos)#,\n",
        "                                                      # kernel_regularizer=regularizers.L1L2(l1l2_weights_hadrons[0][i - 1][0], l1l2_weights_hadrons[0][i - 1][1]), # correct the names of the weights\n",
        "                                                      # bias_regularizer=regularizers.L1(l1l2_weights_hadrons[0][i - 1][2]))(prev_cand_pos)\n",
        "\n",
        "                                  bn_cand_merged_i = BatchNormalization(name=f\"bn_cand_merged_{i}\")(dense_cand_merged_i)\n",
        "                                  leakyrelu_cand_merged_i = LeakyReLU(alpha=alpha, name=f\"leakyrelu_cand_merged_{i}\")(bn_cand_merged_i)\n",
        "                                  dropout_cand_merged_i = Dropout(dropout, name=f\"dropout_cand_merged_{i}\")(leakyrelu_cand_merged_i)\n",
        "                                  prev_cand_merged = dropout_cand_merged_i\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                              units2 = [fc1_unit * i for i in [1, 2, 4, 8, 16, 4, 2, 1, 0.5, 0.25]]\n",
        "                              units2 = [fc1_unit * i for i in [1, 2, 4, 8, 4, 2, 1, 0.5, 0.25]]\n",
        "\n",
        "                              prev_mip = mip_position_input\n",
        "                              prev_momentum = momentum_input\n",
        "                              prev_ref_index = refractive_index_input\n",
        "\n",
        "                              # inputs = [input_cand_status, input_cand_pos, energy_input, phi_input, theta_input, refractive_index_input, momentum_input, rad_position_input, mip_position_input]\n",
        "\n",
        "                              prev_rad = rad_position_input\n",
        "                              prev_phi = phi_input\n",
        "                              prev_theta = theta_input\n",
        "                              prev_energy = energy_input\n",
        "\n",
        "                              for i in range(1, len(units2)):\n",
        "                                  step = int(units2[i])\n",
        "\n",
        "                                  #### REFRACTIVE INDEX #####\n",
        "                                  dense_ref_index_i = Dense(step, name=f\"dense_ref_index_{i}\",\n",
        "                                                            kernel_regularizer=regularizers.L1L2(l1l2_weights_ref_index[0][i - 1][0], l1l2_weights_ref_index[0][i - 1][1]),\n",
        "                                                            bias_regularizer=regularizers.L1(l1l2_weights_ref_index[0][i - 1][2]))(prev_ref_index)\n",
        "\n",
        "                                  bn_ref_index_i = BatchNormalization(name=f\"bn_ref_index_{i}\")(dense_ref_index_i)\n",
        "                                  leakyrelu_ref_index_i = LeakyReLU(alpha=alpha, name=f\"leakyrelu_ref_index_{i}\")(bn_ref_index_i)\n",
        "                                  dropout_ref_index_i = Dropout(dropout, name=f\"dropout_ref_index_{i}\")(leakyrelu_ref_index_i)\n",
        "\n",
        "                                  #### MOMENTUM #####\n",
        "                                  dense_momentum_i = Dense(step, name=f\"dense_momentum_{i}\",\n",
        "                                                          kernel_regularizer=regularizers.L1L2(l1l2_weights_momentum[0][i - 1][0], l1l2_weights_momentum[0][i - 1][1]),\n",
        "                                                          bias_regularizer=regularizers.L1(l1l2_weights_momentum[0][i - 1][2]))(prev_momentum)\n",
        "                                  bn_momentum_i = BatchNormalization(name=f\"bn_momentum_{i}\")(dense_momentum_i)\n",
        "                                  leakyrelu_momentum_i = LeakyReLU(alpha=alpha, name=f\"leakyrelu_momentum_{i}\")(bn_momentum_i)\n",
        "                                  dropout_momentum_i = Dropout(dropout, name=f\"dropout_momentum_{i}\")(leakyrelu_momentum_i)\n",
        "\n",
        "\n",
        "                                  #### MIP POS #####\n",
        "                                  dense_mip_pos_i = Dense(step, name=f\"dense_mip_pos_{i}\",\n",
        "                                                          kernel_regularizer=regularizers.L1L2(l1l2_weights_mip[0][i - 1][0], l1l2_weights_mip[0][i - 1][1]),\n",
        "                                                          bias_regularizer=regularizers.L1(l1l2_weights_mip[0][i - 1][2]))(prev_ref_index)\n",
        "\n",
        "                                  bn_mip_pos_i = BatchNormalization(name=f\"bn_mip_pos_{i}\")(dense_mip_pos_i)\n",
        "                                  leakyrelu_mip_pos_i = LeakyReLU(alpha=alpha, name=f\"leakyrelu_mip_pos_{i}\")(bn_mip_pos_i)\n",
        "                                  dropout_mip_pos_i = Dropout(dropout, name=f\"dropout_mip_pos_{i}\")(leakyrelu_mip_pos_i)\n",
        "\n",
        "\n",
        "                                  # ENERGY\n",
        "                                  dense_energy_i = Dense(step, name=f\"dense_energy_{i}\",\n",
        "                                                      kernel_regularizer=regularizers.L1L2(l1l2_weights_energy[0][i - 1][0], l1l2_weights_energy[0][i - 1][1]),\n",
        "                                                      bias_regularizer=regularizers.L1(l1l2_weights_energy[0][i - 1][2]))(prev_energy)\n",
        "                                  bn_energy_i = BatchNormalization(name=f\"bn_energy_{i}\")(dense_energy_i)\n",
        "                                  leakyrelu_energy_i = LeakyReLU(alpha=alpha, name=f\"leakyrelu_energy_{i}\")(bn_energy_i)\n",
        "                                  dropout_energy_i = Dropout(dropout, name=f\"dropout_energy_{i}\")(leakyrelu_energy_i)\n",
        "\n",
        "                                  # PHI\n",
        "                                  dense_phi_i = Dense(step, name=f\"dense_phi_{i}\",\n",
        "                                                      kernel_regularizer=regularizers.L1L2(l1l2_weights_phi[0][i - 1][0], l1l2_weights_phi[0][i - 1][1]),\n",
        "                                                      bias_regularizer=regularizers.L1(l1l2_weights_phi[0][i - 1][2]))(prev_phi)\n",
        "                                  bn_phi_i = BatchNormalization(name=f\"bn_phi_{i}\")(dense_phi_i)\n",
        "                                  leakyrelu_phi_i = LeakyReLU(alpha=alpha, name=f\"leakyrelu_phi_{i}\")(bn_phi_i)\n",
        "                                  dropout_phi_i = Dropout(dropout, name=f\"dropout_phi_{i}\")(leakyrelu_phi_i)\n",
        "\n",
        "                                  # THETA\n",
        "                                  dense_theta_i = Dense(step, name=f\"dense_theta_{i}\",\n",
        "                                                      kernel_regularizer=regularizers.L1L2(l1l2_weights_theta[0][i - 1][0], l1l2_weights_theta[0][i - 1][1]),\n",
        "                                                      bias_regularizer=regularizers.L1(l1l2_weights_theta[0][i - 1][2]))(prev_theta)\n",
        "                                  bn_theta_i = BatchNormalization(name=f\"bn_theta_{i}\")(dense_theta_i)\n",
        "                                  leakyrelu_theta_i = LeakyReLU(alpha=alpha, name=f\"leakyrelu_theta_{i}\")(bn_theta_i)\n",
        "                                  dropout_theta_i = Dropout(dropout, name=f\"dropout_theta_{i}\")(leakyrelu_theta_i)\n",
        "\n",
        "                                  # RAD_POSITION\n",
        "                                  dense_rad_position_i = Dense(step, name=f\"dense_rad_position_{i}\",\n",
        "                                                      kernel_regularizer=regularizers.L1L2(l1l2_weights_rad[0][i - 1][0], l1l2_weights_rad[0][i - 1][1]),\n",
        "                                                      bias_regularizer=regularizers.L1(l1l2_weights_rad[0][i - 1][2]))(prev_rad)\n",
        "                                  bn_rad_position_i = BatchNormalization(name=f\"bn_rad_position_{i}\")(dense_rad_position_i)\n",
        "                                  leakyrelu_rad_position_i = LeakyReLU(alpha=alpha, name=f\"leakyrelu_rad_position_{i}\")(bn_rad_position_i)\n",
        "                                  dropout_rad_position_i = Dropout(dropout, name=f\"dropout_rad_position_{i}\")(leakyrelu_rad_position_i)\n",
        "\n",
        "\n",
        "\n",
        "                                  prev_mip = dropout_mip_pos_i\n",
        "                                  prev_momentum = dropout_momentum_i\n",
        "                                  prev_ref_index = dropout_ref_index_i\n",
        "\n",
        "                                  prev_rad = dropout_rad_pos_i\n",
        "                                  prev_energy = dropout_energy_i\n",
        "                                  prev_phi = dropout_phi_i\n",
        "                                  prev_theta = dropout_theta_i\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                              epochs = 400\n",
        "                              lr = create_lr_scheduler(num_epochs=epochs)\n",
        "\n",
        "                              # inputs = [input_cand_status, input_cand_pos, energy_input, phi_input, theta_input, refractive_index_input, momentum_input, rad_position_input, mip_position_input]\n",
        "\n",
        "\n",
        "                              # ??\n",
        "                              #concat_layers_ev = [prev_cand_status, prev_cand_pos, prev_energy, prev_phi, prev_theta, prev_ref_index, prev_momentum, prev_rad, prev_mip] # this needs change\n",
        "                              #concat_layers = [prev_pion, prev_kaon, prev_proton]\n",
        "                              # dense_concat = Dense(16, name=\"dense_concat\")(concat)\n",
        "                              # bn_concat = BatchNormalization(name=\"bn_concat\")(dense_concat)\n",
        "                              # relu_concat = LeakyReLU(alpha=alpha, name=f\"relu_concat\")(bn_concat)\n",
        "                              # dropout_concat = Dropout(dropout, name=\"dropout_concat\")(relu_concat)\n",
        "\n",
        "\n",
        "                              concat_layers = [prev_cand_merged, prev_cand_pos, prev_energy, prev_phi, prev_theta, prev_ref_index, prev_momentum, prev_rad, prev_mip] # this needs change\n",
        "\n",
        "                              #inputs=[pion_ip, kaon_ip, proton_ip]#,refractive_index_input, momentum_input, mip_position_input]\n",
        "\n",
        "\n",
        "                              # to specify specific number of inputs :\n",
        "                              # inputs_ev = [pion_ip, refractive_index_input, momentum_input, mip_position_input] # this\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                              # for kun å evaluere en hvis type IPs\n",
        "                              #inputs, concat_layers, x_input_test, x_input_train = [], [], [], []\n",
        "                              #x_input_train_ev = [pion_candidates_train_tf, kaon_candidates_train_tf, proton_candidates_train_tf, X_train[\"X_train_refractive_index\"],X_train[\"X_train_momentum\"], X_train[\"X_train_mip_position\"]]\n",
        "                              #x_input_test_ev = [pion_candidates_test_tf, kaon_candidates_test_tf, proton_candidates_test_tf, X_test[\"X_test_refractive_index\"],X_test[\"X_test_momentum\"], X_test[\"X_test_mip_position\"]]\n",
        "\n",
        "\n",
        "                              # senere : for å kun ta med en hvis type parametere :\n",
        "                              # for index in range(len(inputs_ev)):\n",
        "                              #   if mask[index] == 1:\n",
        "                              #     inputs.append(inputs_ev[index])\n",
        "                              #     concat_layers.append(concat_layers_ev[index])\n",
        "                              #     x_input_train.append(x_input_train_ev[index])\n",
        "                              #     x_input_test.append(x_input_test_ev[index])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                              concat = concatenate(concat_layers)\n",
        "                              output = Dense(3, activation='softmax')(concat) # NB LEGG MERKE TIL AT DENNE ER ENDRET FRA FC2\n",
        "\n",
        "                              #x_input_train = [pion_candidates_train_tf, kaon_candidates_train_tf, proton_candidates_train_tf]\n",
        "                              #x_input_test = [pion_candidates_test_tf, kaon_candidates_test_tf, proton_candidates_test_tf]\n",
        "\n",
        "\n",
        "                              for i, array in enumerate(x_input_train):\n",
        "                                  if np.any(np.isnan(array)):\n",
        "                                      print(f\"Array at index {i} contains NaN values.\")\n",
        "                                  else:\n",
        "                                      print(f\"Array at index {i} does not contain NaN values.\")\n",
        "\n",
        "                              for input_layer in inputs:\n",
        "                                  print(f\"input = {input_layer}, input_shape = {input_layer.shape}, input_name = {input_layer.name}\")\n",
        "\n",
        "\n",
        "                              for layer in concat_layers:\n",
        "                                  print(f\"layer = {layer}, output_shape = {layer.shape}, layer_name = {layer.name}\")\n",
        "\n",
        "\n",
        "\n",
        "                              for i, input in enumerate(inputs):\n",
        "                                print(f\"input = {input} || inputname ={input.name}\")\n",
        "                                print(f\"output = {concat_layers[i]} | output_shape = {concat_layers[i].name}\")\n",
        "                                try:\n",
        "                                  print(f\"x_input_train_shape{i} = {np.asarray(np.asarray(x_input_train[i]), dtype=object).shape}\")\n",
        "                                except Exception as e:\n",
        "                                  print(f\"x_input_train_shape failed w {e}\")\n",
        "\n",
        "                              model = Model(inputs = inputs,\n",
        "                                            outputs=output)\n",
        "\n",
        "                              #model = Model(inputs=[window_input], outputs=[output])\n",
        "                              model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), # NB changed from 0.0002\n",
        "                                            loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "                              model.summary()\n",
        "\n",
        "                              # later change back?\n",
        "                              x_input_train = X_train, x_input_test = X_test\n",
        "                              # Train the model\n",
        "                              history = model.fit(\n",
        "                                  x=x_input_train, # x=[X_train_dist2mip, X_train_momentum, X_train_refractive_index, X_train_ckov, X_train_mip_position]\n",
        "                                  y=y_train,\n",
        "                                  validation_data=(\n",
        "                                      x_input_test,# X_test[\"X_test_refractive_index\"], X_test[\"X_test_momentum\"], X_test[\"X_test_mip_position\"]],\n",
        "                                      y_test\n",
        "                                  ),\n",
        "                                  batch_size=32,\n",
        "                                  epochs=epochs,\n",
        "                                  verbose=1, #NB! kommenterte ut detee\n",
        "                                  callbacks = [lr]\n",
        "                              )\n",
        "\n",
        "                            #   # plotting the worst cases?:\n",
        "                            #   #y_pred_test = model.predict([X_test[\"X_test_map\"], X_test[\"X_test_momentum\"], X_test[\"X_test_refractive_index\"], X_test[\"X_test_mip_position\"]])\n",
        "                            #   #plot_worst(model, y_test, X_test_map, X_test_momentum, X_test_refractive_index, X_test_ckov, X_test_mip_position, y_pred):\n",
        "                            #   print(\"Shape of y_test: \", y_test.shape)\n",
        "                            #   #print(\"Shape of X_test_map: \", X_test[\"X_test_map\"].shape)\n",
        "                            # # print(\"Shape of X_test_windows: \", X_test[\"X_test_windows\"].shape)\n",
        "                            #   print(\"Shape of X_test_momentum: \", X_test[\"X_test_momentum\"].shape)\n",
        "                            #   print(\"Shape of X_test_refractive_index: \", X_test[\"X_test_refractive_index\"].shape)\n",
        "                            #   #print(\"Shape of X_test_ckov: \", X_test[\"X_test_ckov\"].shape)\n",
        "                            #   print(\"Shape of X_test_mip_position: \", X_test[\"X_test_mip_position\"].shape)\n",
        "                            #   #print(\"Shape of y_pred_test: \", y_pred_test.shape)\n",
        "                            #   # try: # NB! commented out this line\n",
        "                            #   #   #plot_worst_(model, y_test, X_test[\"X_test_map\"], X_test[\"X_test_momentum\"], X_test[\"X_test_refractive_index\"], X_test[\"X_test_ckov\"], X_test[\"X_test_mip_position\"], y_pred_test)\n",
        "                            #   #   #plot_worst_(model, y_test, X_test[\"X_test_map\"], X_test[\"X_test_momentum\"], X_test[\"X_test_refractive_index\"], X_test[\"X_test_ckov\"], X_test[\"X_test_mip_position\"], self.resolution)\n",
        "                            #   # except Exception as e:\n",
        "                            #   #   print(f\"skip plot_worst_ due to error: {e}\")\n",
        "\n",
        "                              y_pred_train = model.predict(x_input_train)\n",
        "                              # y_pred_train = model.predict([pion_candidates_train_tf, kaon_candidates_train_tf, proton_candidates_train_tf,\n",
        "                              #                               X_train[\"X_train_refractive_index\"], X_train[\"X_train_momentum\"],\n",
        "                              #                               X_train[\"X_train_mip_position\"]])\n",
        "                              y_pred_test = model.predict(x_input_test)\n",
        "                              # y_pred_test = model.predict([pion_candidates_test_tf, kaon_candidates_test_tf, proton_candidates_test_tf,\n",
        "                              #                              X_test[\"X_test_refractive_index\"], X_test[\"X_test_momentum\"],\n",
        "                              #                              X_test[\"X_test_mip_position\"]])\n",
        "\n",
        "\n",
        "                              #    def plot_training_history(self, history=None, vector_of_weights=None, vector_of_weights2=None, dropout=None, y_pred_train=None, y_pred_test=None, y_train_true=None, y_test_true=None):\n",
        "                              plot_training_history(history = history, vector_of_weights = units, vector_of_weights2 = units2,\n",
        "                                                    dropout = dropout, y_pred_train = y_pred_train, y_pred_test = y_pred_test, y_train_true = y_train,  y_test_true = y_test, relu_alpha = alpha)\n",
        "                              #self.plot_training_history(history, units, units2)\n",
        "                              #self.plot_training_history(history, units, units2)\n",
        "\n",
        "                              # Evaluate the model\n",
        "\n",
        "                              # x_ip_test = [pion_candidates_test_tf, kaon_candidates_test_tf, proton_candidates_test_tf,\n",
        "                              #         X_test[\"X_test_refractive_index\"], X_test[\"X_test_momentum\"], X_test[\"X_test_mip_position\"]]\n",
        "                              x_ip_test = x_input_test\n",
        "                              _, accuracy = model.evaluate(\n",
        "                                  x= x_ip_test,\n",
        "                                  y=y_test,\n",
        "                                  verbose=1\n",
        "                              )\n",
        "\n",
        "                              print(f\"Model Accuracy: {accuracy:.4f} (fc1 Size: {fc1_unit}, Num fc2: {fc2_unit}, dropout = {dropout}\")\n",
        "\n",
        "\n",
        "                              # Check if the current model configuration is better\n",
        "                              if accuracy > best_accuracy:\n",
        "                                  best_accuracy = accuracy\n",
        "                                  best_model = model\n",
        "\n",
        "      # Set the best model as the final model\n",
        "      self.model = best_model\n",
        "\n",
        "\n",
        "  def train_model(self, X_train, X_test, y_train, y_test):\n",
        "      # Compile the model\n",
        "\n",
        "\n",
        "      X_train_cand_status_encoded = X_train[\"X_train_cand_status_encoded\"]\n",
        "      X_train_cand_pos = X_train[\"X_train_cand_pos\"]\n",
        "      X_train_momentum = X_train[\"X_train_momentum\"]\n",
        "      X_train_refractive_index = X_train[\"X_train_refractive_index\"]\n",
        "      X_train_phi = X_train[\"X_train_phi\"]\n",
        "      X_train_theta = X_train[\"X_train_theta\"]\n",
        "      X_train_energy = X_train[\"X_train_energy\"]\n",
        "      X_train_mip_position = X_train[\"X_train_mip_position\"]\n",
        "      X_train_rad_position = X_train[\"X_train_rad_position\"]\n",
        "\n",
        "      X_test_cand_status_encoded = X_test[\"X_test_cand_status_encoded\"]\n",
        "      X_test_cand_pos = X_test[\"X_test_cand_pos\"]\n",
        "      X_test_momentum = X_test[\"X_test_momentum\"]\n",
        "      X_test_refractive_index = X_test[\"X_test_refractive_index\"]\n",
        "      X_test_phi = X_test[\"X_test_phi\"]\n",
        "      X_test_theta = X_test[\"X_test_theta\"]\n",
        "      X_test_energy = X_test[\"X_test_energy\"]\n",
        "      X_test_mip_position = X_test[\"X_test_mip_position\"]\n",
        "      X_test_rad_position = X_test[\"X_test_rad_position\"]\n",
        "\n",
        "      self.model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "      print(\"Fields in the first vector of X_train:\")\n",
        "      print(\"X_train_cand_status_encoded shape:\", X_train_cand_status_encoded.shape)\n",
        "      print(\"X_train_cand_pos shape:\", X_train_cand_pos.shape)\n",
        "      print(\"X_train_momentum shape:\", X_train_momentum.shape)\n",
        "      print(\"X_train_refractive_index shape:\", X_train_refractive_index.shape)\n",
        "      print(\"X_train_phi shape:\", X_train_phi.shape)\n",
        "      print(\"X_train_theta shape:\", X_train_theta.shape)\n",
        "      print(\"X_train_energy shape:\", X_train_energy.shape)\n",
        "      print(\"X_train_mip_position shape:\", X_train_mip_position.shape)\n",
        "      print(\"X_train_rad_position shape:\", X_train_rad_position.shape)\n",
        "\n",
        "      print(\"\\nFields in the first vector of X_test:\")\n",
        "      print(\"X_test_cand_status_encoded shape:\", X_test_cand_status_encoded.shape)\n",
        "      print(\"X_test_cand_pos shape:\", X_test_cand_pos.shape)\n",
        "      print(\"X_test_momentum shape:\", X_test_momentum.shape)\n",
        "      print(\"X_test_refractive_index shape:\", X_test_refractive_index.shape)\n",
        "      print(\"X_test_phi shape:\", X_test_phi.shape)\n",
        "      print(\"X_test_theta shape:\", X_test_theta.shape)\n",
        "      print(\"X_test_energy shape:\", X_test_energy.shape)\n",
        "      print(\"X_test_mip_position shape:\", X_test_mip_position.shape)\n",
        "      print(\"X_test_rad_position shape:\", X_test_rad_position.shape)\n",
        "\n",
        "\n",
        "      # Print the first element of y_train\n",
        "      print(\"First element of y_train:\", y_train[0])\n",
        "\n",
        "      # Train the model\n",
        "      history = self.model.fit(\n",
        "          x=[X_train],\n",
        "          y=y_train,\n",
        "          validation_data=(\n",
        "              [X_test],\n",
        "              y_test\n",
        "          ),\n",
        "          batch_size=16,\n",
        "          epochs=10,\n",
        "          verbose=1\n",
        "      )\n",
        "      return history\n",
        "\n",
        "  def evaluate_model(self, X_test, y_test):\n",
        "\n",
        "\n",
        "      X_test_cand_status_encoded = X_test[\"X_test_cand_status_encoded\"]\n",
        "      X_test_cand_pos = X_test[\"X_test_cand_pos\"]\n",
        "      X_test_momentum = X_test[\"X_test_momentum\"]\n",
        "      X_test_refractive_index = X_test[\"X_test_refractive_index\"]\n",
        "      X_test_phi = X_test[\"X_test_phi\"]\n",
        "      X_test_theta = X_test[\"X_test_theta\"]\n",
        "      X_test_energy = X_test[\"X_test_energy\"]\n",
        "      X_test_mip_position = X_test[\"X_test_mip_position\"]\n",
        "      X_test_rad_position = X_test[\"X_test_rad_position\"]\n",
        "      loss, accuracy = self.model.evaluate(\n",
        "          x=X_test,\n",
        "          y=y_test,\n",
        "          verbose=0\n",
        "      )\n",
        "      print(f\"Test Loss: {loss:.4f}\")\n",
        "      print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "\n",
        "  def train(self, X_train, X_test, y_train, y_test, mask):\n",
        "\n",
        "      #X_ckov_segm = X_train[\"X_train_photon_ckov_segmented\"]\n",
        "      #print(f\"X_train_photon_ckov_segmented shape = {np.asarray(X_ckov_segm, dtype = object).shape}\")\n",
        "      X_train_photon_ckov_segmented = np.asarray(X_train[\"X_train_photon_ckov_segmented\"])\n",
        "\n",
        "\n",
        "\n",
        "      print(f\" in  def train(self, filename) : X_train_photon_ckov_segmented shape : {X_train_photon_ckov_segmented.shape}\")\n",
        "      #X_train_dist2mip = X_train_dist2mip.reshape(X_train_dist2mip.shape[0], X_train_dist2mip.shape[1],, 1)\n",
        "      #for dist in X_train_dist2mip:\n",
        "      #  print(f\"dist = {dist}\")\n",
        "\n",
        "\n",
        "      input_sequence_length = len(max(X_train_photon_ckov_segmented, key=len))\n",
        "\n",
        "      self.build_model(input_sequence_length = input_sequence_length, X_train = X_train, X_test = X_test, y_train = y_train, y_test = y_test, mask = mask)\n",
        "\n",
        "\n",
        "      try:\n",
        "        history = self.train_model(X_train, X_test, y_train, y_test)\n",
        "        self.evaluate_model(X_test, y_test)\n",
        "      except Exception as e:\n",
        "        print(\"    def train(self, filename) failed at history = self.train_model bc of error : {e} \")\n",
        "\n",
        "      #plot_training_history(self, history, vector_of_weights, vector_of_weights2, dropout):\n",
        "      #self.plot_training_history(history)\n",
        "\n",
        "\n",
        "\n",
        "def plot_worst(model, y_test, X_test_map, X_test_momentum, X_test_refractive_index, X_test_ckov, X_test_mip_position, y_pred):\n",
        "  # 1. Predict labels on validation data\n",
        "\n",
        "  # 2. Calculate the difference between predicted and actual labels\n",
        "  losses = tf.keras.losses.categorical_crossentropy(y_test, y_pred).numpy()\n",
        "\n",
        "  # Sort the indices of the losses from highest to lowest\n",
        "  sorted_indices = np.argsort(losses)[::-1]\n",
        "\n",
        "  # Get the indices of the worst performing 10%\n",
        "  worst_10_percent_indices = sorted_indices[:int(0.1*len(sorted_indices))]\n",
        "\n",
        "  # Create figure and axes\n",
        "  num_plots = len(worst_10_percent_indices)\n",
        "  #fig, axes = plt.subplots(num_plots, 1, figsize=(8, 20))\n",
        "  fig, axes = plt.subplots(num_plots,figsize=(8, 20))\n",
        "\n",
        "  # Define mass categories\n",
        "  mass_categories = [\"pion\", \"kaon\", \"proton\"]\n",
        "\n",
        "  # 3. Create plots for these cases, including their feature information and predicted vs actual labels\n",
        "  for i, index in enumerate(worst_10_percent_indices):\n",
        "      # Get the map and corresponding information\n",
        "      map_data = X_test_map[index, :, :]\n",
        "      actual_mass_category = mass_categories[np.argmax(y_test[index])]\n",
        "\n",
        "      print(f\"y_test[index] = {y_test[index]}\")\n",
        "\n",
        "      predicted_mass_category = mass_categories[np.argmax(y_pred[index])]\n",
        "      ckov = X_test_ckov[index]\n",
        "      mip_position = X_test_mip_position[index]\n",
        "      momentum = X_test_momentum[index]\n",
        "      refractive_index = X_test_refractive_index[index]\n",
        "\n",
        "      mass_actual = momentum * np.sqrt(refractive_index**2 * np.cos(ckov)*np.cos(ckov) - 1)\n",
        "\n",
        "      # Check if the value is NaN (invalid Cherenkov angle)\n",
        "      if np.isnan(mass_actual):\n",
        "          mass_actual = \"Invalid\"\n",
        "\n",
        "      # Plot the map\n",
        "      axes[i].imshow(map_data, cmap='gray')\n",
        "\n",
        "      # Add a red dot at the MIP position\n",
        "      axes[i].plot(mip_position[0], mip_position[1], 'ro')\n",
        "\n",
        "      # Set the title with the information\n",
        "      axes[i].set_title(f\"Actual Mass\")#: {actual_mass_category}, Predicted Mass: {predicted_mass_category},\\nMass: {mass_actual}, Mass_prob = {y_pred[index]} \\nCKOV: {ckov}, MIP Position: {mip_position}, \\nMomentum: {momentum}, Refractive Index: {refractive_index}\")\n",
        "      #\n",
        "      axes[i].set_title(f\"Actual Mass: {actual_mass_category}, Predicted Mass: {predicted_mass_category},\\nMass: {mass_actual}, Mass_prob = {y_pred[index]} \\nCKOV: {ckov}, MIP Position: {mip_position}, \\nMomentum: {momentum}, Refractive Index: {refractive_index}\")\n",
        "\n",
        "      #axes[i].set_title(f\"Actual Mass: {actual_mass_category}, Predicted Mass: {predicted_mass_category}, Mass: {mass_actual}\\nCKOV: {ckov}, MIP Position: {mip_position}, Momentum: {momentum}, Refractive Index: {refractive_index}\")\n",
        "      axes[i].axis('off')\n",
        "\n",
        "      print(\"\\n\")\n",
        "      print(f\"  Actual Mass: {actual_mass_category}, Predicted Mass: {predicted_mass_category},\\n Mass: {mass_actual}, Mass_prob = {y_pred[index]} \\n CKOV: {ckov}, MIP Position: {mip_position}, \\n  Momentum: {momentum}, Refractive Index: {refractive_index}\")\n",
        "  # Adjust the spacing between subplots\n",
        "  plt.tight_layout()\n",
        "\n",
        "  # Show the plot\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Usage"
      ],
      "metadata": {
        "id": "PptqTnJ_2iyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Usage example\n",
        "\n",
        "\n",
        "#these three have .008 in stddev\n",
        "# 3 percent occupancy:\n",
        "print(\"classifier = MassClassifier(percentage_to_read = 10, resolution = 4) # pass percentage of dataset to read\")\n",
        "classifier = MassClassifier(percentage_to_read = 10, resolution = 4) # pass percentage of dataset to read\n",
        "\n",
        "\n",
        "file_to_read = \"test/ParticleInfo.h5\"\n",
        "print(\"classifier.load_data(file_to_read)\")\n",
        "classifier.load_data(file_to_read )\n",
        "\n",
        "X_train, X_test, y_train, y_test, maps, positions = classifier.preprocess_data()\n",
        "#print(\"X_train, X_test, y_train, y_test, maps, positions = classifier.preprocess_data()\")\n",
        "\n",
        "classifier.train(X_train = X_train, X_test = X_test, y_train = y_train, y_test = y_test, mask = mask)\n",
        "\n",
        "#classifier.build_model(file_to_read)\n",
        "\n",
        "#classifier.train(file_to_read)\n",
        "\n",
        "\n",
        "# 3 percent occupancy:\n",
        "#classifier = MassClassifier(percentage_to_read = 25, resolution = 4) # pass percentage of dataset to read\n",
        "#classifier.train(\"ParticleInfoProton.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "id": "dP4hLxCiN9SP",
        "outputId": "50145ac1-9e1f-4643-a0b0-4eba86577e5a"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classifier = MassClassifier(percentage_to_read = 10, resolution = 4) # pass percentage of dataset to read\n",
            "classifier.load_data(file_to_read)\n",
            "load_data : reading file test/ParticleInfo.h5\n",
            "load_data : location /content/drive/MyDrive/Colab Notebooks/CERN_ML/CNN_PID/\n",
            "Number of particles: 100\n",
            "Before padding : \n",
            " X_cand_pos_shape = (100,)\n",
            " len_candStatus_shape = (100,)\n",
            "After padding : \n",
            " X_cand_pos_shape = (100, 91, 2)\n",
            " len_candStatus_shape = (100,)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-102-225f07694050>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#print(\"X_train, X_test, y_train, y_test, maps, positions = classifier.preprocess_data()\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#classifier.build_model(file_to_read)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-101-3a824b34aa35>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X_train, X_test, y_train, y_test, mask)\u001b[0m\n\u001b[1;32m    979\u001b[0m       \u001b[0;31m#X_ckov_segm = X_train[\"X_train_photon_ckov_segmented\"]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m       \u001b[0;31m#print(f\"X_train_photon_ckov_segmented shape = {np.asarray(X_ckov_segm, dtype = object).shape}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m       \u001b[0mX_train_photon_ckov_segmented\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"X_train_photon_ckov_segmented\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'X_train_photon_ckov_segmented'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plotting histograms of Training Versus Test Sets"
      ],
      "metadata": {
        "id": "VRrcGqJzL0yO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# classifier.plot_hist(X_train = X_train, X_test = X_test, y_train = y_train, y_test = y_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "yodihp5E2k9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the model\n"
      ],
      "metadata": {
        "id": "82LnNwwvL7Ys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask = [1, 1, 1, 0, 0, 0]\n",
        "\n",
        "classifier.train(X_train = X_train, X_test = X_test, y_train = y_train, y_test = y_test, mask = mask)\n"
      ],
      "metadata": {
        "id": "X4MLTLJX2mA1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "fc3e3451-a951-4ce4-b948-0d23b42fdd4c"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-84-05607e332f6d>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'MassClassifier' object has no attribute 'train'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoauWOieMnWE"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/eflatlan/CNN_PID/models_sacved/helper_functions.py\n",
        "from helper_functions import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvEulDjrHxkx"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/eflatlan/CNN_PID/models_sacved/plot_helper_functions.py\n",
        "from plot_helper_functions import *"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyNmg7/Zst4QHf6uve9pgvGP",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}