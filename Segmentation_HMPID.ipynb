{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eflatlan/CNN_PID/blob/models_sacved/Segmentation_HMPID.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "mrPLFO_92Cvr",
        "outputId": "06f4151d-663d-4dd7-9037-0e89c92ca226"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (3.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0439ba785958>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'load_ext'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tensorboard'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    130\u001b[0m   )\n\u001b[1;32m    131\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "!pip install h5py numpy\n",
        "\n",
        "import os\n",
        "import h5py\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%load_ext tensorboard\n",
        "import datetime\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APUgd22_ffd2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_clF1C9YOo3R"
      },
      "outputs": [],
      "source": [
        "#!wget https://raw.githubusercontent.com/eflatlan/CNN_PID/dev_floatmap/helper_functions.py\n",
        "#from helper_functions.py import print_points, plot_mapsm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCiVl64YHGlc"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "!wget -O ParticleDataUtilsCp.py https://raw.githubusercontent.com/eflatlan/CNN_PID/models_sacved/ParticleDataUtils36.py\n",
        "from ParticleDataUtilsCp import ParticleDataUtils, classify_candidates_with_pad_sequences\n",
        "\n",
        "!wget -O misc_helper_functions.py https://raw.githubusercontent.com/eflatlan/CNN_PID/models_sacved/misc_helper_functions2.py\n",
        "from misc_helper_functions import build_species_layers, calculate_theta, filter_data, create_lr_scheduler,plot_lr, extract_neighborhood_map, create_cnn_model # plot_worst\n",
        "\n",
        "# !wget -O helper_functions.py https://raw.githubusercontent.com/eflatlan/CNN_PID/models_sacved/helper_functions.py\n",
        "# !wget -O plot_helper_functions.py https://raw.githubusercontent.com/eflatlan/CNN_PID/models_sacved/plot_helper_functions17.py\n",
        "\n",
        "# import plot_helper_functions\n",
        "\n",
        "\n",
        "# from plot_helper_functions import plot_hist\n",
        "\n",
        "# from plot_helper_functions import plot_training_history\n",
        "#from plot_helper_functions import plot_training_history#, plot_dist2mip_histograms, plot_maps\n",
        "\n",
        "\n",
        "print(classify_candidates_with_pad_sequences)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import numpy as np\n",
        "MASS_PION = 0.1396\n",
        "MASS_KAON = 0.4937\n",
        "MASS_PROTON = 0.938\n",
        "\n",
        "# Squared masses\n",
        "MASS_PION_SQ = MASS_PION * MASS_PION\n",
        "MASS_KAON_SQ = MASS_KAON * MASS_KAON\n",
        "MASS_PROTON_SQ = MASS_PROTON * MASS_PROTON\n",
        "REF_INDEX_FREON = 1.29  # Given refraction index\n",
        "REF_INDEX_FREON_SQ = REF_INDEX_FREON * REF_INDEX_FREON\n",
        "\n",
        "def threshold_momentum(pdg_code, p):\n",
        "\t\"\"\"\n",
        "\tCalculate the threshold momentum based on the given PDG code.\n",
        "\n",
        "\t:param pdg_code: PDG code of the particle.\n",
        "\tp : momentum of track\n",
        "\t(tbd : refindex)\n",
        "\n",
        "\t:return : boolean value for Cherenkov radiation.\n",
        "\t\"\"\"\n",
        "\n",
        "\t# Determine mass based on PDG code\n",
        "\tif abs(pdg_code) == 211:\n",
        "\t\tmass = MASS_PION\n",
        "\telif abs(pdg_code) == 321:\n",
        "\t\tmass = MASS_KAON\n",
        "\telif abs(pdg_code) == 2212:\n",
        "\t\tmass = MASS_PROTON\n",
        "\telse:\n",
        "\t\traise ValueError(f\"Unsupported PDG code: {pdg_code}\")\n",
        "\n",
        "\tp_lim = mass/(np.sqrt(REF_INDEX_FREON_SQ-1))\n",
        "\t#print(f\" p_lim {p_lim} p {p}\")\n",
        "\treturn p_lim < p\n",
        "\n",
        "\n",
        "\n",
        "def pad_and_stack(sequences, max_length=None):\n",
        "\t# Your existing code\n",
        "\ttry:\n",
        "\t\t# Try padding, if max_length is not None, pad or truncate to that length\n",
        "\t\tpadded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', dtype='float32')  # Changed dtype to 'floar32'\n",
        "\texcept ValueError:\n",
        "\t\t# Fallback: manually pad with zeros\n",
        "\t\tmax_len = max_length if max_length is not None else max(len(seq) for seq in sequences)\n",
        "\t\tpadded_sequences = np.array([np.pad(seq, (0, max_len - len(seq)), 'constant', constant_values=0) for seq in sequences])\n",
        "\n",
        "\treturn padded_sequences\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "\n",
        "def classify_candidates_with_pad_sequences(x_values_data, y_values_data, q_values_data, mCluSize_lista, candStatus_values_data, max_length_nested, xmip_list, ymip_list):\n",
        "\n",
        "\t# Ensure x_values_data has the right dimension\n",
        "\tif np.ndim(x_values_data) == 1:\n",
        "\t\tx_values_data = np.expand_dims(x_values_data, axis=-1)\n",
        "\n",
        "\t# Pad the sequences\n",
        "\tx_padded = pad_and_stack(x_values_data, max_length=max_length_nested)\n",
        "\ty_padded = pad_and_stack(y_values_data, max_length=max_length_nested)\n",
        "\tq_padded = pad_and_stack(q_values_data, max_length=max_length_nested)\n",
        "\tsize_padded = pad_and_stack(mCluSize_lista, max_length=max_length_nested)\n",
        "\tcandStatus_padded = pad_and_stack(candStatus_values_data, max_length=max_length_nested).astype(int)\n",
        "\n",
        "\t# Modify candStatus based on charge and size threshold\n",
        "\tcandStatus_padded[(q_padded >= 150) & (size_padded >= 3)] = 1\n",
        "\t# candStatus_padded[outside_fiducial_zones] = 0\n",
        "\n",
        "\t# Define masks for different particle statuses\n",
        "\tproton_values = [2, 4, 6, 8]\n",
        "\tkaon_values = [3, 4, 7, 8] # 3,5,7,8 ??\n",
        "\tpion_values = [5, 6, 7, 8]\n",
        "\n",
        "\tproton_mask = np.isin(candStatus_padded, proton_values)\n",
        "\tkaon_mask = np.isin(candStatus_padded, kaon_values)\n",
        "\tpion_mask = np.isin(candStatus_padded, pion_values)\n",
        "\n",
        "\t# Stack the data into a single array\n",
        "\tpadded_data = np.stack([x_padded, y_padded, q_padded, size_padded], axis=-1)\n",
        "\n",
        "\t# Create masks for positive and non statuses\n",
        "\tpositive_mask = (candStatus_padded > 0).astype(bool)\n",
        "\tnon_mask = (candStatus_padded <= 1).astype(bool)\n",
        "\n",
        "\t# Populate particle candidates arrays\n",
        "\tpion_candidates = np.zeros_like(padded_data)\n",
        "\tkaon_candidates = np.zeros_like(padded_data)\n",
        "\tproton_candidates = np.zeros_like(padded_data)\n",
        "\tnon_candidates = np.zeros_like(padded_data)\n",
        "\tpion_candidates[positive_mask & pion_mask] = padded_data[positive_mask & pion_mask]\n",
        "\tkaon_candidates[positive_mask & kaon_mask] = padded_data[positive_mask & kaon_mask]\n",
        "\tproton_candidates[positive_mask & proton_mask] = padded_data[positive_mask & proton_mask]\n",
        "\tnon_candidates[non_mask] = padded_data[non_mask]\n",
        "\n",
        "\treturn pion_candidates, kaon_candidates, proton_candidates, non_candidates, candStatus_padded\n",
        "\n",
        "import random\n",
        "\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import cycle\n",
        "\n",
        "import sys\n",
        "\n",
        "print(sys.getrecursionlimit()) # Prints 1000\n",
        "\n",
        "print_vals = False\n",
        "from numpy.linalg import norm\n",
        "from tensorflow.keras.backend import expand_dims\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import precision_recall_curve, confusion_matrix\n",
        "\n",
        "from scipy.signal import find_peaks\n",
        "\n",
        "import os\n",
        "import h5py\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Activation, Input, Conv2D, Lambda, Flatten, Dense, concatenate, BatchNormalization, MaxPooling2D, Dropout, LeakyReLU, Masking, Embedding\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# create a callback\n",
        "early_stopping = EarlyStopping(\n",
        "\tmonitor='val_loss', # you can monitor 'val_loss' or 'val_accuracy'\n",
        "\tpatience=100, # stop training if the monitored quantity does not improve for 50 epochs\n",
        "\trestore_best_weights=True, # restore model weights from the epoch with the best value\n",
        ")\n",
        "\n",
        "class Constants:\n",
        "\tPION_MASS = 0.1396\n",
        "\tKAON_MASS = 0.4937\n",
        "\tPROTON_MASS = 0.938\n",
        "\n",
        "np.set_printoptions(precision=4)\n",
        "\n",
        "@staticmethod\n",
        "def calculate_mass(momentum, refractiveIndex, ckov):\n",
        "\t\"\"\" args : momentum, refractiveIndex, ckov\n",
        "\t\treturns : mass\n",
        "\t\"\"\"\n",
        "\tmass = momentum * np.sqrt((refractiveIndex * np.cos(ckov))**2 - 1)\n",
        "\treturn mass\n",
        "\n",
        "\n",
        "class ParticleDataUtils:\n",
        "\n",
        "\tdef __init__(self, filenames =  [], percentage_to_read = 100):\n",
        "\t\tself.filename = filenames\n",
        "\t\tself.percentage_to_read = percentage_to_read\n",
        "\t\tself.particle_vector = []\n",
        "\t\tself.load_data(filenames=filenames)\n",
        "\n",
        "\t\t#\n",
        "\t\t# jeg flyttet\n",
        "\t\tself.particle_info = self.process_data(self.particle_vector, self.percentage_to_read)\n",
        "\t\tself.num_particles = len(self.particle_info)\n",
        "\n",
        "\n",
        "\t\t# new scalers to be created :\n",
        "\t\tself.phi_scaler, self.phi_stats = self.create_scalar_scaler(\"phiP\")\n",
        "\t\tself.theta_scaler, self.theta_stats = self.create_scalar_scaler(\"thetaP\")\n",
        "\t\tself.refractive_index_scaler, self.refractive_index_stats = self.create_scalar_scaler(\"refractiveIndex\")\n",
        "\t\tself.momentum_scaler, self.momentum_stats = self.create_scalar_scaler(\"momentum\")\n",
        "\t\tself.mCluSize_scaler, self.mCluSize_stats = self.create_scalar_scaler(\"mCluSize\")\n",
        "\t\tself.mCluCharge_scaler, self.mCluCharge_stats = self.create_scalar_scaler(\"mCluCharge\")\n",
        "\t\tprint(\"Created scaler for scalars\")\n",
        "\n",
        "\t\t# 2D\n",
        "\t\tself.mip_scaler, self.mip_stats = self.create_2D_scaler(\"mip_position\")\n",
        "\t\tself.rad_scaler, self.rad_stats = self.create_2D_scaler(\"rad_position\")\n",
        "\n",
        "\t\t# # vector of 2D\n",
        "\t\tself.proton_scalers, self.proton_stats = self.create_vector_scaler(\"proton_candidates\")\n",
        "\t\tself.kaon_scalers, self.kaon_stats = self.create_vector_scaler(\"kaon_candidates\")\n",
        "\t\tself.pion_scalers, self.pion_stats = self.create_vector_scaler(\"pion_candidates\")\n",
        "\n",
        "\n",
        "\n",
        "\t\t#self.ckov_scaler, self.ckov_stats = self.create_scaler(\"ckov\")\n",
        "\t\t#self.distances_scaler, self.distances_stats = self.create_scaler(\"distances\")def classify_candidates(candidates_data):\n",
        "\n",
        "\n",
        "\tclass Candidate2:\n",
        "\t\tdef __init__(self, x_values, y_values, chi2_values, q_values, xe_values, ye_values, candStatus_values):\n",
        "\t\t\tself.x_values = x_values\n",
        "\t\t\tself.y_values = y_values\n",
        "\t\t\tself.chi2_values = chi2_values\n",
        "\t\t\tself.q_values = q_values\n",
        "\t\t\tself.xe_values = xe_values\n",
        "\t\t\tself.ye_values = ye_values\n",
        "\t\t\tself.candStatus_values = candStatus_values\n",
        "\n",
        "\n",
        "\tclass ParticleInfo: # p\n",
        "\t\tdef __init__(self,  momentum, refractiveIndex, xRad, yRad, xMIP, yMIP, thetaP, phiP, mCluCharge, mCluSize, non_candidates, pion_candidates, kaon_candidates, proton_candidates, mTrackPdg, index, pion_flag, kaon_flag, proton_flag):\n",
        "\t\t\tself.momentum = momentum # this dhould be with\n",
        "\t\t\tself.refractiveIndex = refractiveIndex # with\n",
        "\t\t\tself.xRad = xRad # with\n",
        "\t\t\tself.yRad = yRad # with\n",
        "\n",
        "\t\t\tself.xMIP = xMIP # with\n",
        "\t\t\tself.yMIP = yMIP # with\n",
        "\n",
        "\t\t\tself.mCluCharge = mCluCharge # with\n",
        "\t\t\tself.mCluSize = mCluSize # with\n",
        "\n",
        "\t\t\tself.thetaP = thetaP# with\n",
        "\t\t\tself.phiP = phiP# with\n",
        "\t\t\tself.non_candidates = non_candidates # with the field candStatus is a int that is 0..7, please make it categorical\n",
        "\t\t\tself.rad_position = [xRad, yRad]\n",
        "\t\t\tself.mip_position = [xMIP, yMIP]\n",
        "\n",
        "\n",
        "\t\t\tself.pion_candidates = pion_candidates # pion_candidates = [1 if (int(candStatus) & 4) == 4 else 0 for candStatus in candsCombined]\n",
        "\t\t\tself.kaon_candidates = kaon_candidates # = kaon_candidates [1 if (int(candStatus) & 2) == 2 else 0 for candStatus in candsCombined]\n",
        "\t\t\tself.proton_candidates = proton_candidates # = proton_candidates[1 if (int(candStatus) & 1) == 1 else 0 for candStatus in candsCombined]\n",
        "\n",
        "\n",
        "\t\t\tself.pion_flag = pion_flag # pion_candidates = [1 if (int(candStatus) & 4) == 4 else 0 for candStatus in candsCombined]\n",
        "\t\t\tself.kaon_flag = kaon_flag # = kaon_candidates [1 if (int(candStatus) & 2) == 2 else 0 for candStatus in candsCombined]\n",
        "\t\t\tself.proton_flag = proton_flag # = proton_candidates[1 if (int(candStatus) & 1) == 1 else 0 for candStatus in candsCombined]\n",
        "\n",
        "\t\t\tself.mTrackPdg = mTrackPdg # with\n",
        "\t\t\tself.index_particle = index # with\n",
        "\n",
        "\t\t\tabs_mTrackPdg = abs(self.mTrackPdg)  # Take the absolute value\n",
        "\n",
        "\t\t\t# Set particleType based on absolute PDG code\n",
        "\t\t\tif abs_mTrackPdg == 211:\n",
        "\t\t\t\tself.particleType = 'pion'\n",
        "\t\t\telif abs_mTrackPdg == 321:\n",
        "\t\t\t\tself.particleType = 'kaon'\n",
        "\t\t\telif abs_mTrackPdg == 2212:\n",
        "\t\t\t\tself.particleType = 'proton'\n",
        "\t\t\telse:\n",
        "\t\t\t\tself.particleType = 'other'\n",
        "\t\t\t\t#print(f\"pdg type was other : {abs_mTrackPdg}\")\n",
        "\n",
        "\n",
        "\t\t@staticmethod\n",
        "\t\tdef infer_mass_category_from_ckov(momentum, refractiveIndex, ckov):\n",
        "\t\t\tmass = momentum * np.sqrt((refractiveIndex * np.cos(ckov))**2 - 1)\n",
        "\n",
        "\t\t\tmass_category = \"unk_nown\"\n",
        "\t\t\tif abs(mass - Constants.PION_MASS) < 1e-4:\n",
        "\t\t\t\tmass_category = \"pion\"\n",
        "\t\t\telif abs(mass - Constants.KAON_MASS) < 1e-4:\n",
        "\t\t\t\tmass_category = \"kaon\"\n",
        "\t\t\telif abs(mass - Constants.PROTON_MASS) < 1e-4:\n",
        "\t\t\t\tmass_category = \"proton\"\n",
        "\t\t\tif print_vals:\n",
        "\t\t\t  print(f\"\\ninfer_mass_category_from_ckov :  momentum = {momentum}|  mass_calc = {mass} |  mass_category={mass_category} | refractiveIndex = {refractiveIndex} | ckov = {ckov}\")\n",
        "\t\t\treturn mass_category\n",
        "\n",
        "\t\t@staticmethod\n",
        "\t\tdef infer_mass_category(mass):\n",
        "\t\t\tif abs(mass - Constants.PION_MASS) < 1e-6:\n",
        "\t\t\t\treturn \"pion\"\n",
        "\t\t\telif abs(mass - Constants.KAON_MASS) < 1e-6:\n",
        "\t\t\t\treturn \"kaon\"\n",
        "\t\t\telif abs(mass - Constants.PROTON_MASS) < 1e-6:\n",
        "\t\t\t\treturn \"proton\"\n",
        "\t\t\telse:\n",
        "\t\t\t\treturn \"unk_nown\"\n",
        "\n",
        "\t\tdef __str__(self):\n",
        "\t\t\tif print_vals:\n",
        "\t\t\t  return (f\"ParticleInfo(momentum={self.momentum} | mass={self.mass} |  mass_category={self.mass_category} | \"\n",
        "\t\t\t\t\t  f\"refractiveIndex={self.refractiveIndex} | ckov={self.ckov} | rad_position={len(self.rad_position)}, \"\n",
        "\t\t\t\t\t  f\"mip_position={self.mip_position})\")\n",
        "\n",
        "\t#  ''' def calculate_distances_to_mip(self):\n",
        "\t#       \"\"\"Calculate Euclidean distances from all filled bins to MIP position\"\"\"\n",
        "\t#       filledBins_np = np.array(self.filledBins)\n",
        "\t#       mip_position_np = np.array(self.mip_position)\n",
        "\n",
        "\t#       distances = np.linalg.norm(filledBins_np - mip_position_np, axis=1)\n",
        "\t#       return distances'''\n",
        "\n",
        "\n",
        "\tdef load_data(self, filenames):\n",
        "\t\tdrive_path = '/content/drive/MyDrive/Colab Notebooks/CERN_ML/CNN_PID/'\n",
        "\n",
        "\t\tmax_length_nested = 0\n",
        "\t\tnum_particles = 0\n",
        "\t\tfile_num = 0\n",
        "\t\tfor filename in filenames:\n",
        "\t\t\tfile_path = os.path.join(drive_path, filename)\n",
        "\t\t\tprint(f\"Reading file {file_num} : {filename}\")\n",
        "\t\t\twith h5py.File(file_path, 'r') as file:\n",
        "\t\t\t\tfor i, group_name in enumerate(file):\n",
        "\t\t\t\t\tgroup = file[group_name]\n",
        "\t\t\t\t\tnum_particles = num_particles + 1\n",
        "\t\t\t\t\tcurrent_length = len(group['ye_values'][...])\n",
        "\t\t\t\t\tif current_length > max_length_nested:\n",
        "\t\t\t\t\t\tmax_length_nested = current_length\n",
        "\t\t\t\t\t\tprint(f\" i {i} max_length_nested {max_length_nested}\")\n",
        "\n",
        "\n",
        "\t\t# Lists to store scalar and array-like attributes\n",
        "\t\tmomentum_list = np.zeros((num_particles, 1))\n",
        "\t\trefractiveIndex_list = np.zeros((num_particles, 1))\n",
        "\t\txRad_list = np.zeros((num_particles, 1))\n",
        "\t\tyRad_list = np.zeros((num_particles, 1))\n",
        "\t\txMIP_list = np.zeros((num_particles, 1))\n",
        "\t\tyMIP_list = np.zeros((num_particles, 1))\n",
        "\t\tthetaP_list = np.zeros((num_particles, 1))\n",
        "\t\tphiP_list = np.zeros((num_particles, 1))\n",
        "\t\tmCluCharge_list = np.zeros((num_particles, 1))\n",
        "\t\tmCluSize_list = np.zeros((num_particles, 1))\n",
        "\t\tmTrackPdg_list = np.zeros((num_particles, 1))\n",
        "\n",
        "\n",
        "\t\tx_values_data_list = np.zeros((num_particles, max_length_nested))\n",
        "\t\ty_values_data_list =  np.zeros((num_particles, max_length_nested))\n",
        "\t\tchi2_values_data_list =  np.zeros((num_particles, max_length_nested))\n",
        "\t\tq_values_data_list =  np.zeros((num_particles, max_length_nested))\n",
        "\t\txe_values_data_list =  np.zeros((num_particles, max_length_nested))\n",
        "\t\tye_values_data_list =  np.zeros((num_particles, max_length_nested))\n",
        "\t\tcandStatus_values_data_list =  np.zeros((num_particles, max_length_nested))\n",
        "\t\tsize_clu_lst =  np.zeros((num_particles, max_length_nested))\n",
        "\n",
        "\n",
        "\t\tindex_particle = 0\n",
        "\t\tfor filename in filenames:\n",
        "\t\t\tfile_path = os.path.join(drive_path, filename)\n",
        "\t\t\tprint(f\"Reading file {file_num} : {filename}\")\n",
        "\t\t\twith h5py.File(file_path, 'r') as file:\n",
        "\t\t\t\tfor i, group_name in enumerate(file):\n",
        "\t\t\t\t\tgroup = file[group_name]\n",
        "\n",
        "\t\t\t\t\t# Store scalar attributes into lists\n",
        "\t\t\t\t\tmomentum_list[index_particle] = group.attrs['Momentum']\n",
        "\t\t\t\t\trefractiveIndex_list[index_particle] = group.attrs['RefractiveIndex']\n",
        "\t\t\t\t\txRad_list[index_particle] = group.attrs['xRad']\n",
        "\t\t\t\t\tyRad_list[index_particle] = group.attrs['yRad']\n",
        "\t\t\t\t\txMIP_list[index_particle] = group.attrs['xMip']\n",
        "\t\t\t\t\tyMIP_list[index_particle] = group.attrs['yMip']\n",
        "\t\t\t\t\tthetaP_list[index_particle] = group.attrs['ThetaP']\n",
        "\t\t\t\t\tphiP_list[index_particle] = group.attrs['PhiP']\n",
        "\t\t\t\t\tmCluCharge_list[index_particle] = group.attrs['CluCharge']\n",
        "\t\t\t\t\tmCluSize_list[index_particle] = group.attrs['CluSize']\n",
        "\t\t\t\t\tmTrackPdg_list[index_particle] = group.attrs['TrackPdg']\n",
        "\n",
        "\t\t\t\t\tactual_length = len(group['x_values'][...])\n",
        "\t\t\t\t\tx_values_data_list[index_particle, :actual_length] = group['x_values'][...]\n",
        "\n",
        "\t\t\t\t\tactual_length = len(group['y_values'][...])\n",
        "\t\t\t\t\ty_values_data_list[index_particle, :actual_length] = group['y_values'][...]\n",
        "\n",
        "\t\t\t\t\tactual_length = len(group['chi2_values'][...])\n",
        "\t\t\t\t\tchi2_values_data_list[index_particle, :actual_length] = group['chi2_values'][...]\n",
        "\n",
        "\t\t\t\t\tactual_length = len(group['q_values'][...])\n",
        "\t\t\t\t\tq_values_data_list[index_particle, :actual_length] = group['q_values'][...]\n",
        "\n",
        "\t\t\t\t\tactual_length = len(group['xe_values'][...])\n",
        "\t\t\t\t\txe_values_data_list[index_particle, :actual_length] = group['xe_values'][...]\n",
        "\n",
        "\t\t\t\t\tactual_length = len(group['ye_values'][...])\n",
        "\t\t\t\t\tye_values_data_list[index_particle, :actual_length] = group['ye_values'][...]\n",
        "\n",
        "\t\t\t\t\tactual_length = len(group['candStatus_values'][...])\n",
        "\t\t\t\t\tcandStatus_values_data_list[index_particle, :actual_length] = group['candStatus_values'][...]\n",
        "\n",
        "\n",
        "\t\t\t\t\tactual_length = len(group['mSize_values'][...])\n",
        "\t\t\t\t\tsize_clu_lst[index_particle, :actual_length] = group['mSize_values'][...]\n",
        "\n",
        "\t\t\t\t\tindex_particle += 1\n",
        "\n",
        "\t\t\t\t\t# cehck the length of current entry inn   ye_values_data_list and update max_length_nested if logner\n",
        "\t\t\t\t\tcurrent_length = len(group['ye_values'][...])\n",
        "\t\t\t\t\t#print(f\" i {i} appending\")\n",
        "\n",
        "\t\t\t\t\tif current_length > max_length_nested:\n",
        "\t\t\t\t\t\tmax_length_nested = current_length\n",
        "\t\t\t\t\t\tprint(f\" i {i} max_length_nested {max_length_nested}\")\n",
        "\n",
        "\n",
        "\n",
        "\t\tpion_candidates, kaon_candidates, proton_candidates, non_candidates, cand_combined = classify_candidates_with_pad_sequences(\n",
        "\t\t\tx_values_data_list, y_values_data_list, size_clu_lst,\n",
        "\t\t\tq_values_data_list, candStatus_values_data_list,\n",
        "\t\t\tmax_length_nested,xMIP_list, yMIP_list\n",
        "\t  \t)\n",
        "\n",
        "\t\tprint(f\"pion_candidates shape {pion_candidates.shape}\")\n",
        "\t\tprint(f\"Dtype pion_candidates : {pion_candidates.dtype}\")  # Output will be something like: int64\n",
        "\t\tprint(f\"Dtype momentum_list : {momentum_list.dtype}\")  # Output will be something like: int64\n",
        "\n",
        "\t\tprint(f\"momentum_list shape {momentum_list.shape}\")\n",
        "\n",
        "\t\tparticle_vector = [None] * len(momentum_list)\n",
        "\n",
        "\t\tMIP_list = np.hstack([xMIP_list, yMIP_list])\n",
        "\n",
        "\t\t# Reshape the array to (N, 1, 2)\n",
        "\t\tMIP_list_reshaped = MIP_list[:, np.newaxis, :]\n",
        "\n",
        "\t\t# Extract only the x and y coordinates from pion_candidates\n",
        "\t\tpion_candidates_xy = pion_candidates[:, :, :2]\n",
        "\n",
        "\t\t# Compute squared differences\n",
        "\t\tdiff = np.sum((pion_candidates_xy - MIP_list_reshaped)**2, axis=2)\n",
        "\t\tr_max = 35\n",
        "\t\tr_min = 0.5\n",
        "\t\t# Count the number of points within radius r (5 to 40)\n",
        "\t\tnon_padded_mask = np.any(pion_candidates[:, :, :2] != 0, axis=2)\n",
        "\t\twithin_r_mask = (diff >= r_min**2) & (diff <= r_max**2) & non_padded_mask\n",
        "\n",
        "\t\tcount_within_r = np.sum(within_r_mask, axis=1)\n",
        "\n",
        "\t\tprint(f\"momentum_list shape {np.array(momentum_list).shape}\")\n",
        "\t\tprint(f\"pion_candidates[i] shape {pion_candidates.shape}\")\n",
        "\t\tprint(f\"Dtype : {pion_candidates.dtype}\")  # Output will be something like: int64\n",
        "\t\tprint(f\"Dtype : {pion_candidates.dtype}\")  # Output will be something like: int64\n",
        "\n",
        "\n",
        "\n",
        "\t\ttGap = 8.0\n",
        "\t\trW = 1.5\n",
        "\t\tqW = 0.5\n",
        "\t\tL = rW / 2\n",
        "\n",
        "\n",
        "\n",
        "\t\t# Calculate L, dy, dx, X, Y, and dist\n",
        "\t\tL = (rW - L + tGap + qW) * np.tan(thetaP_list)\n",
        "\t\tdy = L * np.sin(phiP_list)\n",
        "\t\tdx = L * np.cos(phiP_list)\n",
        "\t\tX = xRad_list + dx\n",
        "\t\tY = yRad_list + dy\n",
        "\t\tdist = np.sqrt((xMIP_list - X)**2 + (yMIP_list - Y)**2)\n",
        "\n",
        "\n",
        "\t\tfor i in range(len(momentum_list)):\n",
        "\n",
        "\n",
        "\t\t\t#print(f\"p { momentum_list[i]} pdg {mTrackPdg_list[i]}\")\n",
        "\n",
        "\t\t\tabs_pdg = abs(mTrackPdg_list[i])\n",
        "\n",
        "\n",
        "\t\t\tif abs_pdg in [211, 321, 2212]:\n",
        "\t\t\t\t#print(f\"p { momentum_list[i]} pdg {abs_pdg}\")\n",
        "\n",
        "\t\t\t\t# check if exceeds momentum limit for ckov photons\n",
        "\t\t\t\tif threshold_momentum(abs_pdg, momentum_list[i]):\n",
        "\t\t\t\t\tnum_pions = np.count_nonzero(pion_candidates[i])\n",
        "\t\t\t\t\tnum_kaons = np.count_nonzero(kaon_candidates[i])\n",
        "\t\t\t\t\tnum_protons = np.count_nonzero(proton_candidates[i])\n",
        "\n",
        "\t\t\t\t\tif abs_pdg == 211: non_zero_val =  num_pions\n",
        "\t\t\t\t\tif abs_pdg == 321: non_zero_val = num_kaons\n",
        "\t\t\t\t\tif abs_pdg == 2212: non_zero_val =  num_protons\n",
        "\n",
        "\n",
        "\n",
        "\t\t\t\t\t# TODO : do flags on momentum\n",
        "\t\t\t\t\t# penalty on number of photons ?\n",
        "\n",
        "\t\t\t\t\tpion_flag = (num_pions < 6*4) | (~threshold_momentum(211, momentum_list[i]))\n",
        "\t\t\t\t\tkaon_flag = (num_kaons < 5*4) | (~threshold_momentum(321, momentum_list[i]))\n",
        "\t\t\t\t\tproton_flag = (num_protons < 4*4) | (~threshold_momentum(2212, momentum_list[i]))\n",
        "\n",
        "\n",
        "\t\t\t\t\t#print(f\"non_zero_val {non_zero_val}\")\n",
        "\t\t\t\t\t# Check if any candidate has more than 5 non-zero values\n",
        "\n",
        "\t\t\t\t\tif non_zero_val > 20 and dist[i] < 0.2:\n",
        "\t\t\t\t\t\t\tparticle_info = ParticleDataUtils.ParticleInfo(\n",
        "\t\t\t\t\t\t\t\t\tmomentum_list[i], refractiveIndex_list[i], xRad_list[i], yRad_list[i],\n",
        "\t\t\t\t\t\t\t\t\txMIP_list[i], yMIP_list[i], thetaP_list[i], phiP_list[i],\n",
        "\t\t\t\t\t\t\t\t\tmCluCharge_list[i], mCluSize_list[i], non_candidates[i], pion_candidates[i],\n",
        "\t\t\t\t\t\t\t\t\tkaon_candidates[i], proton_candidates[i], mTrackPdg_list[i],\n",
        "\t\t\t\t\t\t\t\t\ti, pion_flag, kaon_flag, proton_flag\n",
        "\t\t\t\t\t\t\t)\n",
        "\t\t\t\t\t\t\tparticle_vector[i] = particle_info\n",
        "\t\t\t\t\t\t\tself.particle_vector.append(particle_info)\n",
        "\n",
        "\t\t\t\t\t#else :\n",
        "\t\t\t\t\t#\t\tprint(f\" dist to long {dist[i]}\")\n",
        "\n",
        "\n",
        "\tdef process_data(self, particle_vector, percentage):\n",
        "\n",
        "\t\t# Calculate the number of particles based on the percentage\n",
        "\t\tnum_particles = int(len(self.particle_vector) * (percentage / 100.0))\n",
        "\t\t# Slice the particle_vector to the desired percentage\n",
        "\t\tparticle_vector = self.particle_vector[:num_particles]\n",
        "\t\treturn particle_vector\n",
        "\n",
        "\tdef create_scalar_scaler(self, feature):\n",
        "\t\ttry:\n",
        "\t\t\t\tif feature in [\"momentum\", \"refractiveIndex\", \"phiP\", \"thetaP\", \"mCluSize\", \"mCluCharge\"]:  # added mCluSize and mCluCharge\n",
        "\t\t\t\t\tvalues = np.array([getattr(info, feature) for info in self.particle_info]).reshape(-1, 1)\n",
        "\t\t\t\t\tif values.size == 0:\n",
        "\t\t\t\t\t\traise ValueError(f\"Empty values array for feature: {feature}\")\n",
        "\t\t\t\t\tscaler = StandardScaler()\n",
        "\t\t\t\t\tscaled_values = scaler.fit_transform(values)\n",
        "\t\t\t\t\tvalues = scaled_values\n",
        "\n",
        "\t\t\t\t\tstats = {\n",
        "\t\t\t\t\t\t\"mean\": scaler.mean_[0],\n",
        "\t\t\t\t\t\t\"std\": scaler.scale_[0]\n",
        "\t\t\t\t\t}\n",
        "\t\t\t\t\treturn scaler, stats\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\traise ValueError(f\"Invalid feature: {feature}\")\n",
        "\n",
        "\t\texcept Exception as e:\n",
        "\t\t\t\tprint(f\"An error occurred in create_scalar_scaler: {e}\")\n",
        "\t\t\t\traise\n",
        "\n",
        "\n",
        "\n",
        "\tdef create_2D_scaler(self, feature):\n",
        "\t\ttry:\n",
        "\t\t\t\tif feature in [\"mip_position\", \"rad_position\"]:\n",
        "\t\t\t\t\t\tvalues = np.array([getattr(info, feature) for info in self.particle_info])\n",
        "\t\t\t\tif values.size == 0:\n",
        "\t\t\t\t\t\traise ValueError(f\"Empty values array for feature: {feature}\")\n",
        "\n",
        "\t\t\t\tscaler_x = StandardScaler()\n",
        "\t\t\t\tscaler_y = StandardScaler()\n",
        "\n",
        "\n",
        "\n",
        "\t\t\t\tprint(f\"create_2D_scalervalues.shape : {values.shape}\")\n",
        "\n",
        "\t\t\t\tn_samples, dim, r = values.shape\n",
        "\t\t\t\tprint(f\"create_2D_scalervalues.shape : {values[:, 0].shape}\")\n",
        "\n",
        "\t\t\t\tfeature_values_x = values[:, 0].reshape(n_samples, 1)\n",
        "\t\t\t\tscaled_values_x = scaler_x.fit_transform(feature_values_x).reshape(n_samples, 1)\n",
        "\t\t\t\tvalues[:, 0] = scaled_values_x\n",
        "\n",
        "\t\t\t\tfeature_values_y = values[:, 1].reshape(n_samples, 1)\n",
        "\t\t\t\tscaled_values_y = scaler_y.fit_transform(feature_values_y).reshape(n_samples, 1)\n",
        "\t\t\t\tvalues[:, 1] = scaled_values_y\n",
        "\n",
        "\n",
        "\t\t\t\t# Store both the scalers\n",
        "\t\t\t\tscalers = {'x': scaler_x, 'y': scaler_y}\n",
        "\t\t\t\tstats = {\n",
        "\t\t\t\t'x': {\n",
        "\t\t\t\t\"mean\": scaler_x.mean_[0],\n",
        "\t\t\t\t\"std\": scaler_x.scale_[0]\n",
        "\t\t\t\t},\n",
        "\t\t\t\t'y': {\n",
        "\t\t\t\t\t\"mean\": scaler_y.mean_[0],\n",
        "\t\t\t\t\t\"std\": scaler_y.scale_[0]\n",
        "\t\t\t\t\t}\n",
        "\t\t\t\t}\n",
        "\t\t\t\treturn scalers, stats\n",
        "\t\texcept Exception as e:\n",
        "\t\t\tprint(f\"An error occurred in create_2D_scaler: {e}\")\n",
        "\t\t\traise\n",
        "\n",
        "\tdef create_vector_scaler(self, feature):\n",
        "\t\ttry:\n",
        "\t\t\tif feature in [\"pion_candidates\", \"kaon_candidates\", \"proton_candidates\"]:\n",
        "\t\t\t\tvalues = np.array([getattr(info, feature) for info in self.particle_info])\n",
        "\t\t\t\tif values.size == 0:\n",
        "\t\t\t\t\t\traise ValueError(f\"Empty values array for feature: {feature}\")\n",
        "\n",
        "\t\t\t\tn_samples, n_clusters, n_features = values.shape\n",
        "\t\t\t\tscalers = []\n",
        "\n",
        "\t\t\t\tfor i in range(n_features):\n",
        "\t\t\t\t\tscaler = StandardScaler()\n",
        "\t\t\t\t\tfeature_values = values[:, :, i].reshape(-1, 1)\n",
        "\t\t\t\t\tscaled_values = scaler.fit_transform(feature_values).reshape(n_samples, n_clusters)\n",
        "\t\t\t\t\tvalues[:, :, i] = scaled_values\n",
        "\t\t\t\t\tscalers.append(scaler)\n",
        "\t\t\t\tstats = []\n",
        "\t\t\t\tfor scaler in scalers:\n",
        "\t\t\t\t\tstat = {\n",
        "\t\t\t\t\t\t\"mean\": scaler.mean_[0],\n",
        "\t\t\t\t\t\t\"std\": scaler.scale_[0]\n",
        "\t\t\t\t}\n",
        "\t\t\t\t\tstats.append(stat)\n",
        "\n",
        "\t\t\t\treturn scalers, stats\n",
        "\n",
        "\t\texcept Exception as e:\n",
        "\t\t\tprint(f\"An error occurred in create_vector_scaler: {e}\")\n",
        "\t\t\traise\n",
        "\n",
        "\n",
        "\n",
        "# TODO : denne skal mulgiens fjernes helt ?\n",
        "# create a map, the resolution is the \"inverse\"\n",
        "def create_map(filledBins=None, resolution=4):\n",
        "\t# Add an offset to your map shape calculation to handle edge cases\n",
        "\toffset = 0\n",
        "\tmap_shape = (int(144 * resolution + offset), int(160 * resolution + offset))\n",
        "\tmap_data = np.zeros(map_shape, dtype=np.int32)\n",
        "\tif filledBins is not None:\n",
        "\t\tfilledBins_np = np.array(filledBins)\n",
        "\t\tindices = (filledBins_np * resolution).astype(int)\n",
        "\t\t#print(f\"create_map : indices shape : {np.array(indices).shape}\")\n",
        "\n",
        "\t\tmap_data[indices[:, 1], indices[:, 0]] = 1\n",
        "\n",
        "\t\tind = np.wherme(map_data == 1)\n",
        "\t\t#print(f\"create_map : ind shape : {np.array(ind).shape}\")\n",
        ", \treturn map_data"
      ],
      "metadata": {
        "id": "do3H88rMcK92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_w-le-CYD5_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import LogNorm\n",
        "from matplotlib.patches import Wedge\n",
        "\n",
        "# Count non-zero charges\n",
        "def count_non_zero_charges(datasets, idx, titles):\n",
        "    non_zero_counts = {}\n",
        "    for data, title in zip(datasets, titles):\n",
        "        non_zero_charges = np.count_nonzero(data[idx, :, 3])\n",
        "        non_zero_counts[title] = non_zero_charges\n",
        "    return non_zero_counts\n",
        "\n",
        "# Plot individual types\n",
        "def plot_individual_types(idx, x_pion, x_kaon, x_proton, x_non, MIP_charge, MIP_position, RAD_position, phi, y_train, log_scale, X_index):\n",
        "    index_of_fig = X_index[idx]\n",
        "    datasets = [x_pion, x_kaon, x_proton, x_non]\n",
        "    titles = [\"Pion\", \"Kaon\", \"Proton\", \"Non\"]\n",
        "    non_zero_counts = count_non_zero_charges(datasets, idx, titles)\n",
        "\n",
        "    mip_charge = MIP_charge[idx]\n",
        "    max_charge = np.max([np.max(data[idx, :, 3]) for data in datasets])\n",
        "    fig, ax = plt.subplots(2, 2, figsize=(14, 14))\n",
        "    ax = ax.flatten()\n",
        "    xm, ym = MIP_position[idx]\n",
        "    xr, yr = RAD_position[idx]\n",
        "    label_type = y_train[idx]\n",
        "    fig.suptitle(f\"Index: {index_of_fig} - Type : {label_type}\", fontsize=16)\n",
        "    cmap = plt.get_cmap('viridis')\n",
        "\n",
        "    if log_scale:\n",
        "        norm = LogNorm(vmin=1, vmax=max_charge)\n",
        "\n",
        "    # Calculate line end points from phi angle\n",
        "    x_end_forward = xm + 50 * np.cos(phi[idx])  # 50 units in the direction of phi\n",
        "    y_end_forward = ym + 50 * np.sin(phi[idx])\n",
        "    x_end_backward = xm - 10 * np.cos(phi[idx])  # 10 units opposite to phi direction\n",
        "    y_end_backward = ym - 10 * np.sin(phi[idx])\n",
        "\n",
        "    for i, (data, title) in enumerate(zip(datasets, titles)):\n",
        "        x = data[idx, :, 0]\n",
        "        y = data[idx, :, 1]\n",
        "        charge = data[idx, :, 3]\n",
        "\n",
        "        if log_scale:\n",
        "            sc = ax[i].scatter(x, y, c=charge, cmap=cmap, norm=norm)\n",
        "            plt.colorbar(sc, ax=ax[i], label='Charge (Log scale)', fraction=0.046, pad=0.04)\n",
        "        else:\n",
        "            sc = ax[i].scatter(x, y, c=charge, cmap=cmap, vmin=0, vmax=max_charge)\n",
        "            plt.colorbar(sc, ax=ax[i], label='Charge', fraction=0.046, pad=0.04)\n",
        "\n",
        "        ax[i].plot([x_end_backward, x_end_forward], [y_end_backward, y_end_forward], 'k-', lw=2)\n",
        "        ax[i].scatter(xm, ym, color='green', marker='x', label='MIP_position')\n",
        "        ax[i].scatter(xr, yr, color='black', marker='o', label='RAD_position')\n",
        "        ax[i].set_title(f\"{title}: {non_zero_counts[title]} non-zero candidates\")\n",
        "        ax[i].set_xlim([0, 130])\n",
        "        ax[i].set_ylim([0, 130])\n",
        "        ax[i].set_aspect('equal', adjustable='box')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    plt.show()\n",
        "\n",
        "# Draw pie for combined types\n",
        "def draw_pie(ax, x, y, sizes, colors):\n",
        "    total_size = sum(sizes)\n",
        "    start_angle = 90\n",
        "    for size, color in zip(sizes, colors):\n",
        "        wedge = Wedge((x, y), 0.5, start_angle, start_angle + 360 * (size / total_size), color=color)\n",
        "        ax.add_patch(wedge)\n",
        "        start_angle += 360 * (size / total_size)\n",
        "def plot_combined_types(i, x_pion, x_kaon, x_proton, x_non, MIP_position, RAD_position, phiP, thetaP, y_train, X_index):\n",
        "    index_of_fig = X_index[i]\n",
        "    label_type = y_train[i]\n",
        "    phiP_val = phiP[i]\n",
        "    thetaP_val = thetaP[i]\n",
        "\n",
        "    fig, ax = plt.subplots(1, 3, figsize=(22, 7))\n",
        "    fig.suptitle(f\"Index: {index_of_fig} - Type : {label_type}\", fontsize=16)\n",
        "\n",
        "    xm, ym = MIP_position[i]\n",
        "    xr, yr = RAD_position[i]\n",
        "\n",
        "    x_end_forward = xm + 50 * np.cos(phiP_val)\n",
        "    y_end_forward = ym + 50 * np.sin(phiP_val)\n",
        "    x_end_backward = xm - 10 * np.cos(phiP_val)\n",
        "    y_end_backward = ym - 10 * np.sin(phiP_val)\n",
        "\n",
        "    for x, label, color in zip([x_proton, x_pion, x_kaon, x_non], ['Proton', 'Pion', 'Kaon', 'Non'], ['r', 'g', 'b', 'y']):\n",
        "        x_val = x[i, :, 0]\n",
        "        y_val = x[i, :, 1]\n",
        "        ax[0].scatter(x_val, y_val, c=color, alpha=0.5, label=label)\n",
        "\n",
        "    ax[0].plot([x_end_backward, x_end_forward], [y_end_backward, y_end_forward], 'k-', lw=2)\n",
        "    ax[0].scatter(xm, ym, color='green', marker='x', label='MIP_position')\n",
        "    ax[0].scatter(xr, yr, color='black', marker='o', label='RAD_position')  # added RAD_position as red marker\n",
        "    ax[0].set_title(f\"All Values \\nphiP: {float(phiP_val):.2f}, thetaP: {float(thetaP_val):.2f}\")\n",
        "    ax[0].set_xlim([0, 130])\n",
        "    ax[0].set_ylim([0, 130])\n",
        "    ax[0].set_aspect('equal', adjustable='box')\n",
        "\n",
        "    charge_cut = 150\n",
        "    for x, label, color in zip([x_proton, x_pion, x_kaon, x_non], ['Proton', 'Pion', 'Kaon', 'Non'], ['r', 'g', 'b', 'y']):\n",
        "        x_val = x[i, :, 0]\n",
        "        y_val = x[i, :, 1]\n",
        "        charge = x[i, :, 3]\n",
        "        size = x[i, :, 2]\n",
        "\n",
        "        mask = (charge > charge_cut) & (size > 2)\n",
        "\n",
        "        ax[1].scatter(x_val[mask], y_val[mask], c=color, alpha=0.5, label=label)\n",
        "\n",
        "    ax[1].scatter(xm, ym, color='green', marker='x', label='MIP_position')\n",
        "    ax[1].scatter(xr, yr, color='black', marker='o', label='RAD_position')  # added RAD_position as red marker\n",
        "    ax[1].set_title(f\"Values with Charge > {charge_cut} and Size > 2 \\nphiP: {float(phiP_val):.2f}, thetaP: {float(thetaP_val):.2f}\")\n",
        "    ax[1].set_xlim([0, 130])\n",
        "    ax[1].set_ylim([0, 130])\n",
        "    ax[1].set_aspect('equal', adjustable='box')\n",
        "\n",
        "    # 3rd subplot - 50x50 window with MIP at the center\n",
        "    for x, label, color in zip([x_proton, x_pion, x_kaon, x_non], ['Proton', 'Pion', 'Kaon', 'Non'], ['r', 'g', 'b', 'y']):\n",
        "        x_val = x[i, :, 0]\n",
        "        y_val = x[i, :, 1]\n",
        "        ax[2].scatter(x_val, y_val, c=color, alpha=0.5, label=label)\n",
        "\n",
        "    ax[2].plot([x_end_backward, x_end_forward], [y_end_backward, y_end_forward], 'k-', lw=2)\n",
        "    ax[2].scatter(xm, ym, color='green', marker='x', label='MIP_position')\n",
        "    ax[2].scatter(xr, yr, color='black', marker='o', label='RAD_position')  # added RAD_position as red marker\n",
        "    ax[2].set_title(f\"50x50 Window with MIP at center\")\n",
        "    ax[2].set_xlim([xm-25, xm+25])\n",
        "    ax[2].set_ylim([ym-25, ym+25])\n",
        "    ax[2].set_aspect('equal', adjustable='box')\n",
        "\n",
        "    plt.legend(loc=\"upper left\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bINqoPuTnc_V"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class CherenkovArrayMin:\n",
        "    def __init__(self, max_ckovs, k_n, nmean, n_constraint, X_train_rad_position, X_phi, X_theta, ):\n",
        "        self.max_ckovs = max_ckovs\n",
        "        self.k_n = k_n\n",
        "        self.nF = nmean - n_constraint  # This replaces getRefIdx, for min we subtract\n",
        "\n",
        "        self.winThick = 0.5\n",
        "        self.radThick = 1.5\n",
        "        self.gapThick = 8\n",
        "        self.gapIdx = 1.0005\n",
        "        self.winIdx = 1.5787\n",
        "\n",
        "        # Incorporate the provided pointers\n",
        "        self.X_train_rad_position = X_train_rad_position  # Track position\n",
        "        self.X_phi = X_phi\n",
        "        self.X_theta = X_theta  # Track direction\n",
        "\n",
        "        self.polygon = self._setArrayMin()\n",
        "\n",
        "    def _setArrayMin(self):\n",
        "        lMax=1.5# This might need adjustment based on your requirement\n",
        "\n",
        "        phiL_array = np.linspace(0, 2*np.pi, self.k_n, endpoint=False)\n",
        "        dirTrs = np.column_stack([np.sin(self.max_ckovs) * np.cos(phiL_array),\n",
        "                                  np.sin(self.max_ckovs) * np.sin(phiL_array),\n",
        "                                  np.cos(self.max_ckovs)])\n",
        "\n",
        "        thetaR, phiR = self.trs2Lors(dirTrs)\n",
        "        dirLORS = np.column_stack([np.sin(thetaR) * np.cos(phiR),\n",
        "                                   np.sin(thetaR) * np.sin(phiR),\n",
        "                                   np.cos(thetaR)])\n",
        "\n",
        "        min_positions = self.traceForward(dirLORS, lMax)\n",
        "\n",
        "        mask_valid = ~np.any(min_positions == -999, axis=1)\n",
        "        polygon = min_positions[mask_valid]\n",
        "\n",
        "        return polygon\n",
        "\n",
        "    def trs2Lors(self, dirCkov):\n",
        "        mtheta = np.array([[np.cos(self.X_theta), 0, np.sin(self.X_theta)],\n",
        "                           [0, 1, 0],\n",
        "                           [-np.sin(self.X_theta), 0, np.cos(self.X_theta)]])\n",
        "        mphi = np.array([[np.cos(self.X_phi), -np.sin(self.X_phi), 0],\n",
        "                         [np.sin(self.X_phi), np.cos(self.X_phi), 0],\n",
        "                         [0, 0, 1]])\n",
        "        mrot = np.dot(mphi, mtheta)\n",
        "        dirCkovLORS = np.dot(mrot, dirCkov.T).T\n",
        "        phiCer = np.arctan2(dirCkovLORS[:, 1], dirCkovLORS[:, 0])\n",
        "        thetaCer = np.arccos(dirCkovLORS[:, 2])\n",
        "        return thetaCer, phiCer\n",
        "\n",
        "    def traceForward(self, dirCkov, L):\n",
        "        mask_invalid = dirCkov[:, 1] > np.arcsin(1.0 / self.nF)\n",
        "        valid_positions = np.ones((len(dirCkov), 2)) * -999\n",
        "        if not np.any(mask_invalid):\n",
        "            zRad = -(self.radThick - L) - 0.5 * self.winThick\n",
        "            posCkov = np.array([self.X_train_rad_position[0], self.X_train_rad_position[1], zRad])\n",
        "            dirCkov, posCkov = self.propagate(dirCkov, posCkov, -0.5 * self.winThick)\n",
        "            dirCkov = self.refract(dirCkov, self.nF, self.winIdx)\n",
        "            dirCkov, posCkov = self.propagate(dirCkov, posCkov, 0.5 * self.winThick)\n",
        "            dirCkov = self.refract(dirCkov, self.winIdx, self.gapIdx)\n",
        "            dirCkov, posCkov = self.propagate(dirCkov, posCkov, 0.5 * self.winThick + self.gapThick)\n",
        "\n",
        "            valid_positions[~mask_invalid] = posCkov[:, :2]\n",
        "\n",
        "        return valid_positions\n",
        "\n",
        "    def propagate(self, dir, pos, z):\n",
        "        nrm = np.array([0, 0, 1])\n",
        "        pnt = np.array([0, 0, z])\n",
        "\n",
        "        diff = pnt - pos\n",
        "        sint = np.dot(nrm, diff.T) / np.dot(nrm, dir.T)\n",
        "        new_pos = pos + sint[:, np.newaxis] * dir\n",
        "\n",
        "        return dir, new_pos\n",
        "\n",
        "    def refract(self, dir, n1, n2):\n",
        "        sinref = (n1 / n2) * dir\n",
        "        cost2 = 1 - np.sum(sinref**2, axis=1)\n",
        "        cost2[cost2 < 0] = 0\n",
        "        costref = np.sqrt(cost2)\n",
        "        return sinref - dir * costref[:, np.newaxis]\n"
      ],
      "metadata": {
        "id": "UW1WR7ULMldT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class CherenkovArrayMax:\n",
        "    def __init__(self, max_ckovs, k_n, nmean, n_constraint, X_train_rad_position, X_phi, X_theta):\n",
        "        self.max_ckovs = max_ckovs\n",
        "        self.k_n = k_n\n",
        "        self.nF = nmean + n_constraint  # This replaces getRefIdx\n",
        "\n",
        "        self.winThick = 0.5\n",
        "        self.radThick = 1.5\n",
        "        self.gapThick = 8\n",
        "        self.gapIdx = 1.0005\n",
        "        self.winIdx = 1.5787\n",
        "\n",
        "        # Incorporate the provided pointers\n",
        "        self.X_train_rad_position = X_train_rad_position  # Track position\n",
        "        self.X_phi = X_phi\n",
        "        self.X_theta = X_theta  # Track direction\n",
        "\n",
        "        self.polygon = self._setArrayMax()\n",
        "\n",
        "    def _setArrayMax(self):\n",
        "        lMin = 0.0\n",
        "\n",
        "        phiL_array = np.linspace(0, 2*np.pi, self.k_n, endpoint=False)\n",
        "        dirTrs = np.column_stack([np.sin(self.max_ckovs) * np.cos(phiL_array),\n",
        "                                  np.sin(self.max_ckovs) * np.sin(phiL_array),\n",
        "                                  np.cos(self.max_ckovs)])\n",
        "\n",
        "        thetaR, phiR = self.trs2Lors(dirTrs)\n",
        "        dirLORS = np.column_stack([np.sin(thetaR) * np.cos(phiR),\n",
        "                                   np.sin(thetaR) * np.sin(phiR),\n",
        "                                   np.cos(thetaR)])\n",
        "\n",
        "        max_positions = self.traceForward(dirLORS, lMin)\n",
        "\n",
        "        mask_valid = ~np.any(max_positions == -999, axis=1)\n",
        "        polygon = max_positions[mask_valid]\n",
        "\n",
        "        return polygon\n",
        "\n",
        "    def trs2Lors(self, dirCkov):\n",
        "        mtheta = np.array([[np.cos(self.X_theta), 0, np.sin(self.X_theta)],\n",
        "                           [0, 1, 0],\n",
        "                           [-np.sin(self.X_theta), 0, np.cos(self.X_theta)]])\n",
        "        mphi = np.array([[np.cos(self.X_phi), -np.sin(self.X_phi), 0],\n",
        "                         [np.sin(self.X_phi), np.cos(self.X_phi), 0],\n",
        "                         [0, 0, 1]])\n",
        "        mrot = np.dot(mphi, mtheta)\n",
        "        dirCkovLORS = np.dot(mrot, dirCkov.T).T\n",
        "        phiCer = np.arctan2(dirCkovLORS[:, 1], dirCkovLORS[:, 0])\n",
        "        thetaCer = np.arccos(dirCkovLORS[:, 2])\n",
        "        return thetaCer, phiCer\n",
        "\n",
        "    def traceForward(self, dirCkov, L):\n",
        "        mask_invalid = dirCkov[:, 1] > np.arcsin(1.0 / self.nF)\n",
        "        valid_positions = np.ones((len(dirCkov), 2)) * -999\n",
        "        if not np.any(mask_invalid):\n",
        "            zRad = -(self.radThick - L) - 0.5 * self.winThick\n",
        "            posCkov = np.array([self.X_train_rad_position[0], self.X_train_rad_position[1], zRad])\n",
        "            dirCkov, posCkov = self.propagate(dirCkov, posCkov, -0.5 * self.winThick)\n",
        "            dirCkov = self.refract(dirCkov, self.nF, self.winIdx)\n",
        "            dirCkov, posCkov = self.propagate(dirCkov, posCkov, 0.5 * self.winThick)\n",
        "            dirCkov = self.refract(dirCkov, self.winIdx, self.gapIdx)\n",
        "            dirCkov, posCkov = self.propagate(dirCkov, posCkov, 0.5 * self.winThick + self.gapThick)\n",
        "\n",
        "            valid_positions[~mask_invalid] = posCkov[:, :2]\n",
        "\n",
        "        return valid_positions\n",
        "\n",
        "    def propagate(self, dir, pos, z):\n",
        "        nrm = np.array([0, 0, 1])\n",
        "        pnt = np.array([0, 0, z])\n",
        "\n",
        "        diff = pnt - pos\n",
        "        sint = np.dot(nrm, diff.T) / np.dot(nrm, dir.T)\n",
        "        new_pos = pos + sint[:, np.newaxis] * dir\n",
        "\n",
        "        return dir, new_pos\n",
        "\n",
        "    def refract(self, dir, n1, n2):\n",
        "        sinref = (n1 / n2) * np.sin(dir[:, 1])\n",
        "        mask_invalid = np.abs(sinref) > 1.0\n",
        "        dir[mask_invalid] = [-999, -999, -999]\n"
      ],
      "metadata": {
        "id": "JjgBjhUTMkR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ui9UZvaI8ri6"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np  # Make sure to import numpy\n",
        "import matplotlib.colors as mcolors\n",
        "\n",
        "cmap = mcolors.LinearSegmentedColormap.from_list(\"\", [\"green\", \"red\"])\n",
        "\n",
        "def plot_first_instance(i, X_pion, X_kaon, X_proton, X_non, MIP_charge, MIP_position, RAD_position, phiP, thetaP, y):\n",
        "    mip_charge = MIP_charge[i]\n",
        "    Pions = X_pion[i]\n",
        "    Kaons = X_kaon[i]\n",
        "    X_proton = X_proton[i]\n",
        "    Non = X_non[i]\n",
        "\n",
        "\n",
        "    pdg = y[i] # one hot encoded -- 1 0\n",
        "    if y[i,0] == 1:\n",
        "      type = \"pion\"\n",
        "      XYp = X_pion[i]\n",
        "\n",
        "    elif y[i,1] == 1:\n",
        "      type = \"kaon\"\n",
        "      XYp = X_kaon[i]\n",
        "\n",
        "    else :\n",
        "      type = \"proton\"\n",
        "      XYp = X_proton[i]\n",
        "\n",
        "    # plot istedet hver type segmentert?\n",
        "\n",
        "    phiP_val = phiP[i]\n",
        "    thetaP_val = thetaP[i]\n",
        "\n",
        "\n",
        "\n",
        "    #XYp = X_pion[i]\n",
        "    Xp = XYp[:, 0]  # X-coordinate\n",
        "    Yp = XYp[:, 1]  # Y-coordinate\n",
        "    Charge = XYp[:, 3]  # Charge\n",
        "    Size = XYp[:, 2]  # Size\n",
        "\n",
        "    fig, ax = plt.subplots(1, 3, figsize=(21, 7))\n",
        "    fig.suptitle(f\"{type} - MIP Charge: {mip_charge:.2f}\", fontsize=16)\n",
        "    plt.tight_layout()  # Automatic layout adjustment\n",
        "\n",
        "    # Plot using charge without log scale\n",
        "    sc0 = ax[0].scatter(Xp, Yp, c=Charge, cmap=cmap, label='X_pion')\n",
        "    cbar0 = plt.colorbar(sc0, ax=ax[0], label='Charge', fraction=0.046, pad=0.04)\n",
        "    ax[0].set_title(f\"Charge as Color\\nphiP: {float(phiP_val):.2f}, thetaP: {float(thetaP_val):.2f}\")\n",
        "\n",
        "    # Plot using charge on a log scale for color\n",
        "    sc1 = ax[1].scatter(Xp, Yp, c=np.log1p(Charge), cmap=cmap, label='X_pion')\n",
        "    cbar1 = plt.colorbar(sc1, ax=ax[1], label='Log(Charge)', fraction=0.046, pad=0.04)\n",
        "    ax[1].set_title(f\"Log-Scale Charge as Color\\nphiP: {float(phiP_val):.2f}, thetaP: {float(thetaP_val):.2f}\")\n",
        "\n",
        "    # Plot using Size for color with a linear color map\n",
        "    sc2 = ax[2].scatter(Xp, Yp, c=Size, cmap=cmap, label='X_pion')\n",
        "    cbar2 = plt.colorbar(sc2, ax=ax[2], label='Size', fraction=0.046, pad=0.04)\n",
        "    ax[2].set_title(f\"Size as Color\\nphiP: {float(phiP_val):.2f}, thetaP: {float(thetaP_val):.2f}\")\n",
        "\n",
        "    xm, ym = MIP_position[i]\n",
        "    xr, yr = RAD_position[i]\n",
        "\n",
        "    # Plot MIP_position and RAD_position on all plots\n",
        "    i = 1\n",
        "    for a in ax:\n",
        "        if i == 2:\n",
        "          a.scatter(xm, ym, color='green', marker='x', label='MIP_position')\n",
        "          a.scatter(xr, yr, color='orange', marker='x', label='RAD_position')\n",
        "          a.legend()\n",
        "        i += 1\n",
        "        # Set x and y limits for the plot\n",
        "        a.set_xlim([0, 130])\n",
        "        a.set_ylim([0, 130])\n",
        "\n",
        "        # Set aspect ratio to be equal\n",
        "        a.set_aspect('equal', adjustable='box')\n",
        "\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "class CherenkovHypothesisCalculator:\n",
        "    def __init__(self, p, n, n_constraint):\n",
        "        self.p = p\n",
        "        self.n = n\n",
        "        self.n_constraint = n_constraint\n",
        "        self.k_n = 200  # number of points to form polygon\n",
        "        self.mass_Pion = 0.1396\n",
        "        self.mass_Kaon = 0.4937\n",
        "        self.mass_Proton = 0.938\n",
        "        self.mass_Pion_sq = self.mass_Pion * self.mass_Pion\n",
        "        self.mass_Kaon_sq = self.mass_Kaon * self.mass_Kaon\n",
        "        self.mass_Proton_sq = self.mass_Proton * self.mass_Proton\n",
        "\n",
        "    def calcCherenkovHyp(self):\n",
        "        len_pion = len(self.p)\n",
        "\n",
        "        X_pion_min_ckov = np.zeros((len_pion, self.k_n))\n",
        "        X_pion_max_ckov = np.zeros((len_pion, self.k_n))\n",
        "        X_kaon_min_ckov = np.zeros((len_pion, self.k_n))\n",
        "        X_kaon_max_ckov = np.zeros((len_pion, self.k_n))\n",
        "        X_proton_min_ckov = np.zeros((len_pion, self.k_n))\n",
        "        X_proton_max_ckov = np.zeros((len_pion, self.k_n))\n",
        "\n",
        "        n_min = self.n - self.n_constraint\n",
        "        n_max = self.n + self.n_constraint\n",
        "\n",
        "        p_sq = self.p * self.p\n",
        "\n",
        "        # Calculations for min_ckov using n_min\n",
        "        cos_ckov_denom_min = self.p * n_min\n",
        "        cos_ckov_Pion_min = np.sqrt(p_sq + self.mass_Pion_sq) / cos_ckov_denom_min\n",
        "        cos_ckov_Kaon_min = np.sqrt(p_sq + self.mass_Kaon_sq) / cos_ckov_denom_min\n",
        "        cos_ckov_Proton_min = np.sqrt(p_sq + self.mass_Proton_sq) / cos_ckov_denom_min\n",
        "\n",
        "        X_pion_min_ckov[:, 0] = np.arccos(cos_ckov_Pion_min)\n",
        "        X_kaon_min_ckov[:, 0] = np.arccos(cos_ckov_Kaon_min)\n",
        "        X_proton_min_ckov[:, 0] = np.arccos(cos_ckov_Proton_min)\n",
        "\n",
        "        # Calculations for max_ckov using n_max\n",
        "        cos_ckov_denom_max = self.p * n_max\n",
        "        cos_ckov_Pion_max = np.sqrt(p_sq + self.mass_Pion_sq) / cos_ckov_denom_max\n",
        "        cos_ckov_Kaon_max = np.sqrt(p_sq + self.mass_Kaon_sq) / cos_ckov_denom_max\n",
        "        cos_ckov_Proton_max = np.sqrt(p_sq + self.mass_Proton_sq) / cos_ckov_denom_max\n",
        "\n",
        "        X_pion_max_ckov[:, 0] = np.arccos(cos_ckov_Pion_max)\n",
        "        X_kaon_max_ckov[:, 0] = np.arccos(cos_ckov_Kaon_max)\n",
        "        X_proton_max_ckov[:, 0] = np.arccos(cos_ckov_Proton_max)\n",
        "\n",
        "        print(f\"Pion Min {np.mean(X_pion_min_ckov[:, 0]):.3f} Max {np.mean(X_pion_max_ckov[:, 0]):.3f}\")\n",
        "        print(f\"Kaon Min {np.mean(X_kaon_min_ckov[:, 0]):.3f} Max {np.mean(X_kaon_max_ckov[:, 0]):.3f}\")\n",
        "        print(f\"Proton Min {np.mean(X_proton_min_ckov[:, 0]):.3f} Max {np.mean(X_proton_max_ckov[:, 0]):.3f}\")\n",
        "\n",
        "        return X_pion_min_ckov, X_pion_max_ckov, X_kaon_min_ckov, X_kaon_max_ckov, X_proton_min_ckov, X_proton_max_ckov\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "c1GBZlHOMiv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Y9ZOOuC_Dhay"
      },
      "outputs": [],
      "source": [
        "k_n = 200 # number of points to form polygon\n",
        "\n",
        "nmean = (1.29197 + 1.29044)/2.\n",
        "n_constraint = 0.005\n",
        "# X_pion_candidates =  np.array(pion_candidates_list).astype(np.float32)#pad_sequences(pion_candidates_list, padding='post')\n",
        "# X_momentum = np.array([info.momentum for info in particle_info])\n",
        "# X_phi = np.array([particle.phiP for particle in particle_info])\n",
        "# X_theta = np.array([particle.thetaP for particle in particle_info])\n",
        "# X_mip_position = np.array([info.mip_position for info in particle_info])\n",
        "\n",
        "class ParticleSegmenter:\n",
        "\n",
        "    def __init__(self, X_pion_candidates, X_kaon_candidates, X_proton_candidates, X_rad_position, X_mip_position, X_momentum, X_phi, X_theta):\n",
        "        self.X_pion_candidates = X_pion_candidates\n",
        "        self.X_kaon_candidates = X_kaon_candidates\n",
        "        self.X_proton_candidates = X_proton_candidates\n",
        "        self.X_rad_position = X_rad_position\n",
        "        self.X_mip_position = X_mip_position # replace mipPos w this !\n",
        "        self.X_momentum = X_momentum\n",
        "        self.X_phi = X_phi\n",
        "        self.X_theta = X_theta\n",
        "        self.k_n = 200  # number of points to form polygon\n",
        "        self.nmean = (1.29197 + 1.29044) / 2.\n",
        "        self.n_constraint = 0.005\n",
        "\n",
        "    def do_segment(self):\n",
        "        len_pion = np.asarray(self.X_pion_candidates, dtype = object).shape[1]\n",
        "        X_pion_constr_min = np.zeros((len_pion, k_n))\n",
        "        X_pion_constr_max = np.zeros((len_pion, k_n))\n",
        "        X_kaon_constr_min = np.zeros((len_pion, k_n))\n",
        "        X_kaon_constr_max = np.zeros((len_pion, k_n))\n",
        "        X_proton_constr_min = np.zeros((len_pion, k_n))\n",
        "        X_proton_constr_max = np.zeros((len_pion, k_n))\n",
        "\n",
        "\n",
        "        # add the array X_mip_position  = fPc\n",
        "        # X_train_rad_position =  trkPos\n",
        "        # X_phi X_theta  = trkDir\n",
        "        # nmean = nF\n",
        "\n",
        "        cherenkov_calculator = CherenkovHypothesisCalculator(self.X_momentum, nmean, n_constraint)\n",
        "        X_pion_min_ckov, X_pion_max_ckov, X_kaon_min_ckov, X_kaon_max_ckov, X_proton_min_ckov, X_proton_max_ckov = cherenkov_calculator.calcCherenkovHyp()\n",
        "\n",
        "        # For Pions:\n",
        "        pion_max_calculator = CherenkovArrayMax(X_pion_max_ckov, k_n, nmean, n_constraint, self.X_rad_position,self.X_phi, self.X_theta)\n",
        "        X_pion_constr_max = pion_max_calculator.setArrayMax()\n",
        "        pion_min_calculator = CherenkovArrayMin(X_pion_min_ckov, k_n, nmean, n_constraint, self.X_rad_position,self.X_phi, self.X_theta)\n",
        "        X_pion_constr_min = pion_min_calculator.setArrayMin()\n",
        "        pion_list = self.segment(self.X_pion_candidates, X_pion_constr_max, X_pion_constr_min)\n",
        "\n",
        "        # For Kaons:\n",
        "        kaon_max_calculator = CherenkovArrayMax(X_kaon_max_ckov, k_n, nmean, n_constraint, self.X_rad_position,self.X_phi, self.X_theta)\n",
        "        X_kaon_constr_max = kaon_max_calculator.setArrayMax()\n",
        "        kaon_min_calculator = CherenkovArrayMin(X_kaon_min_ckov, k_n, nmean, n_constraint, self.X_rad_position,self.X_phi, self.X_theta)\n",
        "        X_kaon_constr_min = kaon_min_calculator.setArrayMin()\n",
        "        kaon_list = self.segment(self.X_kaon_candidates, X_kaon_constr_max, X_kaon_constr_min)\n",
        "\n",
        "\n",
        "        # For Protons:\n",
        "        proton_max_calculator = CherenkovArrayMax(X_proton_max_ckov, k_n, nmean, n_constraint, self.X_rad_position,self.X_phi, self.X_theta)\n",
        "        X_proton_constr_max = proton_max_calculator.setArrayMax()\n",
        "        proton_min_calculator = CherenkovArrayMin(X_proton_min_ckov, k_n, nmean, n_constraint, self.X_rad_position,self.X_phi, self.X_theta)\n",
        "        X_proton_constr_min = proton_min_calculator.setArrayMin()\n",
        "        proton_list = self.segment(X_proton_candidates, X_proton_constr_max, X_proton_constr_min)\n",
        "        return pion_list, X_pion_constr_min, X_pion_constr_max, kaon_list, X_kaon_constr_min, X_kaon_constr_max, proton_list, X_proton_constr_min, X_proton_constr_max # = do_segment(X_pion_candidates, X_kaon_candidates, X_proton_candidates, X_train_rad_position, X_momentum, X_phi, X_theta)\n",
        "\n",
        "\n",
        "\n",
        "    def segment(self, particle_candidates, constr_max, constr_min):\n",
        "        isMaxOk = np.logical_not(self.evaluatePointContour(particle_candidates, constr_max, True))\n",
        "        isMinOk = self.evaluatePointContour(particle_candidates, constr_min, False)\n",
        "        mask = np.logical_and(isMinOk, isMaxOk)\n",
        "        particle_candidates[mask] = 0\n",
        "        return particle_candidates.tolist()\n",
        "\n",
        "    def evaluatePointContour(self, posPhoton, contour, checkInside):\n",
        "\n",
        "        rUncertainty = 2 if checkInside else -2\n",
        "        intersects = np.zeros(posPhoton.shape[:-1], dtype=bool)\n",
        "\n",
        "        shortSidePoint = contour[:, 0, :]\n",
        "        d = (posPhoton[..., 0] - shortSidePoint[..., 0]) * (self.X_mip_position[..., 1] - shortSidePoint[..., 1]) \\\n",
        "            - (posPhoton[..., 1] - shortSidePoint[..., 1]) * (self.X_mip_position[..., 0] - shortSidePoint[..., 0])\n",
        "\n",
        "        # d > 0 case\n",
        "        mask_positive_d = d > 0\n",
        "        for k in range(contour.shape[1] - 1):\n",
        "            intersects[mask_positive_d] |= self.lineIntersects(self.X_mip_position[mask_positive_d], posPhoton[mask_positive_d],\n",
        "                                                          contour[mask_positive_d, k], contour[mask_positive_d, k+1], rUncertainty)\n",
        "\n",
        "        # d <= 0 case\n",
        "        mask_negative_d = d <= 0\n",
        "        for k in range(contour.shape[1] - 1, 0, -1):\n",
        "            intersects[mask_negative_d] |= self.lineIntersects(self.X_mip_position[mask_negative_d], posPhoton[mask_negative_d],\n",
        "                                                          contour[mask_negative_d, k], contour[mask_negative_d, k-1], rUncertainty)\n",
        "\n",
        "        return intersects\n",
        "\n",
        "    import numpy as np\n",
        "\n",
        "    def lineIntersects(self, MIP, B, C, D, rUncertainty):\n",
        "        # Calculate the coefficients of the lines formed by (MIP, B) and (C, D)\n",
        "        a1 = B[..., 1] - MIP[..., 1]\n",
        "        b1 = MIP[..., 0] - B[..., 0]\n",
        "        c1 = a1 * MIP[..., 0] + b1 * MIP[..., 1]\n",
        "\n",
        "        a2 = D[..., 1] - C[..., 1]\n",
        "        b2 = C[..., 0] - D[..., 0]\n",
        "        c2 = a2 * C[..., 0] + b2 * C[..., 1]\n",
        "\n",
        "        det = a1 * b2 - a2 * b1\n",
        "        parallel_lines_mask = det == 0\n",
        "\n",
        "        x = np.where(parallel_lines_mask, 0, (b2 * c1 - b1 * c2) / det)\n",
        "        y = np.where(parallel_lines_mask, 0, (a1 * c2 - a2 * c1) / det)\n",
        "\n",
        "        intersection = np.stack((x, y), axis=-1)\n",
        "\n",
        "        out_of_bounds = (\n",
        "            (x < np.minimum(MIP[..., 0], B[..., 0])) | (x > np.maximum(MIP[..., 0], B[..., 0])) |\n",
        "            (x < np.minimum(C[..., 0], D[..., 0])) | (x > np.maximum(C[..., 0], D[..., 0])) |\n",
        "            (y < np.minimum(MIP[..., 1], B[..., 1])) | (y > np.maximum(MIP[..., 1], B[..., 1])) |\n",
        "            (y < np.minimum(C[..., 1], D[..., 1])) | (y > np.maximum(C[..., 1], D[..., 1]))\n",
        "        )\n",
        "\n",
        "        direction = (intersection - MIP) / np.linalg.norm(intersection - MIP, axis=-1, keepdims=True)\n",
        "        adjusted_intersection = intersection + direction * rUncertainty\n",
        "        x_adj = adjusted_intersection[..., 0]\n",
        "        y_adj = adjusted_intersection[..., 1]\n",
        "\n",
        "        out_of_bounds_adj = (\n",
        "            (x_adj < np.minimum(MIP[..., 0], B[..., 0])) | (x_adj > np.maximum(MIP[..., 0], B[..., 0])) |\n",
        "            (x_adj < np.minimum(C[..., 0], D[..., 0])) | (x_adj > np.maximum(C[..., 0], D[..., 0])) |\n",
        "            (y_adj < np.minimum(MIP[..., 1], B[..., 1])) | (y_adj > np.maximum(MIP[..., 1], B[..., 1])) |\n",
        "            (y_adj < np.minimum(C[..., 1], D[..., 1])) | (y_adj > np.maximum(C[..., 1], D[..., 1]))\n",
        "        )\n",
        "\n",
        "        point_is_inside = np.linalg.norm(adjusted_intersection - MIP, axis=-1) < np.linalg.norm(B - MIP, axis=-1)\n",
        "\n",
        "        intersects = np.logical_not(parallel_lines_mask) & np.logical_not(out_of_bounds_adj)\n",
        "\n",
        "        return intersects, intersection, point_is_inside\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcY6LVGpxZQI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRoviFHFMvUa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXEUp3H8NoS_"
      },
      "source": [
        "# Helping class and imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "es3UneK1Nmof"
      },
      "outputs": [],
      "source": [
        "#resolution = 4\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import cycle\n",
        "\n",
        "import sys\n",
        "\n",
        "print(sys.getrecursionlimit()) # Prints 1000\n",
        "\n",
        "print_vals = False\n",
        "from numpy.linalg import norm\n",
        "from tensorflow.keras.backend import expand_dims\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import precision_recall_curve, confusion_matrix\n",
        "\n",
        "from scipy.signal import find_peaks\n",
        "\n",
        "from __future__ import print_function\n",
        "import os\n",
        "import h5py\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Activation, Input, Conv2D, Lambda, Flatten, Dense, concatenate, BatchNormalization, MaxPooling2D, Dropout, LeakyReLU, Masking, Embedding\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVLkss66iBW5"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "\n",
        "def custom_loss(y_true, y_pred):\n",
        "    base_loss = categorical_crossentropy(y_true, y_pred)\n",
        "    print(\"y_true shape:\", y_true.shape)\n",
        "    print(\"y_pred shape:\", y_pred.shape)\n",
        "    # Assuming the order of classes in y_pred is pion, kaon, and proton.\n",
        "\n",
        "    # Extract flags from y_true (assuming they are appended after the original one-hot encoded labels)\n",
        "    pion_flag = y_true[:, 3]\n",
        "    kaon_flag = y_true[:, 4]\n",
        "    proton_flag = y_true[:, 5]\n",
        "\n",
        "    # Calculate the penalty for each class.\n",
        "    # The penalty increases when the predicted probability for a class is high and the flag for that class is set.\n",
        "    pion_penalty = pion_flag * y_pred[:, 0]\n",
        "    kaon_penalty = kaon_flag * y_pred[:, 1]\n",
        "    proton_penalty = proton_flag * y_pred[:, 2]\n",
        "\n",
        "    total_penalty = pion_penalty + kaon_penalty + proton_penalty\n",
        "\n",
        "    # Add the penalties to the base loss.\n",
        "    return base_loss + total_penalty\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pN8kSaAzUDxj"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8U_5qogtNs15"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjrxNzz0LMqU"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MassClassifier:\n",
        "  def __init__(self, percentage_to_read = 10, resolution = 4):\n",
        "      self.model = None\n",
        "      self.utils = None\n",
        "      self.percentage_to_read = percentage_to_read\n",
        "      self.resolution = resolution\n",
        "      self.particle_vector = None\n",
        "\n",
        "\n",
        "  def load_data(self, filenames):\n",
        "\n",
        "      print(f\"MassClassifier __ load_data\")\n",
        "\n",
        "      self.utils = ParticleDataUtils(filenames, percentage_to_read = self.percentage_to_read) # specify percentage of particles to read..\n",
        "      # when init on ParticleDataUtils is called, it calls its member function function         self.particle_vector = self.load_data(filename)\n",
        "      if  self.particle_vector is None :\n",
        "        print(f\"self.particle_vector is None\")\n",
        "\n",
        "        self.particle_vector = self.utils.particle_vector\n",
        "\n",
        "        np.random.shuffle(self.utils.particle_vector)\n",
        "\n",
        "      else :\n",
        "        print(f\"self.particle_vector is not None\")\n",
        "        self.particle_vector.extend(self.utils.particle_vector)\n",
        "\n",
        "      print(f\"Number of particles: {len(self.particle_vector)}\")\n",
        "\n",
        "  def preprocess_data(self):\n",
        "      particle_info = self.utils.particle_info\n",
        "      particle_info = self.particle_vector\n",
        "      random.shuffle(particle_info)\n",
        "      np.random.shuffle(particle_info)\n",
        "\n",
        "      from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "      # Convert pion_candidates, kaon_candidates, proton_candidates to lists\n",
        "      pion_candidates_list =[info.pion_candidates for info in particle_info]\n",
        "      kaon_candidates_list = ([info.kaon_candidates for info in particle_info])\n",
        "      proton_candidates_list = ([info.proton_candidates for info in particle_info])\n",
        "      non_candidates_list = ([info.non_candidates for info in particle_info])\n",
        "\n",
        "\n",
        "\n",
        "      for info in particle_info:\n",
        "        np.count_nonzero(info.pion_candidates)\n",
        "        np.count_nonzero(info.kaon_candidates)\n",
        "        np.count_nonzero(info.proton_candidates)\n",
        "\n",
        "\n",
        "\n",
        "      #plot_function(X_train_pion_candidates, X_train_kaon_candidates, X_train_proton_candidates, X_train_mip_position, X_train_rad_position, X_train_phi, X_train_theta)\n",
        "      print(\"Fields in the first vector of X_train:\")\n",
        "      print(\"pion_candidates_list shape:\",  np.array(pion_candidates_list).shape)\n",
        "      print(\"kaon_candidates_list shape:\",  np.array(kaon_candidates_list).shape)\n",
        "      print(\"proton_candidates_list shape:\",  np.array(proton_candidates_list).shape)\n",
        "\n",
        "\n",
        "      # Pad the lists to the longest vector per dimension\n",
        "      X_pion_candidates =  np.array(pion_candidates_list).astype(np.float32)#pad_sequences(pion_candidates_list, padding='post')\n",
        "      X_kaon_candidates =  np.array(kaon_candidates_list).astype(np.float32)#pad_sequences(kaon_candidates_list, padding='post')\n",
        "      X_proton_candidates = np.array(proton_candidates_list).astype(np.float32)# pad_sequences(proton_candidates_list, padding='post')\n",
        "      X_non_candidates = np.array(non_candidates_list).astype(np.float32)# pad_sequences(proton_candidates_list, padding='post')\n",
        "\n",
        "\n",
        "\n",
        "      print(f\"PADDED \\n:\")\n",
        "      print(\"X_pion_candidates shape:\", X_pion_candidates.shape)\n",
        "      print(\"X_kaon_candidates shape:\", X_kaon_candidates.shape)\n",
        "      print(\"X_proton_candidates shape:\", X_proton_candidates.shape)\n",
        "\n",
        "\n",
        "      # scalars\n",
        "      X_momentum = np.array([info.momentum for info in particle_info])#.reshape(-1, 32, 32, 1)\n",
        "      X_refractive_index = np.array([info.refractiveIndex for info in particle_info])#.reshape(-1, 32, 32, 1)\n",
        "\n",
        "      # this should not be included? :\n",
        "      print(f\"PADDED \\n:\")\n",
        "\n",
        "      X_phi = np.array([particle.phiP for particle in particle_info])      #\n",
        "      X_theta = np.array([particle.thetaP for particle in particle_info])  #\n",
        "\n",
        "      X_phi = np.array([particle.phiP for particle in particle_info])      #\n",
        "      X_theta = np.array([particle.thetaP for particle in particle_info])  #\n",
        "\n",
        "\n",
        "\n",
        "      X_index_particle = np.array([particle.index_particle for particle in particle_info])  # new\n",
        "\n",
        "      X_pion_flag = np.array([particle.pion_flag for particle in particle_info])  # new\n",
        "      X_kaon_flag = np.array([particle.kaon_flag for particle in particle_info])  # new\n",
        "      X_proton_flag = np.array([particle.proton_flag for particle in particle_info])  # new\n",
        "\n",
        "\n",
        "      # x,y pairs (2,1) :\n",
        "      X_mip_position = np.array([info.mip_position for info in particle_info]) # was already\n",
        "      X_rad_position = np.array([info.rad_position for info in particle_info]) # new\n",
        "\n",
        "      X_mCluCharge = np.array([info.mCluCharge for info in particle_info]) # was already\n",
        "      X_mCluSize = np.array([info.mCluSize for info in particle_info]) # new\n",
        "\n",
        "      print(np.asarray(X_proton_candidates, dtype=object).shape)\n",
        "\n",
        "      len_proton = np.asarray(X_proton_candidates, dtype = object).shape[1]\n",
        "\n",
        "      # Prepare the outputs\n",
        "      y = np.array([info.particleType  for info in particle_info])\n",
        "      # if pdg == 211 -- pion 321 kaon 2212 proton else \"other\"\n",
        "\n",
        "      # Convert the outputs to one-hot encoded vectors\n",
        "      lb = LabelBinarizer()\n",
        "      y = lb.fit_transform(y)\n",
        "\n",
        "      # Split the data into train and test sets\n",
        "      map_size = nb_size = 50\n",
        "\n",
        "      # ef :TODO fix this back again\n",
        "      X_map_pion = extract_neighborhood_map(candidate_positions = X_pion_candidates, mip_positions = X_mip_position, neighborhood_size = nb_size, map_size = map_size)\n",
        "      X_map_kaon =extract_neighborhood_map(candidate_positions = X_kaon_candidates, mip_positions = X_mip_position, neighborhood_size = nb_size, map_size = map_size)\n",
        "      X_map_proton =extract_neighborhood_map(candidate_positions = X_proton_candidates, mip_positions = X_mip_position, neighborhood_size = nb_size, map_size = map_size)\n",
        "\n",
        "\n",
        "      print(\"X_map_pion shape:\", X_map_pion.shape)\n",
        "      print(\"X_map_kaon shape:\", X_map_kaon.shape)\n",
        "      print(\"X_map_proton shape:\", X_map_proton.shape)\n",
        "\n",
        "\n",
        "      X_train_mCluCharge, X_test_mCluCharge = train_test_split(X_mCluCharge, test_size=0.2, random_state=22, shuffle=True)\n",
        "      X_train_mCluSize, X_test_mCluSize = train_test_split(X_mCluSize, test_size=0.2, random_state=22, shuffle=True)\n",
        "      X_train_pion_candidates, X_test_pion_candidates = train_test_split(X_pion_candidates, test_size=0.2, random_state=22, shuffle=True)\n",
        "      X_train_kaon_candidates, X_test_kaon_candidates = train_test_split(X_kaon_candidates, test_size=0.2, random_state=22, shuffle=True)\n",
        "      X_train_proton_candidates, X_test_proton_candidates = train_test_split(X_proton_candidates, test_size=0.2, random_state=22, shuffle=True)\n",
        "      X_train_non_candidates, X_test_non_candidates = train_test_split(X_non_candidates, test_size=0.2, random_state=22, shuffle=True)\n",
        "      X_train_momentum, X_test_momentum = train_test_split(X_momentum, test_size=0.2, random_state=22, shuffle=True)\n",
        "      X_train_refractive_index, X_test_refractive_index = train_test_split(X_refractive_index, test_size=0.2, random_state=22, shuffle=True)\n",
        "      X_train_phi, X_test_phi = train_test_split(X_phi, test_size=0.2, random_state=22, shuffle=True)\n",
        "      X_train_theta, X_test_theta = train_test_split(X_theta, test_size=0.2, random_state=22, shuffle=True)\n",
        "      X_train_mip_position, X_test_mip_position = train_test_split(X_mip_position, test_size=0.2, random_state=22, shuffle=True)\n",
        "      X_train_rad_position, X_test_rad_position = train_test_split(X_rad_position, test_size=0.2, random_state=22, shuffle=True)\n",
        "      X_train_map_pion, X_test_map_pion = train_test_split(X_map_pion, test_size=0.2, random_state=22, shuffle=True)\n",
        "      X_train_map_kaon, X_test_map_kaon = train_test_split(X_map_kaon, test_size=0.2, random_state=22, shuffle=True)\n",
        "      X_train_map_proton, X_test_map_proton = train_test_split(X_map_proton, test_size=0.2, random_state=22, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      X_train_pion_flag, X_test_pion_flag = train_test_split(X_pion_flag, test_size=0.2, random_state=22, shuffle=True)\n",
        "      X_train_kaon_flag, X_test_kaon_flag = train_test_split(X_kaon_flag, test_size=0.2, random_state=22, shuffle=True)\n",
        "      X_train_proton_flag, X_test_proton_flag = train_test_split(X_proton_flag, test_size=0.2, random_state=22, shuffle=True)\n",
        "\n",
        "\n",
        "      X_train_index_particle, X_test_index_particle = train_test_split(X_index_particle, test_size=0.2, random_state=22, shuffle=True)\n",
        "\n",
        "      y_train, y_test = train_test_split(y, test_size=0.2, random_state=22, shuffle=True)\n",
        "\n",
        "      segmenter = ParticleSegmenter(X_pion_candidates, X_kaon_candidates, X_proton_candidates, X_rad_position, X_mip_position, X_momentum, X_phi, X_theta)\n",
        "\n",
        "      X_pion_candidates, X_pion_constr_min, X_pion_constr_max, X_kaon_candidates, X_kaon_constr_min,\\\n",
        "      X_kaon_constr_max, X_proton_candidates, X_proton_constr_min, X_proton_constr_max = segmenter.do_segment()\n",
        "\n",
        "      regions = {\n",
        "          \"X_pion_constr_min\" : X_pion_constr_min,\n",
        "          \"X_pion_constr_max\" : X_pion_constr_max,\n",
        "          \"X_kaon_constr_min\" : X_kaon_constr_min,\n",
        "          \"X_kaon_constr_max\" : X_kaon_constr_max,\n",
        "          \"X_proton_constr_min\" : X_proton_constr_min,\n",
        "          \"X_proton_constr_max\" : X_proton_constr_max,\n",
        "      }\n",
        "\n",
        "      # add them to all the dictionaries :\n",
        "      import copy\n",
        "      X_train = {\n",
        "        \"X_train_index_particle\": X_train_index_particle,\n",
        "          \"X_train_momentum\": X_train_momentum,\n",
        "          \"X_train_refractive_index\": X_train_refractive_index,\n",
        "          \"X_train_phi\": X_train_phi,\n",
        "          \"X_train_theta\": X_train_theta,\n",
        "          \"X_train_mip_position\": X_train_mip_position,\n",
        "          \"X_train_rad_position\": X_train_rad_position,\n",
        "          \"X_train_pion_candidates\": X_train_pion_candidates,\n",
        "          \"X_train_kaon_candidates\": X_train_kaon_candidates,\n",
        "          \"X_train_proton_candidates\": X_train_proton_candidates,\n",
        "          \"X_train_non_candidates\": X_train_non_candidates,\n",
        "          \"X_train_map_pion\": X_train_map_pion,\n",
        "          \"X_train_map_kaon\": X_train_map_kaon,\n",
        "          \"X_train_map_proton\": X_train_map_proton,\n",
        "          \"X_train_mCluCharge\": X_train_mCluCharge,  # New addition\n",
        "          \"X_train_mCluSize\": X_train_mCluSize       # New addition\n",
        "      }\n",
        "\n",
        "      X_test = {\n",
        "          \"X_test_index_particle\": X_test_index_particle,\n",
        "          \"X_test_momentum\": X_test_momentum,\n",
        "          \"X_test_refractive_index\": X_test_refractive_index,\n",
        "          \"X_test_phi\": X_test_phi,\n",
        "          \"X_test_theta\": X_test_theta,\n",
        "          \"X_test_mip_position\": X_test_mip_position,\n",
        "          \"X_test_rad_position\": X_test_rad_position,\n",
        "          \"X_test_pion_candidates\": X_test_pion_candidates,\n",
        "          \"X_test_kaon_candidates\": X_test_kaon_candidates,\n",
        "          \"X_test_proton_candidates\": X_test_proton_candidates,\n",
        "          \"X_test_non_candidates\" : X_test_non_candidates,\n",
        "          \"X_test_map_pion\": X_test_map_pion,\n",
        "          \"X_test_map_kaon\": X_test_map_kaon,\n",
        "          \"X_test_map_proton\": X_test_map_proton,\n",
        "          \"X_test_mCluCharge\": X_test_mCluCharge,  # New addition\n",
        "          \"X_test_mCluSize\": X_test_mCluSize        # New addition\n",
        "      }\n",
        "      X_train[\"X_train_pion_flag\"] = X_train_pion_flag\n",
        "      X_train[\"X_train_kaon_flag\"] = X_train_kaon_flag\n",
        "      X_train[\"X_train_proton_flag\"] = X_train_proton_flag\n",
        "\n",
        "      X_test[\"X_test_pion_flag\"] = X_test_pion_flag\n",
        "      X_test[\"X_test_kaon_flag\"] = X_test_kaon_flag\n",
        "      X_test[\"X_test_proton_flag\"] = X_test_proton_flag\n",
        "\n",
        "      X_train_unscaled = copy.deepcopy(X_train)\n",
        "      X_test_unscaled = copy.deepcopy(X_test)\n",
        "\n",
        "      def log_scaler_pion_size(X):\n",
        "          return np.log1p(X)  # log(x + 1)\n",
        "\n",
        "      def log_scaler_pion_charge(X):\n",
        "          return np.log(X + 1e-5)  # log(x + small constant)\n",
        "      from sklearn.preprocessing import FunctionTransformer, MinMaxScaler, StandardScaler, RobustScaler\n",
        "\n",
        "      log_transformer_pion_size = FunctionTransformer(func=np.vectorize(log_scaler_pion_size), validate=True)\n",
        "      log_transformer_pion_charge = FunctionTransformer(func=np.vectorize(log_scaler_pion_charge), validate=True)\n",
        "\n",
        "      min_scaler = MinMaxScaler()\n",
        "      std_scaler = StandardScaler()\n",
        "      robust_scaler = RobustScaler()\n",
        "\n",
        "      n_features = 4\n",
        "\n",
        "      for i in range(n_features):\n",
        "          if i < 2:\n",
        "              scaler = min_scaler\n",
        "          elif i == 2:  # size: peak around 2-- exp decr\n",
        "              scaler = log_transformer_pion_size\n",
        "          else:  # charge ; peak at 0 decr exp\n",
        "              scaler = log_transformer_pion_charge\n",
        "\n",
        "          X_train_pion_candidates[:, :, i] = scaler.fit_transform(X_train_pion_candidates[:, :, i])\n",
        "          X_test_pion_candidates[:, :, i] =scaler.transform(X_test_pion_candidates[:, :, i])\n",
        "          X_train_kaon_candidates[:, :, i] = scaler.fit_transform(X_train_kaon_candidates[:, :, i])\n",
        "          X_test_kaon_candidates[:, :, i] = scaler.transform(X_test_kaon_candidates[:, :, i])\n",
        "          X_train_proton_candidates[:, :, i] = scaler.fit_transform(X_train_proton_candidates[:, :, i])\n",
        "          X_test_proton_candidates[:, :, i] =scaler.transform(X_test_proton_candidates[:, :, i])\n",
        "\n",
        "\n",
        "          X_train_non_candidates[:, :, i] = scaler.fit_transform(X_train_non_candidates[:, :, i])\n",
        "          X_test_non_candidates[:, :, i] =scaler.transform(X_test_non_candidates[:, :, i])\n",
        "\n",
        "      # print(X_train_rad_position.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      X_train_rad_position[:, 0] = min_scaler.fit_transform(X_train_rad_position[:, 0])\n",
        "      X_train_rad_position[:, 1] = min_scaler.fit_transform(X_train_rad_position[:, 1])\n",
        "\n",
        "      X_test_rad_position[:, 0] = min_scaler.transform(X_test_rad_position[:, 0])\n",
        "      X_test_rad_position[:, 1] = min_scaler.transform(X_test_rad_position[:, 1])\n",
        "\n",
        "      X_train_mip_position[:, 0] = min_scaler.fit_transform(X_train_mip_position[:, 0])\n",
        "      X_train_mip_position[:, 1] = min_scaler.fit_transform(X_train_mip_position[:, 1])\n",
        "\n",
        "      X_test_mip_position[:, 0] = min_scaler.transform(X_test_mip_position[:, 0])\n",
        "      X_test_mip_position[:, 1] = min_scaler.transform(X_test_mip_position[:, 1])\n",
        "\n",
        "      X_train_momentum = robust_scaler.fit_transform(X_train_momentum.reshape(-1, 1)).flatten()\n",
        "      X_test_momentum = robust_scaler.transform(X_test_momentum.reshape(-1, 1)).flatten()\n",
        "\n",
        "      # For phi\n",
        "      X_train_phi = robust_scaler.fit_transform(X_train_phi.reshape(-1, 1)).flatten()\n",
        "      X_test_phi = robust_scaler.transform(X_test_phi.reshape(-1, 1)).flatten()\n",
        "\n",
        "      # For theta\n",
        "      X_train_theta = robust_scaler.fit_transform(X_train_theta.reshape(-1, 1)).flatten()\n",
        "      X_test_theta = robust_scaler.transform(X_test_theta.reshape(-1, 1)).flatten()\n",
        "\n",
        "      # For refractive index\n",
        "      X_train_refractive_index = self.utils.refractive_index_scaler.fit_transform(X_train_refractive_index.reshape(-1, 1)).flatten()\n",
        "      X_test_refractive_index = self.utils.refractive_index_scaler.transform(X_test_refractive_index.reshape(-1, 1)).flatten()\n",
        "\n",
        "      # For mCluCharge\n",
        "      X_train_mCluCharge =robust_scaler.fit_transform(X_train_mCluCharge.reshape(-1, 1)).flatten()  # Assumes you've created mCluCharge_scaler\n",
        "      X_test_mCluCharge = robust_scaler.transform(X_test_mCluCharge.reshape(-1, 1)).flatten()\n",
        "\n",
        "      # For mCluSize\n",
        "      X_train_mCluSize = robust_scaler.fit_transform(X_train_mCluSize.reshape(-1, 1)).flatten()  # Assumes you've created mCluSize_scaler\n",
        "      X_test_mCluSize = robust_scaler.transform(X_test_mCluSize.reshape(-1, 1)).flatten()\n",
        "\n",
        "\n",
        "      # aldos to thse dicts\n",
        "      X_train = {\n",
        "          \"X_train_index_particle\": X_train_index_particle,\n",
        "          \"X_train_momentum\": X_train_momentum,\n",
        "          \"X_train_refractive_index\": X_train_refractive_index,\n",
        "          \"X_train_phi\": X_train_phi,\n",
        "          \"X_train_theta\": X_train_theta,\n",
        "          \"X_train_mip_position\": X_train_mip_position,\n",
        "          \"X_train_rad_position\": X_train_rad_position,\n",
        "          \"X_train_pion_candidates\": X_train_pion_candidates,\n",
        "          \"X_train_kaon_candidates\": X_train_kaon_candidates,\n",
        "          \"X_train_proton_candidates\": X_train_proton_candidates,\n",
        "          \"X_train_non_candidates\" : X_train_non_candidates,\n",
        "          \"X_train_map_pion\": X_train_map_pion,\n",
        "          \"X_train_map_kaon\": X_train_map_kaon,\n",
        "          \"X_train_map_proton\": X_train_map_proton,\n",
        "          \"X_train_mCluCharge\": X_train_mCluCharge,  # New addition\n",
        "          \"X_train_mCluSize\": X_train_mCluSize       # New addition\n",
        "      }\n",
        "\n",
        "      X_test = {\n",
        "          \"X_test_index_particle\": X_test_index_particle,\n",
        "          \"X_test_momentum\": X_test_momentum,\n",
        "          \"X_test_refractive_index\": X_test_refractive_index,\n",
        "          \"X_test_phi\": X_test_phi,\n",
        "          \"X_test_theta\": X_test_theta,\n",
        "          \"X_test_mip_position\": X_test_mip_position,\n",
        "          \"X_test_rad_position\": X_test_rad_position,\n",
        "          \"X_test_pion_candidates\": X_test_pion_candidates,\n",
        "          \"X_test_kaon_candidates\": X_test_kaon_candidates,\n",
        "          \"X_test_proton_candidates\": X_test_proton_candidates,\n",
        "          \"X_test_non_candidates\" : X_test_non_candidates,\n",
        "          \"X_test_map_pion\": X_test_map_pion,\n",
        "          \"X_test_map_kaon\": X_test_map_kaon,\n",
        "          \"X_test_map_proton\": X_test_map_proton,\n",
        "          \"X_test_mCluCharge\": X_test_mCluCharge,  # New addition\n",
        "          \"X_test_mCluSize\": X_test_mCluSize        # New addition\n",
        "      }\n",
        "      X_train[\"X_train_pion_flag\"] = X_train_pion_flag\n",
        "      X_train[\"X_train_kaon_flag\"] = X_train_kaon_flag\n",
        "      X_train[\"X_train_proton_flag\"] = X_train_proton_flag\n",
        "\n",
        "      X_test[\"X_test_pion_flag\"] = X_test_pion_flag\n",
        "      X_test[\"X_test_kaon_flag\"] = X_test_kaon_flag\n",
        "      X_test[\"X_test_proton_flag\"] = X_test_proton_flag\n",
        "\n",
        "      print(f\"  return (X_train, X_test, y_train, y_test) \\n:\")\n",
        "      return (X_train, X_test, X_train_unscaled, X_test_unscaled, y_train, y_test, regions)\n",
        "\n",
        "\n",
        "\n",
        "  def build_model(self, input_sequence_length = None, X_train = None, X_test = None, y_train = None, y_test = None, mask = None, epochs = None, include_cnn = False, units = None, units2 = None, final_concat_units = None, CNN_units = None):\n",
        "\n",
        "      dropout = 0.15\n",
        "      alpha = 0.05\n",
        "\n",
        "      #map_shape = (None, None, 1)  # Variable shape for the map input\n",
        "      #X_train_photon_ckov_segmented shape : (1955, 3, 74)\n",
        "\n",
        "      X_train_pion_candidates = X_train[\"X_train_pion_candidates\"]\n",
        "      X_train_kaon_candidates = X_train[\"X_train_kaon_candidates\"]\n",
        "      X_train_proton_candidates = X_train[\"X_train_proton_candidates\"]\n",
        "\n",
        "      X_train_non_candidates = X_train[\"X_train_non_candidates\"]\n",
        "      X_train_momentum = X_train[\"X_train_momentum\"]\n",
        "      X_train_refractive_index = X_train[\"X_train_refractive_index\"]\n",
        "      X_train_phi = X_train[\"X_train_phi\"]\n",
        "      X_train_theta = X_train[\"X_train_theta\"]\n",
        "      X_train_mip_position = X_train[\"X_train_mip_position\"]\n",
        "      X_train_rad_position = X_train[\"X_train_rad_position\"]\n",
        "      X_train_map_pion = X_train[\"X_train_map_pion\"]\n",
        "      X_train_map_kaon = X_train[\"X_train_map_kaon\"]\n",
        "      X_train_map_proton = X_train[\"X_train_map_proton\"]\n",
        "\n",
        "      X_test_pion_candidates = X_test[\"X_test_pion_candidates\"]\n",
        "      X_test_kaon_candidates = X_test[\"X_test_kaon_candidates\"]\n",
        "      X_test_proton_candidates = X_test[\"X_test_proton_candidates\"]\n",
        "      X_test_non_candidates = X_test[\"X_test_non_candidates\"]\n",
        "      X_test_momentum = X_test[\"X_test_momentum\"]\n",
        "      X_test_refractive_index = X_test[\"X_test_refractive_index\"]\n",
        "      X_test_phi = X_test[\"X_test_phi\"]\n",
        "      X_test_theta = X_test[\"X_test_theta\"]\n",
        "      X_test_mip_position = X_test[\"X_test_mip_position\"]\n",
        "      X_test_rad_position = X_test[\"X_test_rad_position\"]\n",
        "      X_test_map_pion = X_test[\"X_test_map_pion\"]\n",
        "      X_test_map_kaon = X_test[\"X_test_map_kaon\"]\n",
        "      X_test_map_proton = X_test[\"X_test_map_proton\"]\n",
        "\n",
        "      X_train_mCluCharge = X_train[\"X_train_mCluCharge\"]\n",
        "      X_train_mCluSize = X_train[\"X_train_mCluSize\"]\n",
        "      X_test_mCluCharge = X_test[\"X_test_mCluCharge\"]\n",
        "      X_test_mCluSize = X_test[\"X_test_mCluSize\"]\n",
        "\n",
        "\n",
        "\n",
        "      X_train_pion_flag = X_train[\"X_train_pion_flag\"]\n",
        "      X_train_kaon_flag = X_train[\"X_train_kaon_flag\"]\n",
        "      X_train_proton_flag = X_train[\"X_train_proton_flag\"]\n",
        "\n",
        "      X_test_pion_flag = X_test[\"X_test_pion_flag\"]\n",
        "      X_test_kaon_flag = X_test[\"X_test_kaon_flag\"]\n",
        "      X_test_proton_flag = X_test[\"X_test_proton_flag\"]\n",
        "\n",
        "\n",
        "      X_test_map_pion = tf.expand_dims(X_test_map_pion, axis=-1)\n",
        "      X_test_map_kaon = tf.expand_dims(X_test_map_kaon, axis=-1)\n",
        "      X_test_map_proton = tf.expand_dims(X_test_map_proton, axis=-1)\n",
        "\n",
        "\n",
        "      X_train_map_pion = tf.expand_dims(X_train_map_pion, axis=-1)\n",
        "      X_train_map_kaon = tf.expand_dims(X_train_map_kaon, axis=-1)\n",
        "      X_train_map_proton = tf.expand_dims(X_train_map_proton, axis=-1)\n",
        "\n",
        "\n",
        "\n",
        "      #plot_function(X_train_pion_candidates, X_train_kaon_candidates, X_train_proton_candidates, X_train_mip_position, X_train_rad_position, X_train_phi, X_train_theta)\n",
        "      print(\"Fields in the first vector of X_train:\")\n",
        "      print(\"X_train_pion_candidates shape:\", X_train_pion_candidates.shape)\n",
        "      print(\"X_train_kaon_candidates shape:\", X_train_kaon_candidates.shape)\n",
        "      print(\"X_train_proton_candidates shape:\", X_train_proton_candidates.shape)\n",
        "      print(\"X_train_momentum shape:\", X_train_momentum.shape)\n",
        "      print(\"X_train_refractive_index shape:\", X_train_refractive_index.shape)\n",
        "      print(\"X_train_phi shape:\", X_train_phi.shape)\n",
        "      print(\"X_train_theta shape:\", X_train_theta.shape)\n",
        "      print(\"X_train_mip_position shape:\", X_train_mip_position.shape)\n",
        "      print(\"X_train_rad_position shape:\", X_train_rad_position.shape)\n",
        "      print(\"X_train_map_pion shape:\", X_train_map_pion.shape)\n",
        "      print(\"X_train_map_kaon shape:\", X_train_map_kaon.shape)\n",
        "      print(\"X_train_map_proton shape:\", X_train_map_proton.shape)\n",
        "\n",
        "      print(\"\\nFields in the first vector of X_test:\")\n",
        "      print(\"X_test_pion_candidates shape:\", X_test_pion_candidates.shape)\n",
        "      print(\"X_test_kaon_candidates shape:\", X_test_kaon_candidates.shape)\n",
        "      print(\"X_test_proton_candidates shape:\", X_test_proton_candidates.shape)\n",
        "      print(\"X_test_momentum shape:\", X_test_momentum.shape)\n",
        "      print(\"X_test_refractive_index shape:\", X_test_refractive_index.shape)\n",
        "      print(\"X_test_phi shape:\", X_test_phi.shape)\n",
        "      print(\"X_test_theta shape:\", X_test_theta.shape)\n",
        "      print(\"X_test_mip_position shape:\", X_test_mip_position.shape)\n",
        "      print(\"X_test_rad_position shape:\", X_test_rad_position.shape)\n",
        "      print(\"X_test_map_pion shape:\", X_test_map_pion.shape)\n",
        "      print(\"X_test_map_kaon shape:\", X_test_map_kaon.shape)\n",
        "      print(\"X_test_map_proton shape:\", X_test_map_proton.shape)\n",
        "\n",
        "      #for i in range(1,5):\n",
        "      #  print(\"X_train_pion_candidates[0,0,i]\", X_train_pion_candidates[0,0,i])\n",
        "\n",
        "      #print(X_train_pion_candidates[:3])\n",
        "\n",
        "      X_train_pion_candidates = np.array(X_train_pion_candidates, dtype='float32')\n",
        "      X_train_kaon_candidates = np.array(X_train_kaon_candidates, dtype='float32')\n",
        "      X_train_proton_candidates = np.array(X_train_proton_candidates, dtype='float32')\n",
        "      X_train_non_candidates = np.array(X_train_non_candidates, dtype='float32')\n",
        "\n",
        "\n",
        "      X_train_pion_candidates = np.array(X_train_pion_candidates, dtype='float32')\n",
        "      X_train_kaon_candidates = np.array(X_train_kaon_candidates, dtype='float32')\n",
        "      X_train_proton_candidates = np.array(X_train_proton_candidates, dtype='float32')\n",
        "      X_test_non_candidates = np.array(X_test_non_candidates, dtype='float32')\n",
        "      # # For train data\n",
        "      # train_variables = [X_train_pion_candidates, X_train_kaon_candidates, X_train_proton_candidates, X_train_non_candidates,\n",
        "      #                   X_train_momentum, X_train_refractive_index, X_train_phi, X_train_theta,\n",
        "      #                   X_train_mip_position, X_train_rad_position,\n",
        "      #                    X_train_map_pion, X_train_map_kaon, X_train_map_proton,\n",
        "      #                    X_train_mCluCharge, X_train_mCluSize]\n",
        "\n",
        "\n",
        "\n",
        "      # train_labels = ['Train Pion Candidates', 'Train Kaon Candidates', 'Train Proton Candidates',\n",
        "      #                 'Train Momentum', 'Train Refractive Index', 'Train Phi', 'Train Theta',\n",
        "      #                 'Train MIP Position', 'Train Rad Position',\n",
        "      #                 'Train Map Pion','Train Map Kaon', 'Train Map Proton',\n",
        "      #                 'Train mCluCharge', 'Train mCluSize']\n",
        "\n",
        "      # for label, variable in zip(train_labels, train_variables):\n",
        "      #     print(f\"{label}: {variable.dtype}\")\n",
        "\n",
        "\n",
        "\n",
        "      # test_variables = [X_test_pion_candidates, X_test_kaon_candidates, X_test_proton_candidates, X_test_non_candidates,\n",
        "      #                   X_test_momentum, X_test_refractive_index, X_test_phi, X_test_theta,\n",
        "      #                   X_test_mip_position, X_test_rad_position,\n",
        "      #                   X_test_map_pion, X_test_map_kaon, X_test_map_proton,\n",
        "      #                   X_test_mCluCharge, X_test_mCluSize]\n",
        "\n",
        "      # test_labels = ['Test Pion Candidates', 'Test Kaon Candidates', 'Test Proton Candidates',\n",
        "      #               'Test Momentum', 'Test Refractive Index', 'Test Phi', 'Test Theta',\n",
        "      #               'Test MIP Position', 'Test Rad Position',\n",
        "      #               'Test Map Pion','Test Map Kaon', 'Test Map Proton',\n",
        "      #               'Test mCluCharge', 'Test mCluSize']\n",
        "\n",
        "\n",
        "      # TODO: evaluer hva som er best !\n",
        "      # 1. sende inn X_test_cand_status_encoded, X_test_cand_pos som det er naa\n",
        "      # 2. bare sende inn de som har status != 0\n",
        "      # 3 splitte til pion, proton, kaon candidates og sende disse inn\n",
        "\n",
        "\n",
        "      len_pion = np.asarray(X_test_pion_candidates, dtype = object).shape[1]\n",
        "      len_kaon = np.asarray(X_test_kaon_candidates, dtype = object).shape[1]\n",
        "      len_proton = np.asarray(X_test_proton_candidates, dtype = object).shape[1]\n",
        "\n",
        "      num_clu_vars = X_test_non_candidates.shape[2]\n",
        "      pion_ip = Input(shape=(len_pion, num_clu_vars), name=\"pion_ip\")\n",
        "      kaon_ip = Input(shape=(len_kaon, num_clu_vars), name=\"kaon_ip\")\n",
        "      proton_ip = Input(shape=(len_proton, num_clu_vars), name=\"proton_ip\")\n",
        "      none_ip = Input(shape=(len_proton, num_clu_vars), name=\"none_ip\")\n",
        "\n",
        "      map_size = nb_size = 50\n",
        "      pion_ip_map =Input(shape=(map_size, map_size, 1), name=\"pion_ip_map\")\n",
        "      kaon_ip_map  = Input(shape=(map_size, map_size, 1), name=\"kaon_ip_map\")\n",
        "      proton_ip_map  =Input(shape=(map_size, map_size, 1), name=\"proton_ip_map\")\n",
        "\n",
        "      # pion_ip = Input(shape=(len_pion, 2), name=\"pion_ip\")\n",
        "      # kaon_ip = Input(shape=(len_kaon, 2), name=\"kaon_ip\")\n",
        "      # proton_ip = Input(shape=(len_proton, 2), name=\"proton_ip\")\n",
        "\n",
        "      none_ip_mask = Masking(mask_value=0.)(none_ip)\n",
        "\n",
        "      pion_ip_mask = Masking(mask_value=0.)(pion_ip)\n",
        "      kaon_ip_mask = Masking(mask_value=0.)(kaon_ip)\n",
        "      proton_ip_mask = Masking(mask_value=0.)(proton_ip)\n",
        "\n",
        "      prev_pion = pion_ip_mask # NB change back to mask\n",
        "      prev_kaon = kaon_ip_mask\n",
        "      prev_proton = proton_ip_mask\n",
        "      prev_none = none_ip_mask\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      # Grid search parameters\n",
        "      # Grid search parameters\n",
        "      filter_sizes = [5]  # Filter sizes to test\n",
        "      num_filters = [32]#[16, 32]  # Number of filters to test\n",
        "      strides = [(2, 2)]#[(1, 1), (2, 2)]  # Strides to test\n",
        "      pool_sizes = [(2, 2)]  # Max pooling sizes to test\n",
        "      fc1_units = [64]#, 128]  # Number of units in fc1 to test\n",
        "      fc2_units = [32]#, 32]  # Number of units in fc2 to test\n",
        "\n",
        "      dropouts = [0.225, 0.3, 0.35, 0.4]\n",
        "      best_accuracy = 0\n",
        "      best_model = None\n",
        "      alphas = [0.05, 0.1] # 0.05 var bra ca 81%\n",
        "\n",
        "      #\n",
        "      #\n",
        "      #35s 171ms/step - loss: 0.7475 - accuracy: 0.8152 - val_loss: 1.1083 - val_accuracy: 0.7360 - lr: 9.8639e-05 0.05\n",
        "      #loss: 0.7370 - accuracy: 0.8106 - val_loss: 1.1769 - val_accuracy: 0.7280 - lr: 9.8639e-05 0.1\n",
        "\n",
        "      l1l2_weights_mip = [\n",
        "          [(0.01, 0.01, 0.01), (0.02, 0.02, 0.02), (0.03, 0.03, 0.03), (0.04, 0.04, 0.04), (0.05, 0.05, 0.05),\n",
        "          (0.06, 0.06, 0.06), (0.07, 0.07, 0.07), (0.08, 0.08, 0.08), (0.09, 0.09, 0.09), (0.1, 0.1, 0.1), (0.09, 0.09, 0.09), (0.1, 0.1, 0.1), (0.09, 0.09, 0.09), (0.1, 0.1, 0.1)]\n",
        "      ]\n",
        "      l1l2_weights_mip = [[(w1/10, w2/10, w3/10) for (w1, w2, w3) in sublist] for sublist in l1l2_weights_mip]\n",
        "\n",
        "      l1l2_weights_ref_index = [\n",
        "          [(0.01, 0.01, 0.01), (0.02, 0.02, 0.02), (0.03, 0.03, 0.03), (0.04, 0.04, 0.04), (0.05, 0.05, 0.05),\n",
        "          (0.06, 0.06, 0.06), (0.07, 0.07, 0.07), (0.08, 0.08, 0.08), (0.09, 0.09, 0.09), (0.1, 0.1, 0.1), (0.09, 0.09, 0.09), (0.1, 0.1, 0.1), (0.09, 0.09, 0.09), (0.1, 0.1, 0.1)]\n",
        "      ]\n",
        "      l1l2_weights_ref_index = [[(w1/10, w2/10, w3/10) for (w1, w2, w3) in sublist] for sublist in l1l2_weights_ref_index]\n",
        "\n",
        "\n",
        "      l1l2_weights_pos = [[(w1/10, w2/10, w3/10) for (w1, w2, w3) in sublist] for sublist in l1l2_weights_ref_index]\n",
        "\n",
        "      l1l2_weights_hadrons = [\n",
        "          [(0.1, 0.1, 0.1), (0.1, 0.1, 0.1), (0.1, 0.1, 0.1),(0.1, 0.1, 0.1),(0.1, 0.1, 0.1),(0.1, 0.1, 0.1),(0.1, 0.1, 0.1),(0.1, 0.1, 0.1),\n",
        "         (0.1, 0.1, 0.1),(0.1, 0.1, 0.1),(0.1, 0.1, 0.1),(0.1, 0.1, 0.1),(0.1, 0.1, 0.1),(0.1, 0.1, 0.1),\n",
        "          (0.1, 0.1, 0.1),(0.1, 0.1, 0.1),(0.1, 0.1, 0.1),(0.1, 0.1, 0.1),(0.1, 0.1, 0.1),(0.1, 0.1, 0.1),(0.1, 0.1, 0.1),(0.1, 0.1, 0.1),(0.1, 0.1, 0.1)]\n",
        "      ]\n",
        "      l1l2_weights_hadrons = [[(w1/10, w2/10, w3/10) for (w1, w2, w3) in sublist] for sublist in l1l2_weights_hadrons]\n",
        "\n",
        "      l1l2_weights_momentum = [\n",
        "          [(0.01, 0.01, 0.01), (0.02, 0.02, 0.02), (0.1, 0.1, 0.1), (0.1, 0.1, 0.1), (0.1, 0.1, 0.1),\n",
        "          (0.1, 0.1, 0.1), (0.1, 0.1, 0.1), (0.1, 0.1, 0.1), (0.1, 0.1, 0.), (0.1, 0.1, 0.1), (0.09, 0.09, 0.09), (0.1, 0.1, 0.1), (0.09, 0.09, 0.09), (0.1, 0.1, 0.1)]\n",
        "      ]\n",
        "      l1l2_weights_momentum = [[(w1/10, w2/10, w3/10) for (w1, w2, w3) in sublist] for sublist in l1l2_weights_momentum]\n",
        "\n",
        "\n",
        "\n",
        "      scale = 0.01\n",
        "      # Assigning a constant regularization weight for all sublists, with the same scale\n",
        "      l1l2_weights_mip = [[(0.001*scale, 0.001*scale, 0.001*scale) for _ in sublist] for sublist in l1l2_weights_mip]\n",
        "      l1l2_weights_ref_index = [[(0.001*scale, 0.001*scale, 0.001*scale) for _ in sublist] for sublist in l1l2_weights_ref_index]\n",
        "      l1l2_weights_momentum = [[(0.001*scale, 0.001*scale, 0.001*scale) for _ in sublist] for sublist in l1l2_weights_momentum]\n",
        "      l1l2_weights_rad = [[(0.001*scale, 0.001*scale, 0.001*scale) for _ in sublist] for sublist in l1l2_weights_momentum]\n",
        "      l1l2_weights_phi = [[(0.001*scale, 0.001*scale, 0.001*scale) for _ in sublist] for sublist in l1l2_weights_momentum]\n",
        "      l1l2_weights_theta = [[(0.001*scale, 0.001*scale, 0.001*scale) for _ in sublist] for sublist in l1l2_weights_momentum]\n",
        "      l1l2_weights_cand_pos = [[(0.001*scale, 0.001*scale, 0.001*scale) for _ in sublist] for sublist in l1l2_weights_momentum]\n",
        "      l1l2_weights_cand_status = [[(0.001*scale, 0.001*scale, 0.001*scale) for _ in sublist] for sublist in l1l2_weights_momentum]\n",
        "      l1l2_weights_pos = [[(0.001*scale, 0.001*scale, 0.001*scale) for _ in sublist] for sublist in l1l2_weights_momentum]\n",
        "\n",
        "\n",
        "      print(f\"l1l2_weights_mip shape {np.array(l1l2_weights_mip, dtype =object).shape}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      momentum_shape = refractive_index_shape = phi_shape = theta_shape = energy_shape = (1, )\n",
        "\n",
        "\n",
        "      pion_flag_ip = kaon_flag_ip = proton_flag_ip  = (1, )\n",
        "\n",
        "\n",
        "      #momentum_shape = refractive_index_shape = phi_shape = theta_shape = energy_shape = (len_proton, 1)\n",
        "\n",
        "      # add the flag inputs here\n",
        "      pion_flag_input = Input(shape=pion_flag_ip, name=\"pion_flag_input\")\n",
        "      kaon_flag_input = Input(shape=kaon_flag_ip, name=\"kaon_flag_input\")\n",
        "      proton_flag_input = Input(shape=proton_flag_ip, name=\"proton_flag_input\")\n",
        "\n",
        "      mip_charge_input  = Input(shape=phi_shape, name=\"mip_charge_input\")\n",
        "      mip_size_input  = Input(shape=phi_shape, name=\"mip_size_input\")\n",
        "\n",
        "      phi_input = Input(shape=phi_shape, name=\"phi_shape\")\n",
        "      theta_input = Input(shape=theta_shape, name=\"theta_shape\")\n",
        "      momentum_input = Input(shape=momentum_shape, name=\"momentum_input\")\n",
        "      refractive_index_input = Input(shape=refractive_index_shape, name=\"refractive_index_input\")\n",
        "      mip_position_shape = rad_position_shape = (2,)\n",
        "      #mip_position_shape = rad_position_shape = (len_proton, 2)\n",
        "\n",
        "      rad_position_input = Input(shape=rad_position_shape, name=\"rad_position_input\")\n",
        "      mip_position_input = Input(shape=mip_position_shape, name=\"mip_position_input\")\n",
        "\n",
        "      print(\"mapplying inputs\")\n",
        "\n",
        "      # For training data\n",
        "      # For train data\n",
        "\n",
        "\n",
        "\n",
        "      print(\"now create ip shapes pion_input_shape\")\n",
        "\n",
        "\n",
        "\n",
        "      # CNN\n",
        "      # pion_input_shape = (map_size[0], map_size[1], 1)\n",
        "      # kaon_input_shape = (map_size[0], map_size[1], 1)\n",
        "      # proton_input_shape = (map_size[0], map_size[1], 1)\n",
        "\n",
        "\n",
        "      a = 1\n",
        "      if include_cnn == True:\n",
        "          inputs = [pion_ip_mask, kaon_ip_mask, proton_ip_mask, phi_input, theta_input, refractive_index_input, momentum_input, rad_position_input, mip_position_input, mip_charge_input, mip_size_input, pion_ip_map, kaon_ip_map, proton_ip_map]\n",
        "\n",
        "          train_variables = [X_train_pion_candidates, X_train_kaon_candidates, X_train_proton_candidates, X_train_phi, X_train_theta, X_train_refractive_index, X_train_momentum, X_train_rad_position, X_train_mip_position, X_train_mCluCharge, X_train_mCluSize, X_train_map_pion, X_train_map_kaon, X_train_map_proton]\n",
        "\n",
        "          train_labels = ['Train Pion Candidates', 'Train Kaon Candidates', 'Train Proton Candidates', 'Train Phi', 'Train Theta', 'Train Refractive Index', 'Train Momentum', 'Train Rad Position', 'Train MIP Position', 'Train mCluCharge', 'Train mCluSize', 'Train Map Pion','Train Map Kaon', 'Train Map Proton']\n",
        "\n",
        "          # Likewise, for the test data\n",
        "          test_variables = [X_test_pion_candidates, X_test_kaon_candidates, X_test_proton_candidates, X_test_phi, X_test_theta, X_test_refractive_index, X_test_momentum, X_test_rad_position, X_test_mip_position, X_test_mCluCharge, X_test_mCluSize, X_test_map_pion, X_test_map_kaon, X_test_map_proton]\n",
        "\n",
        "          test_labels = ['Test Pion Candidates', 'Test Kaon Candidates', 'Test Proton Candidates', 'Test Phi', 'Test Theta', 'Test Refractive Index', 'Test Momentum', 'Test Rad Position', 'Test MIP Position','Test mCluCharge', 'Test mCluSize', 'Test Map Pion','Test Map Kaon', 'Test Map Proton']\n",
        "\n",
        "      elif a == 1:\n",
        "          inputs = [pion_ip_mask, kaon_ip_mask, proton_ip_mask, none_ip_mask, phi_input, theta_input, refractive_index_input, momentum_input, rad_position_input, mip_position_input, mip_charge_input, mip_size_input]\n",
        "\n",
        "          train_variables = [X_train_pion_candidates, X_train_kaon_candidates, X_train_proton_candidates, X_train_non_candidates, X_train_phi, X_train_theta, X_train_refractive_index, X_train_momentum, X_train_rad_position, X_train_mip_position, X_train_mCluCharge, X_train_mCluSize]\n",
        "\n",
        "          train_labels = ['Train Pion Candidates', 'Train Kaon Candidates', 'Train Proton Candidates', 'Train Phi', 'Train Theta', 'Train Refractive Index', 'Train Momentum', 'Train Rad Position', 'Train MIP Position', 'Train mCluCharge', 'Train mCluSize']\n",
        "\n",
        "          # Likewise, for the test data\n",
        "          test_variables = [X_test_pion_candidates, X_test_kaon_candidates, X_test_proton_candidates, X_test_non_candidates, X_test_phi, X_test_theta, X_test_refractive_index, X_test_momentum, X_test_rad_position, X_test_mip_position, X_test_mCluCharge, X_test_mCluSize]\n",
        "\n",
        "          test_labels = ['Test Pion Candidates', 'Test Kaon Candidates', 'Test Proton Candidates', 'Test Phi', 'Test Theta', 'Test Refractive Index', 'Test Momentum', 'Test Rad Position', 'Test MIP Position','Test mCluCharge', 'Test mCluSize']\n",
        "\n",
        "      else:\n",
        "          inputs = [pion_ip_mask, kaon_ip_mask, proton_ip_mask, mip_position_input]\n",
        "\n",
        "          train_variables = [X_train_pion_candidates, X_train_kaon_candidates, X_train_proton_candidates, X_train_mip_position]\n",
        "\n",
        "          train_labels = ['Train Pion Candidates', 'Train Kaon Candidates', 'Train Proton Candidates', 'Train Phi', 'Train Theta', 'Train Refractive Index', 'Train Momentum', 'Train Rad Position', 'Train MIP Position', 'Train mCluCharge', 'Train mCluSize']\n",
        "\n",
        "          test_variables = [X_test_pion_candidates, X_test_kaon_candidates, X_test_proton_candidates, X_test_mip_position]\n",
        "          test_labels = ['Test Pion Candidates', 'Test Kaon Candidates', 'Test Proton Candidates', 'Test Phi', 'Test Theta', 'Test Refractive Index', 'Test Momentum', 'Test Rad Position', 'Test MIP Position','Test mCluCharge', 'Test mCluSize']\n",
        "\n",
        "      d=1\n",
        "      for input_layer in inputs:\n",
        "          print(f\"input{d} = {input_layer}, input_shape = {input_layer.shape}\")\n",
        "          d = d+1\n",
        "      num_filters = strides = 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      # print the number of nan\n",
        "      #print(f\"Number of nan values in pion_candidates_test = {nan_count}\")\n",
        "      pion_ip_mask = Masking(mask_value=0.)(pion_ip)\n",
        "      kaon_ip_mask = Masking(mask_value=0.)(kaon_ip)\n",
        "      proton_ip_mask = Masking(mask_value=0.)(proton_ip)\n",
        "      none_ip_mask = Masking(mask_value=0.)(none_ip)\n",
        "\n",
        "\n",
        "      for i in range(1, len(units)):\n",
        "          unit = int(units[i])\n",
        "\n",
        "          # For pion\n",
        "          dense_pion_i = Dense(unit, name=f\"dense_pion_{i}{dropout}\", kernel_initializer='he_normal',\n",
        "                              kernel_regularizer=regularizers.L1L2(l1l2_weights_hadrons[0][i - 1][0], l1l2_weights_hadrons[0][i - 1][1]),\n",
        "                              bias_regularizer=regularizers.L1(l1l2_weights_hadrons[0][i - 1][2]))(prev_pion)\n",
        "\n",
        "          bn_pion_i = BatchNormalization(name=f\"bn_pion_{i}{dropout}\")(dense_pion_i)\n",
        "          leakyrelu_pion_i = LeakyReLU(alpha=alpha, name=f\"leakyrelu_pion_{i}{dropout}\")(bn_pion_i)\n",
        "          dropout_pion_i = Dropout(dropout, name=f\"dropout_pion_{i}{dropout}\")(leakyrelu_pion_i)\n",
        "          prev_pion = dropout_pion_i\n",
        "\n",
        "          # For kaon\n",
        "          dense_kaon_i = Dense(unit, name=f\"dense_kaon_{i}{dropout}\", kernel_initializer='he_normal',\n",
        "                              kernel_regularizer=regularizers.L1L2(l1l2_weights_hadrons[0][i - 1][0], l1l2_weights_hadrons[0][i - 1][1]),\n",
        "                              bias_regularizer=regularizers.L1(l1l2_weights_hadrons[0][i - 1][2]))(prev_kaon)\n",
        "\n",
        "          bn_kaon_i = BatchNormalization(name=f\"bn_kaon_{i}{dropout}\")(dense_kaon_i)\n",
        "          leakyrelu_kaon_i = LeakyReLU(alpha=alpha, name=f\"leakyrelu_kaon_{i}{dropout}\")(bn_kaon_i)\n",
        "          dropout_kaon_i = Dropout(dropout, name=f\"dropout_kaon_{i}{dropout}\")(leakyrelu_kaon_i)\n",
        "          prev_kaon = dropout_kaon_i\n",
        "\n",
        "          # For proton\n",
        "          dense_proton_i = Dense(unit, name=f\"dense_proton_{i}{dropout}\", kernel_initializer='he_normal',\n",
        "                                kernel_regularizer=regularizers.L1L2(l1l2_weights_hadrons[0][i - 1][0], l1l2_weights_hadrons[0][i - 1][1]),\n",
        "                                bias_regularizer=regularizers.L1(l1l2_weights_hadrons[0][i - 1][2]))(prev_proton)\n",
        "\n",
        "          bn_proton_i = BatchNormalization(name=f\"bn_proton_{i}{dropout}\")(dense_proton_i)\n",
        "          leakyrelu_proton_i = LeakyReLU(alpha=alpha, name=f\"leakyrelu_proton_{i}{dropout}\")(bn_proton_i)\n",
        "          dropout_proton_i = Dropout(dropout, name=f\"dropout_proton_{i}{dropout}\")(leakyrelu_proton_i)\n",
        "          prev_proton = dropout_proton_i\n",
        "\n",
        "\n",
        "          # For none\n",
        "          dense_none_i = Dense(unit, name=f\"dense_none_{i}{dropout}\", kernel_initializer='he_normal',\n",
        "                                kernel_regularizer=regularizers.L1L2(l1l2_weights_hadrons[0][i - 1][0], l1l2_weights_hadrons[0][i - 1][1]),\n",
        "                                bias_regularizer=regularizers.L1(l1l2_weights_hadrons[0][i - 1][2]))(prev_none)\n",
        "\n",
        "          bn_non_i = BatchNormalization(name=f\"bn_nonn_{i}{dropout}\")(dense_none_i)\n",
        "          leakyrelu_non_i = LeakyReLU(alpha=alpha, name=f\"leakyrelu_non_{i}{dropout}\")(bn_non_i)\n",
        "          dropout_non_i = Dropout(dropout, name=f\"dropout_non_{i}{dropout}\")(leakyrelu_non_i)\n",
        "          prev_non = dropout_non_i\n",
        "\n",
        "\n",
        "      prev_momentum = momentum_input\n",
        "      prev_momentum = tf.identity(prev_momentum, name=\"prev_momentum\")\n",
        "\n",
        "      prev_ref_index = refractive_index_input\n",
        "      prev_ref_index = tf.identity(prev_ref_index, name=\"prev_ref_index\")\n",
        "\n",
        "      prev_mip = mip_position_input\n",
        "      prev_mip = tf.identity(prev_mip, name=\"prev_mip\")\n",
        "\n",
        "      prev_rad = rad_position_input\n",
        "      prev_rad = tf.identity(prev_rad, name=\"prev_rad\")\n",
        "\n",
        "      prev_phi = phi_input\n",
        "      prev_phi = tf.identity(prev_phi, name=\"prev_phi\")\n",
        "\n",
        "      prev_theta = theta_input\n",
        "      prev_theta = tf.identity(prev_theta, name=\"prev_theta\")\n",
        "\n",
        "      prev_mCluCharge = mip_charge_input\n",
        "      prev_mCluCharge = tf.identity(prev_mCluCharge, name=\"prev_mCluCharge\")\n",
        "\n",
        "      prev_mCluSize = mip_size_input\n",
        "      prev_mCluSize = tf.identity(prev_mCluSize, name=\"prev_mCluSize\")\n",
        "\n",
        "\n",
        "      for i in range(1, len(units2)):\n",
        "          step = int(units2[i])\n",
        "          unit = int(units2[i])\n",
        "\n",
        "          #### REFRACTIVE INDEX #####\n",
        "          dense_ref_index_i = Dense(step, name=f\"dense_ref_index_{i}{dropout}\", kernel_initializer='he_normal',\n",
        "                                    kernel_regularizer=regularizers.L1L2(l1l2_weights_ref_index[0][i - 1][0], l1l2_weights_ref_index[0][i - 1][1]),\n",
        "                                    bias_regularizer=regularizers.L1(l1l2_weights_ref_index[0][i - 1][2]))(prev_ref_index)\n",
        "\n",
        "          bn_ref_index_i = BatchNormalization(name=f\"bn_ref_index_{i}{dropout}\")(dense_ref_index_i)\n",
        "          leakyrelu_ref_index_i = LeakyReLU(alpha=alpha, name=f\"leakyrelu_ref_index_{i}{dropout}\")(bn_ref_index_i)\n",
        "          dropout_ref_index_i = Dropout(dropout, name=f\"dropout_ref_index_{i}{dropout}\")(leakyrelu_ref_index_i)\n",
        "\n",
        "          #### MOMENTUM #####\n",
        "          dense_momentum_i = Dense(step, name=f\"dense_momentum_{i}{dropout}\", kernel_initializer='he_normal', kernel_regularizer=regularizers.L1L2(l1l2_weights_momentum[0][i - 1][0], l1l2_weights_momentum[0][i - 1][1]), bias_regularizer=regularizers.L1(l1l2_weights_momentum[0][i - 1][2]))(prev_momentum)\n",
        "          bn_momentum_i = BatchNormalization(name=f\"bn_momentum_{i}{dropout}\")(dense_momentum_i)\n",
        "          leakyrelu_momentum_i = LeakyReLU(alpha=alpha, name=f\"leakyrelu_momentum_{i}{dropout}\")(bn_momentum_i)\n",
        "          dropout_momentum_i = Dropout(dropout, name=f\"dropout_momentum_{i}{dropout}\")(leakyrelu_momentum_i)\n",
        "\n",
        "          # PHI\n",
        "          dense_phi_i = Dense(step, name=f\"dense_phi_{i}{dropout}\", kernel_initializer='he_normal',\n",
        "                              kernel_regularizer=regularizers.L1L2(l1l2_weights_phi[0][i - 1][0], l1l2_weights_phi[0][i - 1][1]),\n",
        "                              bias_regularizer=regularizers.L1(l1l2_weights_phi[0][i - 1][2]))(prev_phi)\n",
        "          bn_phi_i = BatchNormalization(name=f\"bn_phi_{i}{dropout}\")(dense_phi_i)\n",
        "          leakyrelu_phi_i = LeakyReLU(alpha=alpha, name=f\"leakyrelu_phi_{i}{dropout}\")(bn_phi_i)\n",
        "          dropout_phi_i = Dropout(dropout, name=f\"dropout_phi_{i}{dropout}\")(leakyrelu_phi_i)\n",
        "\n",
        "          # THETA\n",
        "          dense_theta_i = Dense(step, name=f\"dense_theta_{i}{dropout}\", kernel_initializer='he_normal',\n",
        "                              kernel_regularizer=regularizers.L1L2(l1l2_weights_theta[0][i - 1][0], l1l2_weights_theta[0][i - 1][1]),\n",
        "                              bias_regularizer=regularizers.L1(l1l2_weights_theta[0][i - 1][2]))(prev_theta)\n",
        "          bn_theta_i = BatchNormalization(name=f\"bn_theta_{i}{dropout}\")(dense_theta_i)\n",
        "          leakyrelu_theta_i = LeakyReLU(alpha=alpha, name=f\"leakyrelu_theta_{i}{dropout}\")(bn_theta_i)\n",
        "          dropout_theta_i = Dropout(dropout, name=f\"dropout_theta_{i}{dropout}\")(leakyrelu_theta_i)\n",
        "\n",
        "          # RAD_POSITION\n",
        "          dense_rad_position_i = Dense(step, name=f\"dense_rad_position_{i}{dropout}\", kernel_initializer='he_normal',\n",
        "                              kernel_regularizer=regularizers.L1L2(l1l2_weights_rad[0][i - 1][0], l1l2_weights_rad[0][i - 1][1]),\n",
        "                              bias_regularizer=regularizers.L1(l1l2_weights_rad[0][i - 1][2]))(prev_rad)\n",
        "          bn_rad_position_i = BatchNormalization(name=f\"bn_rad_position_{i}{dropout}\")(dense_rad_position_i)\n",
        "          leakyrelu_rad_position_i = LeakyReLU(alpha=alpha, name=f\"leakyrelu_rad_position_{i}{dropout}\")(bn_rad_position_i)\n",
        "          dropout_rad_position_i = Dropout(dropout, name=f\"dropout_rad_position_{i}{dropout}\")(leakyrelu_rad_position_i)\n",
        "\n",
        "\n",
        "          #### MIP POS #####\n",
        "          dense_mip_pos_i = Dense(step/2, name=f\"dense_mip_pos_{i}{dropout}\", kernel_initializer='he_normal',\n",
        "                                  kernel_regularizer=regularizers.L1L2(l1l2_weights_mip[0][i - 1][0], l1l2_weights_mip[0][i - 1][1]),\n",
        "                                  bias_regularizer=regularizers.L1(l1l2_weights_mip[0][i - 1][2]))(prev_mip)\n",
        "\n",
        "          bn_mip_pos_i = BatchNormalization(name=f\"bn_mip_pos_{i}{dropout}\")(dense_mip_pos_i)\n",
        "          leakyrelu_mip_pos_i = LeakyReLU(alpha=alpha, name=f\"leakyrelu_mip_pos_{i}{dropout}\")(bn_mip_pos_i)\n",
        "          dropout_mip_pos_i = Dropout(dropout, name=f\"dropout_mip_pos_{i}{dropout}\")(leakyrelu_mip_pos_i)\n",
        "\n",
        "\n",
        "          prev_mip = dropout_mip_pos_i\n",
        "          prev_rad = dropout_rad_position_i\n",
        "\n",
        "          prev_momentum = dropout_momentum_i\n",
        "          prev_ref_index = dropout_ref_index_i\n",
        "          prev_phi = dropout_phi_i\n",
        "          prev_theta = dropout_theta_i\n",
        "          #### mClucharge #####\n",
        "          dense_mClucharge_i = Dense(step, name=f\"dense_mClucharge_{i}{dropout}\", kernel_initializer='he_normal',\n",
        "                                     kernel_regularizer=regularizers.L1L2(l1l2_weights_theta[0][i - 1][0], l1l2_weights_theta[0][i - 1][1]),\n",
        "                                     bias_regularizer=regularizers.L1(l1l2_weights_theta[0][i - 1][2]))(prev_mCluCharge)\n",
        "          bn_mClucharge_i = BatchNormalization(name=f\"bn_mClucharge_{i}{dropout}\")(dense_mClucharge_i)\n",
        "          leakyrelu_mClucharge_i = LeakyReLU(alpha=alpha, name=f\"leakyrelu_mClucharge_{i}{dropout}\")(bn_mClucharge_i)\n",
        "          dropout_mClucharge_i = Dropout(dropout, name=f\"dropout_mClucharge_{i}{dropout}\")(leakyrelu_mClucharge_i)\n",
        "\n",
        "          #### mCluSize #####\n",
        "          dense_mCluSize_i = Dense(step, name=f\"dense_mCluSize_{i}{dropout}\", kernel_initializer='he_normal',\n",
        "                                   kernel_regularizer=regularizers.L1L2(l1l2_weights_theta[0][i - 1][0], l1l2_weights_theta[0][i - 1][1]),\n",
        "                                   bias_regularizer=regularizers.L1(l1l2_weights_theta[0][i - 1][2]))(prev_mCluSize)\n",
        "          bn_mCluSize_i = BatchNormalization(name=f\"bn_mCluSize_{i}{dropout}\")(dense_mCluSize_i)\n",
        "          leakyrelu_mCluSize_i = LeakyReLU(alpha=alpha, name=f\"leakyrelu_mCluSize_{i}{dropout}\")(bn_mCluSize_i)\n",
        "          dropout_mCluSize_i = Dropout(dropout, name=f\"dropout_mCluSize_{i}{dropout}\")(leakyrelu_mCluSize_i)\n",
        "\n",
        "          prev_mCluCharge  = dropout_mClucharge_i\n",
        "          prev_mCluSize = dropout_mCluSize_i\n",
        "          #  do the same as for the others for mClucharge and mCluSize\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      epochs = epochs\n",
        "      lr = create_lr_scheduler(num_epochs=epochs)\n",
        "\n",
        "      # concat_layers should have same ordeR :\n",
        "\n",
        "      print(\"now concat scalar 2D\")\n",
        "\n",
        "\n",
        "      #  add  mClucharge and mCluSize :\n",
        "      pre_dense_layer = tf.keras.layers.Concatenate(name=\"concat_scalars_2D\")([prev_mip])\n",
        "\n",
        "      pre_dense_layer = tf.keras.layers.Concatenate(name=\"concat_scalars_2D\")([prev_phi, prev_theta, prev_ref_index, prev_momentum, prev_rad, prev_mip, prev_mCluCharge, prev_mCluSize])\n",
        "      # can i flatten something here to make the dimensions match? :\n",
        "\n",
        "      flat_pion = Flatten(name = \"flatpion\")(prev_pion)\n",
        "      flat_kaon = Flatten(name = \"flatkaon\")(prev_kaon)\n",
        "      flat_proton = Flatten(name = \"flatproton\")(prev_proton)\n",
        "      flat_non = Flatten(name = \"flat_non\")(prev_non)\n",
        "\n",
        "\n",
        "      print(\"now call extract_neighborhood_map\")\n",
        "\n",
        "\n",
        "      filters = [128, 128, 128]\n",
        "      stride_arr = [2,2,2]\n",
        "      filter_sizes = [3,5,7]\n",
        "\n",
        "\n",
        "\n",
        "      flat_mapPion = build_species_layers(pion_ip_map, filters, filter_sizes, stride_arr, alpha)\n",
        "      flat_mapKaon = build_species_layers(kaon_ip_map, filters, filter_sizes, stride_arr, alpha)\n",
        "      flat_mapProton = build_species_layers(proton_ip_map, filters, filter_sizes, stride_arr, alpha)\n",
        "\n",
        "      print(\"now concat_cnn done \")\n",
        "      concat_layers = [flat_pion, flat_kaon, flat_proton]#, flat_non]\n",
        "      concat_layers = [flat_pion, flat_kaon, flat_proton, flat_non]\n",
        "\n",
        "      dense_comb = Dense(16)(pre_dense_layer)\n",
        "      leaky_dense_comb = LeakyReLU(alpha=alpha)(dense_comb)\n",
        "      dropout_comb = Dropout(dropout)(leaky_dense_comb)\n",
        "\n",
        "\n",
        "      if include_cnn:\n",
        "        concat_cnn = tf.keras.layers.Concatenate(name=\"ConcatCNN\")([flat_mapPion, flat_mapKaon, flat_mapProton])\n",
        "\n",
        "\n",
        "\n",
        "        un = [128, 64, 16]\n",
        "        for i in un:\n",
        "          dense_cnn = Dense(i, kernel_initializer='he_normal')(concat_cnn)\n",
        "\n",
        "          # Then apply BatchNormalization\n",
        "          bn_dense_cnn = BatchNormalization()(dense_cnn)\n",
        "\n",
        "          # Then apply Activation (LeakyReLU)\n",
        "          leaky_dense_cnn = LeakyReLU(alpha=alpha)(bn_dense_cnn)\n",
        "\n",
        "          # Finally apply Dropout\n",
        "          dropout_cnn = Dropout(dropout)(leaky_dense_cnn)\n",
        "\n",
        "\n",
        "        print(\"now concat all\")\n",
        "        final_concat_layer = tf.keras.layers.Concatenate(name=\"concat_scalars_all\")(concat_layers  +  [pre_dense_layer]  + [dropout_cnn])\n",
        "\n",
        "        if final_concat_units is None:\n",
        "          final_concat_units = [128, 64, 16]\n",
        "        for i in final_concat_units:\n",
        "          dense_final = Dense(i, kernel_initializer='he_normal')(final_concat_layer)\n",
        "          bn_final = BatchNormalization()(dense_final)\n",
        "          leaky_final = LeakyReLU(alpha=alpha)(bn_final)\n",
        "          final_concat_layer = Dropout(dropout)(leaky_final)\n",
        "          num_labels = np.array(y_test).shape[1]\n",
        "\n",
        "\n",
        "          d = 1\n",
        "          # for layer in final_concat_layer:\n",
        "          #     print(f\"concat_layers{d} : layer = {layer}, output_shape = {layer.shape}, layer_name = {layer.name}\")\n",
        "          #     d = d + 1\n",
        "          # print(\"\\n\")\n",
        "\n",
        "          output = Dense(num_labels, activation='softmax')(final_concat_layer) # NB LEGG MERKE TIL AT DENNE ER ENDRET FRA FC2\n",
        "      else :\n",
        "        final_concat_layer = tf.keras.layers.Concatenate(name=\"concat_scalars_all\")(concat_layers + [pre_dense_layer] )\n",
        "\n",
        "\n",
        "        if final_concat_units is None:\n",
        "          final_concat_units = [128, 64, 16]\n",
        "        for i in final_concat_units:\n",
        "          dense_final = Dense(i)(final_concat_layer)\n",
        "          bn_final = BatchNormalization()(dense_final)\n",
        "          leaky_final = LeakyReLU(alpha=alpha)(bn_final)\n",
        "          final_concat_layer = Dropout(dropout)(leaky_final)\n",
        "\n",
        "          num_labels = np.array(y_test).shape[1]\n",
        "\n",
        "\n",
        "          d = 1\n",
        "          # for layer in final_concat_layer:\n",
        "          #     print(f\"concat_layers{d} : layer = {layer}, output_shape = {layer.shape}, layer_name = {layer.name}\")\n",
        "          #     d = d + 1\n",
        "          # print(\"\\n\")\n",
        "          output = Dense(num_labels, activation='softmax')(final_concat_layer) # NB LEGG MERKE TIL AT DENNE ER ENDRET FRA FC2\n",
        "\n",
        "\n",
        "\n",
        "      # for kun å evaluere en hvis type IPs\n",
        "      #inputs, concat_layers, x_input_test, x_input_train = [], [], [], []\n",
        "      #x_input_train_ev = [pion_candidates_train_tf, kaon_candidates_train_tf, proton_candidates_train_tf, X_train[\"X_train_refractive_index\"],X_train[\"X_train_momentum\"], X_train[\"X_train_mip_position\"]]\n",
        "      #x_input_test_ev = [pion_candidates_test_tf, kaon_candidates_test_tf, proton_candidates_test_tf, X_test[\"X_test_refractive_index\"],X_test[\"X_test_momentum\"], X_test[\"X_test_mip_position\"]]\n",
        "\n",
        "\n",
        "      # senere : for å kun ta med en hvis type parametere :\n",
        "      # for index in range(len(inputs_ev)):\n",
        "      #   if mask[index] == 1:\n",
        "      #     inputs.append(inputs_ev[index])\n",
        "      #     concat_layers.append(concat_layers_ev[index])\n",
        "      #     x_input_train.append(x_input_train_ev[index])\n",
        "      #     x_input_test.append(x_input_test_ev[index])\n",
        "\n",
        "      print(\"Concatenating layers by : concat = concatenate(concat_layers)\")\n",
        "      concat = concatenate(concat_layers)\n",
        "\n",
        "\n",
        "\n",
        "      ## number of op typs : other pion kaon proton\n",
        "      # changed to 3 outputs -- ie no noncandidate\n",
        "      print(f\"y_test.shape : {np.array(y_test).shape}\")\n",
        "\n",
        "\n",
        "\n",
        "      #x_input_train = [pion_candidates_train_tf, kaon_candidates_train_tf, proton_candidates_train_tf]\n",
        "      #x_input_test = [pion_candidates_test_tf, kaon_candidates_test_tf, proton_candidates_test_tf]\n",
        "\n",
        "      x_input_train = train_variables\n",
        "      x_input_test = test_variables\n",
        "      # for i, array in enumerate(x_input_train):\n",
        "      #     if np.isnan(array).any():\n",
        "      #         print(f\"Array at index {i} contains NaN values.\")\n",
        "      #     else:\n",
        "      #         print(f\"Array at index {i} is valid.\")\n",
        "      # for i, array in enumerate(x_input_train):\n",
        "      #     if np.any(np.isnan(array)):\n",
        "      #         print(f\"Array at index {i} contains NaN values.\")\n",
        "      #     else:\n",
        "      #         print(f\"Array at index {i} does not contain NaN values.\")\n",
        "      # d = 1\n",
        "      # for input_layer in inputs:\n",
        "      #     print(f\"input{d} = {input_layer}, input_shape = {input_layer.shape}, input_name = {input_layer.name}\")\n",
        "      #     d = d + 1\n",
        "\n",
        "      # d = 1\n",
        "      # for layer in concat_layers:\n",
        "      #     print(f\"concat_layers{d} : layer = {layer}, output_shape = {layer.shape}, layer_name = {layer.name}\")\n",
        "      #     d = d + 1\n",
        "\n",
        "\n",
        "      # for i, input in enumerate(inputs):\n",
        "      #   print(f\"input{i} = {input} || inputname ={input.name}\")\n",
        "      #   print(f\"output{i} = {concat_layers[i]} | output_shape = {concat_layers[i].name}\")\n",
        "      #   try:\n",
        "      #     print(f\"x_input_train_shape{i} = {np.asarray(np.asarray(x_input_train[i]), dtype=object).shape}\")\n",
        "      #   except Exception as e:\n",
        "      #     print(f\"x_input_train_shape failed w {e}\")\n",
        "\n",
        "      model = Model(inputs = inputs, outputs=output)\n",
        "\n",
        "\n",
        "\n",
        "      y_train_with_flags = np.concatenate([y_train, X_train[\"X_train_pion_flag\"], X_train[\"X_train_kaon_flag\"], X_train[\"X_train_proton_flag\"]], axis=1)\n",
        "\n",
        "      y_test_with_flags = np.concatenate([y_test, X_test[\"X_test_pion_flag\"], X_test[\"X_test_kaon_flag\"], X_test[\"X_test_proton_flag\"]], axis=1)\n",
        "\n",
        "      #model.compile(optimizer='adam', loss=custom_loss, metrics=['accuracy'])\n",
        "      model.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "      history = model.fit(\n",
        "          x=train_variables,\n",
        "          y=y_train,\n",
        "          validation_data=(\n",
        "              test_variables,\n",
        "              y_test\n",
        "          ),\n",
        "          batch_size=64,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          callbacks=[lr, tensorboard_callback]\n",
        "      )\n",
        "\n",
        "\n",
        "      #   # plotting the worst cases?:\n",
        "      #   #y_pred_test = model.predict([X_test[\"X_test_map\"], X_test[\"X_test_momentum\"], X_test[\"X_test_refractive_index\"], X_test[\"X_test_mip_position\"]])\n",
        "      #   #plot_worst(model, y_test, X_test_map, X_test_momentum, X_test_refractive_index, X_test_ckov, X_test_mip_position, y_pred):\n",
        "      #   print(\"Shape of y_test: \", y_test.shape)\n",
        "      #   #print(\"Shape of X_test_map: \", X_test[\"X_test_map\"].shape)\n",
        "      # # print(\"Shape of X_test_windows: \", X_test[\"X_test_windows\"].shape)\n",
        "      #   print(\"Shape of X_test_momentum: \", X_test[\"X_test_momentum\"].shape)\n",
        "      #   print(\"Shape of X_test_refractive_index: \", X_test[\"X_test_refractive_index\"].shape)\n",
        "      #   #print(\"Shape of X_test_ckov: \", X_test[\"X_test_ckov\"].shape)\n",
        "      #   print(\"Shape of X_test_mip_position: \", X_test[\"X_test_mip_position\"].shape)\n",
        "      #   #print(\"Shape of y_pred_test: \", y_pred_test.shape)\n",
        "      #   # try: # NB! commented out this line\n",
        "      #   #   #plot_worst_(model, y_test, X_test[\"X_test_map\"], X_test[\"X_test_momentum\"], X_test[\"X_test_refractive_index\"], X_test[\"X_test_ckov\"], X_test[\"X_test_mip_position\"], y_pred_test)\n",
        "      #   #   #plot_worst_(model, y_test, X_test[\"X_test_map\"], X_test[\"X_test_momentum\"], X_test[\"X_test_refractive_index\"], X_test[\"X_test_ckov\"], X_test[\"X_test_mip_position\"], self.resolution)\n",
        "      #   # except Exception as e:\n",
        "      #   #   print(f\"skip plot_worst_ due to error: {e}\")\n",
        "\n",
        "      # Evaluate the model\n",
        "\n",
        "      # x_ip_test = [pion_candidates_test_tf, kaon_candidates_test_tf, proton_candidates_test_tf,\n",
        "      #         X_test[\"X_test_refractive_index\"], X_test[\"X_test_momentum\"], X_test[\"X_test_mip_position\"]]\n",
        "      x_ip_test = x_input_test\n",
        "      _, accuracy = model.evaluate( x= x_ip_test,y=y_test,verbose=1 )\n",
        "\n",
        "\n",
        "\n",
        "      # Check if the current model configuration is better\n",
        "      if accuracy > best_accuracy:\n",
        "          best_accuracy = accuracy\n",
        "          best_model = model\n",
        "\n",
        "      # Set the best model as the final model\n",
        "      self.model = best_model\n",
        "      return model, history\n",
        "\n",
        "\n",
        "  def train_model(self, X_train, X_test, y_train, y_test):\n",
        "      # Compile the model\n",
        "\n",
        "      X_train_pion_candidates = X_train[\"X_train_pion_candidates\"]\n",
        "      X_train_kaon_candidates = X_train[\"X_train_kaon_candidates\"]\n",
        "      X_train_proton_candidates = X_train[\"X_train_proton_candidates\"]\n",
        "      X_train_momentum = X_train[\"X_train_momentum\"]\n",
        "      X_train_refractive_index = X_train[\"X_train_refractive_index\"]\n",
        "      X_train_phi = X_train[\"X_train_phi\"]\n",
        "      X_train_theta = X_train[\"X_train_theta\"]\n",
        "      X_train_mip_position = X_train[\"X_train_mip_position\"]\n",
        "      X_train_rad_position = X_train[\"X_train_rad_position\"]\n",
        "\n",
        "      X_test_pion_candidates = X_test[\"X_test_pion_candidates\"]\n",
        "      X_test_kaon_candidates = X_test[\"X_test_kaon_candidates\"]\n",
        "      X_test_proton_candidates = X_test[\"X_test_proton_candidates\"]\n",
        "      X_test_momentum = X_test[\"X_test_momentum\"]\n",
        "      X_test_refractive_index = X_test[\"X_test_refractive_index\"]\n",
        "      X_test_phi = X_test[\"X_test_phi\"]\n",
        "      X_test_theta = X_test[\"X_test_theta\"]\n",
        "      X_test_mip_position = X_test[\"X_test_mip_position\"]\n",
        "      X_test_rad_position = X_test[\"X_test_rad_position\"]\n",
        "\n",
        "      print(\"Fields in the first vector of X_train:\")\n",
        "      print(\"X_train_pion_candidates shape:\", X_train_pion_candidates.shape)\n",
        "      print(\"X_train_kaon_candidates shape:\", X_train_kaon_candidates.shape)\n",
        "      print(\"X_train_proton_candidates shape:\", X_train_proton_candidates.shape)\n",
        "      print(\"X_train_momentum shape:\", X_train_momentum.shape)\n",
        "      print(\"X_train_refractive_index shape:\", X_train_refractive_index.shape)\n",
        "      print(\"X_train_phi shape:\", X_train_phi.shape)\n",
        "      print(\"X_train_theta shape:\", X_train_theta.shape)\n",
        "      print(\"X_train_mip_position shape:\", X_train_mip_position.shape)\n",
        "      print(\"X_train_rad_position shape:\", X_train_rad_position.shape)\n",
        "\n",
        "      print(\"\\nFields in the first vector of X_test:\")\n",
        "      print(\"X_test_pion_candidates shape:\", X_test_pion_candidates.shape)\n",
        "      print(\"X_test_kaon_candidates shape:\", X_test_kaon_candidates.shape)\n",
        "      print(\"X_test_proton_candidates shape:\", X_test_proton_candidates.shape)\n",
        "      print(\"X_test_momentum shape:\", X_test_momentum.shape)\n",
        "      print(\"X_test_refractive_index shape:\", X_test_refractive_index.shape)\n",
        "      print(\"X_test_phi shape:\", X_test_phi.shape)\n",
        "      print(\"X_test_theta shape:\", X_test_theta.shape)\n",
        "      print(\"X_test_mip_position shape:\", X_test_mip_position.shape)\n",
        "      print(\"X_test_rad_position shape:\", X_test_rad_position.shape)\n",
        "\n",
        "      self.model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "\n",
        "      # Print the first element of y_train\n",
        "      print(\"First element of y_train:\", y_train[0])\n",
        "\n",
        "      # Train the model\n",
        "      history = self.model.fit( x=[X_train], y=y_train,validation_data=( [X_test],y_test), batch_size=16,epochs=10,verbose=1)\n",
        "      return history\n",
        "\n",
        "  def evaluate_model(self, X_test, y_test):\n",
        "\n",
        "\n",
        "      X_test_pion_candidates = X_test[\"X_test_pion_candidates\"]\n",
        "      X_test_kaon_candidates = X_test[\"X_test_kaon_candidates\"]\n",
        "      X_test_proton_candidates = X_test[\"X_test_proton_candidates\"]\n",
        "      X_test_momentum = X_test[\"X_test_momentum\"]\n",
        "      X_test_refractive_index = X_test[\"X_test_refractive_index\"]\n",
        "      X_test_phi = X_test[\"X_test_phi\"]\n",
        "      X_test_theta = X_test[\"X_test_theta\"]\n",
        "      X_test_mip_position = X_test[\"X_test_mip_position\"]\n",
        "      X_test_rad_position = X_test[\"X_test_rad_position\"]\n",
        "      loss, accuracy = self.model.evaluate(x=X_test, y=y_test,verbose=0)\n",
        "      print(f\"Test Loss: {loss:.4f}\")\n",
        "      print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "\n",
        "  def train(self, X_train, X_test, y_train, y_test, mask, num_epochs, include_cnn = False, units = None, units2 = None, final_concat_units = None, CNN_units = None):\n",
        "\n",
        "      #X_ckov_segm = X_train[\"X_train_photon_ckov_segmented\"]\n",
        "      #print(f\"X_train_photon_ckov_segmented shape = {np.asarray(X_ckov_segm, dtype = object).shape}\")\n",
        "      X_train_photon_ckov_segmented = 0# np.asarray(X_train[\"X_train_photon_ckov_segmented\"])\n",
        "\n",
        "\n",
        "\n",
        "      #print(f\" in  def train(self, filename) : X_train_photon_ckov_segmented shape : {X_train_photon_ckov_segmented.shape}\")\n",
        "      #X_train_dist2mip = X_train_dist2mip.reshape(X_train_dist2mip.shape[0], X_train_dist2mip.shape[1],, 1)\n",
        "      #for dist in X_train_dist2mip:\n",
        "      #  print(f\"dist = {dist}\")\n",
        "\n",
        "\n",
        "      input_sequence_length = 0# len(max(X_train_photon_ckov_segmented, key=len))\n",
        "\n",
        "      model, history = self.build_model(input_sequence_length = input_sequence_length, X_train = X_train, X_test = X_test, y_train = y_train, y_test = y_test, mask = mask, epochs = num_epochs , include_cnn = include_cnn, units = units, units2 = units2, CNN_units = CNN_units)\n",
        "\n",
        "\n",
        "\n",
        "      try:\n",
        "        history = self.train_model(X_train, X_test, y_train, y_test)\n",
        "        self.evaluate_model(X_test, y_test)\n",
        "      except Exception as e:\n",
        "        print(\"    def train(self, filename) failed at history = self.train_model bc of error : {e} \")\n",
        "\n",
        "      return model, history\n",
        "      #plot_training_history(self, history, vector_of_weights, vector_of_weights2, dropout):\n",
        "      #self.plot_training_history(history)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PptqTnJ_2iyl"
      },
      "source": [
        "# Usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dP4hLxCiN9SP"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Usage example\n",
        "\n",
        "\n",
        "#these three have .008 in stddev\n",
        "# 3 percent occupancy:\n",
        "print(\"classifier = MassClassifier(percentage_to_read = 100, resolution = 4) # pass percentage of dataset to read\")\n",
        "classifier = MassClassifier(percentage_to_read = 100, resolution = 4) # pass percentage of dataset to read\n",
        "\n",
        "# L whole range; 4 std\n",
        "#file_to_read = \"test/ParticleInfoProton.h5\" , Prtoon\n",
        "#file_to_read = \"test/ParticleInfo20k.h5\"\n",
        "\n",
        "# Proton, 3 std, L 0.1 1.4\n",
        "#file_to_read = \"test/diffLandSegm/ParticleInfo.h5\" proton proton\n",
        "\n",
        "#file_to_read = \"test/diffLandSegm/ParticleInfoPb.h5\" # PbPB 2STD 2k\n",
        "file_to_read = \"test/diffLandSegm/ParticleInfoPb20k.h5\" # PbPB 2STD, 20k particles\n",
        "\n",
        "\n",
        "#file_to_read = \"ParticleInfo2.h5\" # PbPB 2STD, 20k particles\n",
        "#print(f\"classifier.load_data{file_to_read}\")\n",
        "\n",
        "## 1-5 range :\n",
        "#file_to_read = \"ParticleInfo211_2.h5\" # PbPB 2STD, 20k particles\n",
        "#classifier.load_data(file_to_read)\n",
        "#file_to_read = \"ParticleInfo321_2.h5\" # PbPB 2STD, 20k particles\n",
        "#classifier.load_data(file_to_read)\n",
        "#file_to_read = \"ParticleInfo22212_2.h5\" # PbPB 2STD, 20k particles\n",
        "#classifier.load_data(file_to_read)\n",
        "#file_to_read = \"ParticleInfo211.h5\" # PbPB 2STD, 20k particles\n",
        "#classifier.load_data(file_to_read)\n",
        "#file_to_read = \"ParticleInfo321.h5\" # PbPB 2STD, 20k particles\n",
        "#classifier.load_data(file_to_read)\n",
        "#file_to_read = \"ParticleInfo2212.h5\" # PbPB 2STD, 20k particles\n",
        "#classifier.load_data(file_to_read)\n",
        "\n",
        "#files_to_read = [\"ParticleInfo211_prange32_2.h5\", ... other files having \"prang\"]\n",
        "\n",
        "import glob\n",
        "drive_path = '/content/drive/MyDrive/Colab Notebooks/CERN_ML/CNN_PID/'\n",
        "\n",
        "#files_to_read2 = glob.glob(drive_path + \"*prang*.h5\")\n",
        "#files_to_read3 = glob.glob(drive_path + \"*Open*.h5\")\n",
        "\n",
        "\n",
        "#files_to_read = files_to_read1 + files_to_read2\n",
        "\n",
        "\n",
        "\n",
        "# files_to_read = glob.glob(drive_path + \"High/\"+\"*Highcon*.h5\")\n",
        "# files_to_read = files_to_read + glob.glob(drive_path  +  \"High/\"+\"cp/\" + \"*Highcon*.h5\")\n",
        "# files_to_read = files_to_read + glob.glob(drive_path  +  \"High/\"+\"cp2/\" + \"*.h5\")\n",
        "# files_to_read = files_to_read + glob.glob(drive_path  +  \"High/\"+\"cp3/\" + \"*.h5\")\n",
        "# files_to_read = glob.glob(drive_path  +  \"Study/\" + \"*h5\")\n",
        "# files_to_read = glob.glob(drive_path  +  \"NewSegment/\" + \"*h5\")\n",
        "#files_to_read = glob.glob(drive_path  +  \"ref/\" + \"*Particle211_ref.h5\")\n",
        "\n",
        "\n",
        "# files_to_read = glob.glob(drive_path  +  \"data2.h5\")\n",
        "\n",
        "# files_to_read = glob.glob(drive_path  +  \"fixedMipCharge/\" + \"*.h5\")\n",
        "files_to_read = glob.glob(drive_path  +  \"fixedMipCharge/lowocc/\" + \"*.h5\")\n",
        "# files_to_read = glob.glob(drive_path  +  \"fixedMipCharge/refStudy/\" + \"*.h5\")\n",
        "\n",
        "\n",
        "\n",
        "#files_to_read = glob.glob(drive_path + \"Lead/\" + \"*.h5\")\n",
        "\n",
        "#files_to_read = glob.glob(drive_path + \"*Open*.h5\")\n",
        "\n",
        "#X_train = X_test = y_train = y_test = classifier.preprocess_data()\n",
        "classifier.load_data(files_to_read)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03T63KCg4atu"
      },
      "outputs": [],
      "source": [
        "\n",
        "mask = [1,1,1,1]\n",
        "X_train_scaled, X_test_scaled, X_train, X_test, y_train, y_test = classifier.preprocess_data()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRrcGqJzL0yO"
      },
      "source": [
        "# Plotting histograms of Training Versus Test Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGcPI9SNAYC4"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/eflatlan/CNN_PID/models_sacved/misc_helper_functions.py\n",
        "from misc_helper_functions import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXfgrBxNi0Gb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TS8iVdaej4S-"
      },
      "outputs": [],
      "source": [
        "# print(\"===== Evaluating Training Data =====\")\n",
        "# for feature, data in X_train.items():\n",
        "#     print(f\"Evaluating feature: {feature}\")\n",
        "#     eval_data(np.array(data))  # Assuming your function is designed to work with numpy arrays\n",
        "#     print(\"---------------------------\")\n",
        "\n",
        "# # Loop through test_data dictionary\n",
        "# print(\"===== Evaluating Test Data =====\")\n",
        "# for feature, data in X_test.items():\n",
        "#     print(f\"Evaluating feature: {feature}\")\n",
        "#     eval_data(np.array(data))  # Assuming your function is designed to work with numpy arrays\n",
        "#     print(\"---------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yodihp5E2k9X"
      },
      "outputs": [],
      "source": [
        "# plot_hist(X_train=X_train, X_test=X_test, description = \"Unscaled\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Hue3RFcmfSu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qw-tBHZEp_ph"
      },
      "source": [
        "# Plot samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5rA7r6VJzF9"
      },
      "outputs": [],
      "source": [
        "\n",
        "X_train_pion_candidates = X_train[\"X_train_pion_candidates\"]\n",
        "X_train_kaon_candidates = X_train[\"X_train_kaon_candidates\"]\n",
        "X_train_proton_candidates = X_train[\"X_train_proton_candidates\"]\n",
        "X_train_non_candidates = X_train[\"X_train_non_candidates\"]\n",
        "X_train_non_candidates = X_train[\"X_train_non_candidates\"]\n",
        "\n",
        "X_train_momentum = X_train[\"X_train_momentum\"]\n",
        "X_train_refractive_index = X_train[\"X_train_refractive_index\"]\n",
        "X_train_phi = X_train[\"X_train_phi\"]\n",
        "X_train_theta = X_train[\"X_train_theta\"]\n",
        "\n",
        "X_train_mip_position = X_train[\"X_train_mip_position\"]\n",
        "X_train_rad_position = X_train[\"X_train_rad_position\"]\n",
        "\n",
        "\n",
        "X_test_non_candidates = X_test[\"X_test_non_candidates\"]\n",
        "\n",
        "X_test_pion_candidates = X_test[\"X_test_pion_candidates\"]\n",
        "X_test_kaon_candidates = X_test[\"X_test_kaon_candidates\"]\n",
        "X_test_proton_candidates = X_test[\"X_test_proton_candidates\"]\n",
        "X_test_momentum = X_test[\"X_test_momentum\"]\n",
        "X_test_refractive_index = X_test[\"X_test_refractive_index\"]\n",
        "X_test_phi = X_test[\"X_test_phi\"]\n",
        "X_test_theta = X_test[\"X_test_theta\"]\n",
        "X_test_mip_position = X_test[\"X_test_mip_position\"]\n",
        "X_test_rad_position = X_test[\"X_test_rad_position\"]\n",
        "\n",
        "below_zero_indexes = np.where(X_train_phi < 0)[0]\n",
        "n = 10\n",
        "first_n_below_zero_indexes = below_zero_indexes[:n]\n",
        "\n",
        "print(first_n_below_zero_indexes)\n",
        "#ef : TODO fix this again\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEcxmpVi2mxO"
      },
      "source": [
        "# PDG of train vs test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DlGQeDT2l1S"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# for i in range(1,5):\n",
        "#   print(y_test[i])\n",
        "\n",
        "\n",
        "\n",
        "# # Counting occurrences in the training set\n",
        "\n",
        "\n",
        "\n",
        "print(f\"shape : y_test {y_test.shape}\")\n",
        "print(f\"shape : y_train {y_train.shape}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_pion_count = np.sum(y_train[:, 0])\n",
        "train_kaon_count = np.sum(y_train[:, 1])\n",
        "train_proton_count = np.sum(y_train[:, 2])\n",
        "\n",
        "# # Counting occurrences in the test set\n",
        "test_pion_count = np.sum(y_test[:, 0])\n",
        "test_kaon_count = np.sum(y_test[:, 0])\n",
        "test_proton_count = np.sum(y_test[:, 2])\n",
        "\n",
        "# print(f'Train : pion_count {train_pion_count}  kaon_count {train_kaon_count} proton_count {train_proton_count}')\n",
        "# print(f'Test : pion_count {test_pion_count}  kaon_count {test_kaon_count} proton_count {test_proton_count}')\n",
        "\n",
        "# # Create subplots\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "# # Training set histogram\n",
        "axs[0].bar(['Pion', 'Kaon', 'Proton'], [train_pion_count, train_kaon_count, train_proton_count])\n",
        "axs[0].set_title('Training Set')\n",
        "axs[0].set_xlabel('Particle Type')\n",
        "axs[0].set_ylabel('Frequency')\n",
        "\n",
        "# # Test set histogram\n",
        "axs[1].bar(['Pion', 'Kaon', 'Proton'], [test_pion_count, test_kaon_count, test_proton_count])\n",
        "axs[1].set_title('Test Set')\n",
        "axs[1].set_xlabel('Particle Type')\n",
        "axs[1].set_ylabel('Frequency')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqd9OiuKd8AF"
      },
      "outputs": [],
      "source": [
        "#plot_first_instance(1, X_train_pion_candidates, X_train_kaon_candidates, X_train_proton_candidates,\\\n",
        "#                    X_train_non_candidates,X_train_mCluSize, X_train_mip_position, X_train_rad_position, X_train_phi, X_train_theta, y_train)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awUAR6mnhmSM"
      },
      "outputs": [],
      "source": [
        "#plot_combined_types(1, X_train_pion_candidates, X_train_kaon_candidates, X_train_proton_candidates, X_train_non_candidates)\n",
        "# plot_individual_types(5, X_train_pion_candidates, X_train_kaon_candidates, X_train_proton_candidates, X_train_non_candidates)\n",
        "\n",
        "X_train_index_particle = X_train[\"X_train_index_particle\"]\n",
        "X_test_index_particle = X_test[\"X_test_index_particle\"]\n",
        "\n",
        "index = 2\n",
        "X_train_mCluCharge = X_train[\"X_train_mCluCharge\"]\n",
        "X_train_mCluSize = X_train[\"X_train_mCluSize\"]\n",
        "X_test_mCluCharge = X_test[\"X_test_mCluCharge\"]\n",
        "X_test_mCluSize = X_test[\"X_test_mCluSize\"]\n",
        "\n",
        "#print(f\"X_test_mCluCharge shape {X_test_mCluCharge[1]}\")\n",
        "\n",
        "#print(f\"X_test_rad_position  {X_test_rad_position[0,:,:]}\")\n",
        "\n",
        "#print(f\"X_test_mip_position  {X_test_mip_position[0,:,:]}\")\n",
        "\n",
        "#def plot_individual_types(idx, x_pion, x_kaon, x_proton, x_non, MIP_charge, MIP_position, RAD_position, y_train, log_scale):\n",
        "\n",
        "\n",
        "plot_individual_types(idx=index, x_pion=X_test_pion_candidates, x_kaon=X_test_kaon_candidates, x_proton=X_test_proton_candidates, x_non=X_test_non_candidates, MIP_charge=X_test_mCluCharge, MIP_position=X_test_mip_position, RAD_position=X_test_rad_position, phi=X_test_phi, y_train=y_test, log_scale=True, X_index=X_test_index_particle)\n",
        "\n",
        "\n",
        "#def plot_combined_types(i, x_pion, x_kaon, x_proton, x_non, MIP_position, RAD_position, phiP, thetaP, y_train):\n",
        "plot_combined_types(index, X_test_pion_candidates, X_test_kaon_candidates, X_test_proton_candidates, X_test_non_candidates,X_test_mip_position, X_test_rad_position, X_test_phi, X_test_theta, y_train = y_test, X_index = X_test_index_particle)\n",
        "\n",
        "\n",
        "#plot_combined_types3(index, X_test_pion_candidates, X_test_kaon_candidates, X_test_proton_candidates, X_test_non_candidates,X_test_mip_position, X_test_rad_position, X_test_phi, X_test_theta, y_test)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82LnNwwvL7Ys"
      },
      "source": [
        "# Training the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ti16RD0oVawr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZARs3NKt4ToR"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "!wget -O ParticleDataUtilsCp.py https://raw.githubusercontent.com/eflatlan/CNN_PID/models_sacved/ParticleDataUtils21.py\n",
        "from ParticleDataUtilsCp import ParticleDataUtils, classify_candidates_with_pad_sequences\n",
        "\n",
        "!wget -O misc_helper_functions.py https://raw.githubusercontent.com/eflatlan/CNN_PID/models_sacved/misc_helper_functions2.py\n",
        "from misc_helper_functions import build_species_layers, calculate_theta, filter_data, create_lr_scheduler,plot_lr, extract_neighborhood_map, create_cnn_model # plot_worst\n",
        "\n",
        "!wget -O helper_functions.py https://raw.githubusercontent.com/eflatlan/CNN_PID/models_sacved/helper_functions.py\n",
        "!wget -O plot_helper_functions.py https://raw.githubusercontent.com/eflatlan/CNN_PID/models_sacved/plot_helper_functions22.py\n",
        "\n",
        "\n",
        "\n",
        "from plot_helper_functions import plot_hist, plot_training_history, plot_confusion_matrix\n",
        "\n",
        "\n",
        "print(classify_candidates_with_pad_sequences)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot_helper_functions.plot_hist(X_train=X_train, X_test=X_test, description = \"Unscaled\")\n"
      ],
      "metadata": {
        "id": "q1vhz4I-Sgmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4MLTLJX2mA1"
      },
      "outputs": [],
      "source": [
        "mask = [1,1,1,1]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "fc1_unit = 64\n",
        "units = [fc1_unit * i for i in [4, 2, 2, 1]]\n",
        "units2 = [fc1_unit * i for i in [4, 2, 2, 1]]\n",
        "\n",
        "\n",
        "final_concat_units = [16 * i for i in [4, 1]]\n",
        "\n",
        "# units = [fc1_unit * i for i in [16, 8, 4, 2, 2, 1]]\n",
        "# units2 = [fc1_unit * i for i in [16, 8, 4, 2, 2, 1]]\n",
        "\n",
        "CNN_units =  [32, 8, 4, 2, 2, 1]\n",
        "n_epochs = 40\n",
        "model, history = classifier.train(X_train = X_train, X_test = X_test, y_train = y_train, y_test = y_test, mask = mask, num_epochs=n_epochs, include_cnn = False, units = units, units2 = units2, final_concat_units = final_concat_units, CNN_units = CNN_units)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "#\n",
        "\n",
        "# weight_list = [layer.get_weights()[0] for layer in model.layers if len(layer.get_weights()) > 0]\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# for i, w in enumerate(weight_list):\n",
        "#     plt.hist(w.flatten(), bins=50)\n",
        "#     plt.title(f\"Layer {i} Weight Distribution\")\n",
        "#     plt.show()\n",
        "\n",
        "# import seaborn as sns\n",
        "\n",
        "# for i, w in enumerate(weights):\n",
        "#     if len(w.shape) == 2:  # Ensure it's a 2D matrix\n",
        "#         plt.figure(figsize=(10, 10))\n",
        "#         sns.heatmap(w, cmap='viridis')\n",
        "#         plt.title(f\"Layer {i} Weight Heatmap\")\n",
        "#         plt.show()\n",
        "\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "\n",
        "# for index, weights in enumerate(weight_list):\n",
        "#     plt.figure(figsize=(10, 5))\n",
        "\n",
        "#     # You can take the absolute value if you're interested in magnitude\n",
        "#     # Otherwise, just remove np.abs\n",
        "#     plt.imshow(np.abs(weights), cmap='hot', interpolation='nearest')\n",
        "\n",
        "#     plt.colorbar()\n",
        "#     plt.title(f'Layer {index} Weights Intensity')\n",
        "#     plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir logs/fit"
      ],
      "metadata": {
        "id": "Gyoj0ofB6UFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_pion_candidates = X_train[\"X_train_pion_candidates\"]\n",
        "X_train_kaon_candidates = X_train[\"X_train_kaon_candidates\"]\n",
        "X_train_proton_candidates = X_train[\"X_train_proton_candidates\"]\n",
        "\n",
        "X_train_non_candidates = X_train[\"X_train_non_candidates\"]\n",
        "X_train_momentum = X_train[\"X_train_momentum\"]\n",
        "X_train_refractive_index = X_train[\"X_train_refractive_index\"]\n",
        "X_train_phi = X_train[\"X_train_phi\"]\n",
        "X_train_theta = X_train[\"X_train_theta\"]\n",
        "X_train_mip_position = X_train[\"X_train_mip_position\"]\n",
        "X_train_rad_position = X_train[\"X_train_rad_position\"]\n",
        "X_train_map_pion = X_train[\"X_train_map_pion\"]\n",
        "X_train_map_kaon = X_train[\"X_train_map_kaon\"]\n",
        "X_train_map_proton = X_train[\"X_train_map_proton\"]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_test_pion_candidates = X_test[\"X_test_pion_candidates\"]\n",
        "X_test_kaon_candidates = X_test[\"X_test_kaon_candidates\"]\n",
        "X_test_proton_candidates = X_test[\"X_test_proton_candidates\"]\n",
        "X_test_non_candidates = X_test[\"X_test_non_candidates\"]\n",
        "X_test_momentum = X_test[\"X_test_momentum\"]\n",
        "X_test_refractive_index = X_test[\"X_test_refractive_index\"]\n",
        "X_test_phi = X_test[\"X_test_phi\"]\n",
        "X_test_theta = X_test[\"X_test_theta\"]\n",
        "X_test_mip_position = X_test[\"X_test_mip_position\"]\n",
        "X_test_rad_position = X_test[\"X_test_rad_position\"]\n",
        "X_test_map_pion = X_test[\"X_test_map_pion\"]\n",
        "X_test_map_kaon = X_test[\"X_test_map_kaon\"]\n",
        "X_test_map_proton = X_test[\"X_test_map_proton\"]\n",
        "\n",
        "X_train_mCluCharge = X_train[\"X_train_mCluCharge\"]\n",
        "X_train_mCluSize = X_train[\"X_train_mCluSize\"]\n",
        "X_test_mCluCharge = X_test[\"X_test_mCluCharge\"]\n",
        "X_test_mCluSize = X_test[\"X_test_mCluSize\"]"
      ],
      "metadata": {
        "id": "wYQgGnIEddR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import precision_recall_curve, confusion_matrix, f1_score\n",
        "\n",
        "def plot_confusion_matrix(ax, cm, title=\"Confusion Matrix\"):\n",
        "    \"\"\"Utility function to plot the confusion matrix.\"\"\"\n",
        "    ax.imshow(cm, cmap='Blues', interpolation='nearest')\n",
        "    ax.set_xticks(np.arange(3))\n",
        "    ax.set_yticks(np.arange(3))\n",
        "    ax.set_xticklabels(['Pion', 'Kaon', 'Proton'])\n",
        "    ax.set_yticklabels(['Pion', 'Kaon', 'Proton'])\n",
        "    ax.set_title(title)\n",
        "    for x in range(3):\n",
        "        for y in range(3):\n",
        "            percent = cm[x, y] / np.sum(cm[x, :]) * 100\n",
        "            ax.text(y, x, f\"{cm[x, y]} ({percent:.1f}%)\", ha='center', va='center', color='red')\n",
        "\n",
        "def plot_training_history(history, y_pred_train, y_pred_test, y_train_true, y_test_true):\n",
        "    fig2, axs2 = plt.subplots(1, 2, figsize=(25, 6))\n",
        "    cm_train = confusion_matrix(y_train_true.argmax(axis=1), y_pred_train.argmax(axis=1))\n",
        "    cm_test = confusion_matrix(y_test_true.argmax(axis=1), y_pred_test.argmax(axis=1))\n",
        "    plot_confusion_matrix(axs2[0], cm_train, title=\"Train Confusion Matrix\")\n",
        "    plot_confusion_matrix(axs2[1], cm_test, title=\"Validation Confusion Matrix\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(16, 18))\n",
        "    axs[0].plot(history.history[\"loss\"], label=\"Train Loss\")\n",
        "    axs[0].plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
        "    axs[0].set_xlabel(\"Epochs\")\n",
        "    axs[0].set_ylabel(\"Loss\")\n",
        "    axs[0].legend()\n",
        "\n",
        "    axs[1].plot(history.history[\"accuracy\"], label=\"Train Accuracy\")\n",
        "    axs[1].plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
        "    axs[1].set_xlabel(\"Epochs\")\n",
        "    axs[1].set_ylabel(\"Accuracy\")\n",
        "    axs[1].legend()\n",
        "\n",
        "    class_labels = ['Pion', 'Kaon', 'Proton']\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(36, 12))\n",
        "\n",
        "    # Train P-R Curve and F1 Score\n",
        "    y_train_bin = label_binarize(y_train_true, classes=[0, 1, 2])\n",
        "    for i in range(3):\n",
        "        precision_train, recall_train, _ = precision_recall_curve(y_train_bin[:, i], y_pred_train[:, i])\n",
        "        axs[0].plot(recall_train, precision_train, lw=2, label=f\"Train {class_labels[i]}\", linestyle='--', color=f'C{i}')\n",
        "        f1_train = 2 * (precision_train * recall_train) / (precision_train + recall_train)\n",
        "        axs[2].plot(recall_train, f1_train, lw=2, label=f\"Train {class_labels[i]}\", linestyle='--', color=f'C{i}')\n",
        "\n",
        "    # Validation P-R Curve and F1 Score\n",
        "    y_test_bin = label_binarize(y_test_true, classes=[0, 1, 2])\n",
        "    for i in range(3):\n",
        "        precision_test, recall_test, _ = precision_recall_curve(y_test_bin[:, i], y_pred_test[:, i])\n",
        "        axs[1].plot(recall_test, precision_test, lw=2, label=f\"Validation {class_labels[i]}\", color=f'C{i}')\n",
        "        f1_test = 2 * (precision_test * recall_test) / (precision_test + recall_test)\n",
        "        axs[2].plot(recall_test, f1_test, lw=2, label=f\"Validation {class_labels[i]}\", color=f'C{i}')\n",
        "\n",
        "    axs[0].set_title(\"P-R Curve for Training Data\")\n",
        "    axs[1].set_title(\"P-R Curve for Validation Data\")\n",
        "    axs[2].set_title(\"F1 Score Curve\")\n",
        "    axs[0].legend()\n",
        "    axs[1].legend()\n",
        "    axs[2].legend()\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "aQ06PNqd9K2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "train_variables = [X_train_pion_candidates, X_train_kaon_candidates, X_train_proton_candidates, X_train_non_candidates, X_train_momentum, X_train_refractive_index, X_train_phi, X_train_theta, X_train_mip_position, X_train_rad_position, X_train_mCluCharge, X_train_mCluSize]\n",
        "test_variables = [X_test_pion_candidates, X_test_kaon_candidates, X_test_proton_candidates, X_test_non_candidates, X_test_momentum, X_test_refractive_index, X_test_phi, X_test_theta, X_test_mip_position, X_test_rad_position, X_test_mCluCharge, X_test_mCluSize]\n",
        "\n",
        "y_pred_train = model.predict(train_variables)\n",
        "y_pred_test = model.predict(test_variables)\n",
        "# First function call with 'self'\n",
        "try:\n",
        "    plot_training_history(history=history, y_pred_train=y_pred_train, y_pred_test=y_pred_test, y_train_true=y_train, y_test_true=y_test)\n",
        "except Exception as e:\n",
        "    warnings.warn(f\"Exception caught during first function call: {e}\")"
      ],
      "metadata": {
        "id": "IHE-pjOx9uhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import LogNorm\n",
        "\n",
        "def plot_histograms(X_train_pion_candidates, X_train_kaon_candidates, X_train_proton_candidates, y_train):\n",
        "    labels = ['piontrack', 'kaontrack', 'protontrack']\n",
        "    segments = ['pionsegm', 'kaonsegm', 'protonsegm']\n",
        "\n",
        "    # Extract and store data\n",
        "    data_matrix = {}\n",
        "    for track in labels:\n",
        "        data_matrix[track] = {}\n",
        "        for segm in segments:\n",
        "            data_matrix[track][segm] = {'charge': [], 'size': []}\n",
        "\n",
        "    for idx, track in enumerate(labels):\n",
        "        mask = np.all(y_train == np.eye(3)[idx], axis=1)\n",
        "\n",
        "        data_matrix[track]['pionsegm']['charge'].extend(X_train_pion_candidates[mask, :, 3].ravel())\n",
        "        data_matrix[track]['pionsegm']['size'].extend(X_train_pion_candidates[mask, :, 2].ravel())\n",
        "        data_matrix[track]['kaonsegm']['charge'].extend(X_train_kaon_candidates[mask, :, 3].ravel())\n",
        "        data_matrix[track]['kaonsegm']['size'].extend(X_train_kaon_candidates[mask, :, 2].ravel())\n",
        "        data_matrix[track]['protonsegm']['charge'].extend(X_train_proton_candidates[mask, :, 3].ravel())\n",
        "        data_matrix[track]['protonsegm']['size'].extend(X_train_proton_candidates[mask, :, 2].ravel())\n",
        "\n",
        "    # Helper function to plot\n",
        "    def generate_plot(filtered_charge=0, filtered_size=0, plot_type=\"all\"):\n",
        "        fig_hist, ax_hist = plt.subplots(3, 3, figsize=(15, 15))\n",
        "\n",
        "        for i, track_type in enumerate(labels):\n",
        "            for j, segm_type in enumerate(segments):\n",
        "                charges = [c for c in data_matrix[track_type][segm_type]['charge'] if c > filtered_charge]\n",
        "                sizes = [s for s in data_matrix[track_type][segm_type]['size'] if s > filtered_size]\n",
        "\n",
        "                if plot_type == \"charge\":\n",
        "                    ax_hist[i, j].set_yscale('log')\n",
        "                    ax_hist[i, j].hist(charges, bins=125, range=(0, 250), edgecolor='black', alpha=0.6)\n",
        "                    ax_hist[i, j].set_title(f\"{track_type}, {segm_type} - Charge\")\n",
        "\n",
        "                elif plot_type == \"size\":\n",
        "                    ax_hist[i, j].hist(sizes, bins=6, range=(0, 6), edgecolor='black', alpha=0.6)\n",
        "                    ax_hist[i, j].set_yscale('log')\n",
        "                    ax_hist[i, j].set_title(f\"{track_type}, {segm_type} - Size\")\n",
        "\n",
        "                elif plot_type == \"2d\":\n",
        "                    valid_charges = [c for idx, c in enumerate(data_matrix[track_type][segm_type]['charge']) if c > filtered_charge and data_matrix[track_type][segm_type]['size'][idx] > filtered_size]\n",
        "                    valid_sizes = [s for idx, s in enumerate(data_matrix[track_type][segm_type]['size']) if data_matrix[track_type][segm_type]['charge'][idx] > filtered_charge and s > filtered_size]\n",
        "\n",
        "                    hist = ax_hist[i, j].hist2d(valid_charges, valid_sizes, bins=(25, 6), range=[[0, 250], [0, 6]], cmin=1, norm=LogNorm())\n",
        "                    plt.colorbar(hist[3], ax=ax_hist[i, j])\n",
        "                    ax_hist[i, j].set_title(f\"{track_type}, {segm_type} - 2D Charge vs Size\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # Generate the plots\n",
        "    generate_plot(plot_type=\"size\")\n",
        "    generate_plot(plot_type=\"charge\")\n",
        "    generate_plot(plot_type=\"2d\")\n",
        "    generate_plot(filtered_size=1, plot_type=\"size\")\n",
        "    generate_plot(filtered_charge=10, plot_type=\"charge\")\n",
        "    generate_plot(filtered_charge=10, filtered_size=1, plot_type=\"2d\")\n",
        "\n"
      ],
      "metadata": {
        "id": "yVgyzRzN1exI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_histograms(X_train_pion_candidates, X_train_kaon_candidates, X_train_proton_candidates, y_train)"
      ],
      "metadata": {
        "id": "_pSbGJbh1h6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def freedman_diaconis_bins(data):\n",
        "    \"\"\"Calculate number of hist bins using Freedman-Diaconis rule.\"\"\"\n",
        "    # Assuming data is 1D array\n",
        "    data_range = np.nanmax(data) - np.nanmin(data)\n",
        "    iqr = np.subtract(*np.percentile(data, [75, 25]))\n",
        "    bin_width = 2 * iqr * (len(data) ** -0.33)\n",
        "    return int(data_range / bin_width) if bin_width > 0 else 1\n",
        "\n",
        "def check_nan_inf(arr):\n",
        "    \"\"\"Check if array contains NaN or Inf values.\"\"\"\n",
        "    return np.isnan(arr).any(), np.isinf(arr).any()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def plot_hist(X_train=None, X_test=None, y_train = None, y_test = None, description=None):\n",
        "    try:\n",
        "\n",
        "        def plot_specialized_histograms(data, y_data, title):\n",
        "\n",
        "\n",
        "          try:\n",
        "\n",
        "              fig, axs = plt.subplots(3, 4, figsize=(25, 12))\n",
        "              fig.suptitle(f\"{title}\", fontsize=20)\n",
        "\n",
        "              # Separate data based on y-labels\n",
        "              pion_indices = np.where(y_data == np.array([1, 0, 0]))[0]\n",
        "              kaon_indices = np.where(y_data == np.array([0, 1, 0]))[0]\n",
        "              proton_indices = np.where(y_data == np.array([0, 0, 1]))[0]\n",
        "              print(y_data.shape)\n",
        "              data_map = {\n",
        "              'Pion': pion_indices,\n",
        "              'Kaon': kaon_indices,\n",
        "              'Proton': proton_indices\n",
        "              }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "              for idx, (particle, indices) in enumerate(data_map.items()):\n",
        "                  clu_size = data['mCluSize'][indices]\n",
        "                  clu_charge = data['mCluCharge'][indices]\n",
        "\n",
        "\n",
        "                  clu_size = clu_size.reshape(-1)\n",
        "                  clu_charge = clu_charge.reshape(-1)\n",
        "\n",
        "                  print(f\"np.any(np.isnan(clu_size)) {np.any(np.isnan(clu_size))}\")\n",
        "\n",
        "                  print(f\"np.any(np.isnan(clu_charge)) {np.any(np.isnan(clu_charge))}\")\n",
        "                  print(f\"np.any(np.isinf(clu_size)) {np.any(np.isinf(clu_size))}\")\n",
        "                  print(f\"np.any(np.isinf(clu_charge)) {np.any(np.isinf(clu_charge))}\")\n",
        "\n",
        "\n",
        "\n",
        "                  print(f\"len(clu_size) {len(clu_size)}\")\n",
        "                  print(f\"len(clu_charge) {len(clu_charge)}\")\n",
        "                  print(f\"pion_indices.shape {pion_indices.shape}\")\n",
        "                  print(f\"kaon_indices.shape {kaon_indices.shape}\")\n",
        "                  print(f\"proton_indices.shape {proton_indices.shape}\")\n",
        "\n",
        "\n",
        "                  # 1D histograms\n",
        "                  axs[idx, 0].hist(clu_size, bins=9, range=(2, 11), edgecolor='black', alpha=0.6, label='MIP Cluster size')\n",
        "                  axs[idx, 0].legend()\n",
        "                  axs[idx, 0].set_title(f\"{particle} MIP Cluster Size \")\n",
        "\n",
        "\n",
        "                  # 1D histograms\n",
        "                  axs[idx, 1].hist(clu_charge, bins=200, range=(0, 4000), edgecolor='black', alpha=0.6, label='MIP Cluster charge')\n",
        "                  axs[idx, 1].legend()\n",
        "                  axs[idx, 1].set_title(f\"{particle} 'MIP Cluster charge\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                  try: # works!!\n",
        "\n",
        "                      gridx = np.linspace(min(clu_size), max(clu_size),11)\n",
        "                      gridy = np.linspace(min(clu_charge), max(clu_charge),200)\n",
        "                      try:\n",
        "                          h, xedges, yedges = np.histogram2d(clu_size, clu_charge, bins=[9, 30], range=[(2, 11), (200, 800)])\n",
        "                      except ValueError as ve:\n",
        "                          print(\"Error unpacking histogram2d results:\", ve)\n",
        "                          results = np.histogram2d(clu_size, clu_charge, bins=[gridx, gridy])\n",
        "                          print(\"Results returned from histogram2d:\", results)\n",
        "\n",
        "                      norm = LogNorm()  # Define the logarithmic normalization\n",
        "                      im = axs[idx, 2].pcolormesh(xedges, yedges, h.T, cmap='viridis', norm=norm)\n",
        "                      axs[idx, 2].set_title(f\"{particle} 2D Hist\")\n",
        "                      axs[idx, 2].set_xlabel(\"'MIP Cluster size\")\n",
        "                      axs[idx, 2].set_ylabel(\"'MIP Cluster charge\")\n",
        "                      #axs[idx, 2].set_yscale('log')\n",
        "\n",
        "                      plt.colorbar(im, ax=axs[idx, 2])  # Add colorbar with logarithmic scale\n",
        "\n",
        "                  except Exception as e:\n",
        "                      warnings.warn(f\"plot_specialized_histograms failed histogram2d : {e}\")\n",
        "                  try:\n",
        "                      try:\n",
        "                          h, xedges, yedges = np.histogram2d(clu_size, clu_charge, bins=[11, 200], range=[(0, 11), (0, 8000)])\n",
        "\n",
        "                          ax_3d = axs[idx, 3]\n",
        "\n",
        "                          ax_3d = fig.add_subplot(3, 4, 4 * idx + 4, projection='3d')  # change the layout\n",
        "                          x_pos, y_pos = np.meshgrid(xedges[:-1], yedges[:-1], indexing=\"ij\")\n",
        "                          x_pos = x_pos.flatten('F')\n",
        "                          y_pos = y_pos.flatten('F')\n",
        "                          z_pos = np.zeros_like(x_pos)\n",
        "                          dx = dy = 0.5\n",
        "                          dz = h.flatten()\n",
        "                          ax_3d.bar3d(x_pos, y_pos, z_pos, dx, dy, dz, shade=True)\n",
        "                          ax_3d.set_title(f\"{particle} 3D Bar Chart\")\n",
        "                          ax_3d.set_xlabel(\"mCluSize\")\n",
        "                          ax_3d.set_ylabel(\"mCluCharge\")\n",
        "                          ax_3d.set_zlabel(\"Counts\")\n",
        "\n",
        "                      except Exception as e:\n",
        "                          warnings.warn(f\"plot_specialized_histograms failed  np.histogram2 w : {e}\")\n",
        "                  except Exception as e:\n",
        "                      warnings.warn(f\"plot_specialized_histograms failed  # 3D View w : {e}\")\n",
        "\n",
        "\n",
        "              plt.tight_layout()\n",
        "              plt.show()\n",
        "\n",
        "          except Exception as e:\n",
        "                warnings.warn(f\"Exception caught during plot_specialized_histograms: {e}\")\n",
        "\n",
        "\n",
        "        # Call the new function to generate plots for training and testing datasets\n",
        "        train_data = {\n",
        "            'mCluSize': X_train[\"X_train_mCluSize\"],\n",
        "            \"mCluCharge\": X_train[\"X_train_mCluCharge\"]\n",
        "        }\n",
        "\n",
        "        test_data = {\n",
        "            'mCluSize': X_test[\"X_test_mCluSize\"],\n",
        "            \"mCluCharge\": X_test[\"X_test_mCluCharge\"]\n",
        "        }\n",
        "\n",
        "        plot_specialized_histograms(train_data, y_train, title=\"Training Data\")\n",
        "        plot_specialized_histograms(test_data, y_test, title=\"Testing Data\")\n",
        "\n",
        "\n",
        "        fig, axs = plt.subplots(2, 4, figsize=(25, 10)) # For 'Refractive Index', 'Momentum', 'Phi', and 'Theta'\n",
        "        fig.suptitle(f\"Training and Testing Data: {description}\", fontsize=20)\n",
        "\n",
        "        fig_mClu, axs_mClu = plt.subplots(2, 2, figsize=(16, 10)) # For 'mCluSize' and 'mCluCharge'\n",
        "        fig_mClu.suptitle(f\"Cluster Features: {description}\", fontsize=20)\n",
        "\n",
        "        fig3, axs3 = plt.subplots(2, 5, figsize=(25, 10))\n",
        "        fig3.suptitle(f\"Training and Testing Data: {description}\", fontsize=20)\n",
        "\n",
        "\n",
        "\n",
        "        # impact points on rad / MIP spatial distribution\n",
        "        fig1, axs1 = plt.subplots(2, 3, figsize=(18, 10))\n",
        "        fig1.suptitle(f\"2D Maps and Projections: {description}\", fontsize=20)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        for key, value in X_train.items():\n",
        "            print(key, type(value))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        def plot_routine(variables, row_idx):\n",
        "            axs_idx = 0\n",
        "            axs_idx = 0\n",
        "            axs_mClu_idx = 0\n",
        "            for label, variable in variables.items():\n",
        "                has_nan, has_inf = check_nan_inf(variable)\n",
        "                if has_nan or has_inf:\n",
        "                    warnings.warn(f\"{label} contains NaN or Inf values. This may lead to issues.\")\n",
        "\n",
        "                bins = freedman_diaconis_bins(variable)\n",
        "                range_val = (np.nanmin(variable), np.nanmax(variable))\n",
        "\n",
        "                if label == 'MIP Position' or label == 'Rad Position':\n",
        "                    variable = np.asarray(variable).reshape(-1, 2)\n",
        "                    mask = (variable[:, 0] != 0) & (variable[:, 1] != 0)\n",
        "                    variable = variable[mask]\n",
        "                    axs1[row_idx, 0].scatter(variable[:, 0], variable[:, 1], marker='o', s=10)\n",
        "                    axs1[row_idx, 0].set_title(f\"{'Train' if row_idx == 0 else 'Test'} 2D Map: {label}\")\n",
        "                    axs1[row_idx, 1].hist(variable[:, 0], bins=bins, range=range_val, edgecolor='black')\n",
        "                    axs1[row_idx, 1].set_title(f\"{label} X\")\n",
        "                    axs1[row_idx, 2].hist(variable[:, 1], bins=bins, range=range_val, edgecolor='black')\n",
        "                    axs1[row_idx, 2].set_title(f\"{label} Y\")\n",
        "\n",
        "\n",
        "                elif label == 'mCluSize':\n",
        "                    axs_mClu[row_idx, axs_mClu_idx].hist(variable, bins=11, range=(0, 11), edgecolor='black')\n",
        "                    axs_mClu[row_idx, axs_mClu_idx].set_title(f\"{'Train' if row_idx == 0 else 'Test'} {label}\")\n",
        "                    axs_mClu_idx += 1\n",
        "\n",
        "                elif label == 'mCluCharge':\n",
        "                    axs_mClu[row_idx, axs_mClu_idx].hist(variable, bins=200, range=(0, 8000), edgecolor='black')\n",
        "                    axs_mClu[row_idx, axs_mClu_idx].set_title(f\"{'Train' if row_idx == 0 else 'Test'} {label}\")\n",
        "                    axs_mClu_idx += 1\n",
        "\n",
        "                elif label == 'Momentum':\n",
        "                    axs[row_idx, axs_idx].hist(variable, bins=50, range=(0, 5), edgecolor='black')\n",
        "                    axs[row_idx, axs_idx].set_title(f\"{'Train' if row_idx == 0 else 'Test'} {label}\")\n",
        "                    axs_idx += 1\n",
        "\n",
        "                else:\n",
        "                    axs[row_idx, axs_idx].hist(variable, bins=bins, range=range_val, edgecolor='black')\n",
        "                    axs[row_idx, axs_idx].set_title(f\"{'Train' if row_idx == 0 else 'Test'} {label}\")\n",
        "                    axs_idx += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        X_train_variables = {\n",
        "        'Refractive Index': X_train[\"X_train_refractive_index\"],\n",
        "        'Momentum': X_train[\"X_train_momentum\"],\n",
        "        'Phi': X_train[\"X_train_phi\"],\n",
        "        'Theta': X_train[\"X_train_theta\"],\n",
        "        'mCluSize': X_train[\"X_train_mCluSize\"],\n",
        "        \"mCluCharge\": X_train[\"X_train_mCluCharge\"],  # New addition\n",
        "        'MIP Position': X_train[\"X_train_mip_position\"],\n",
        "        'Rad Position': X_train[\"X_train_rad_position\"],\n",
        "        'Pion Candidates': X_train[\"X_train_pion_candidates\"],\n",
        "        'Kaon Candidates': X_train[\"X_train_kaon_candidates\"],\n",
        "        'Proton Candidates': X_train[\"X_train_proton_candidates\"]\n",
        "\n",
        "        }\n",
        "\n",
        "\n",
        "        # add kaon and proton candidates ;\n",
        "        X_test_variables = {\n",
        "        'Refractive Index': X_test[\"X_test_refractive_index\"],\n",
        "        'Momentum': X_test[\"X_test_momentum\"],\n",
        "        'Phi': X_test[\"X_test_phi\"],\n",
        "        'Theta': X_test[\"X_test_theta\"],\n",
        "        'mCluSize': X_test[\"X_test_mCluSize\"],\n",
        "        \"mCluCharge\": X_test[\"X_test_mCluCharge\"],  # New addition\n",
        "        'MIP Position': X_test[\"X_test_mip_position\"],\n",
        "        'Rad Position': X_test[\"X_test_rad_position\"],\n",
        "        'Pion Candidates': X_test[\"X_test_pion_candidates\"],\n",
        "        }\n",
        "\n",
        "        train_data = {\n",
        "        'mCluSize': X_train[\"X_train_mCluSize\"],\n",
        "        \"mCluCharge\": X_train[\"X_train_mCluCharge\"]\n",
        "        }\n",
        "\n",
        "        test_data = {\n",
        "        'mCluSize': X_test[\"X_test_mCluSize\"],\n",
        "        \"mCluCharge\": X_test[\"X_test_mCluCharge\"]\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "        plot_routine(X_train_variables, 0)\n",
        "        plot_routine(X_test_variables, 1)\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    except Exception as e:\n",
        "        warnings.warn(f\"Exception caught during plot_histl: {e}\")\n",
        "\n",
        "import warnings\n",
        "\n",
        "try:\n",
        "\n",
        "    plot_hist(X_train = X_train, X_test =  X_test, y_train = y_train, y_test = y_test,  description = \"unscaled\")\n",
        "except Exception as e:\n",
        "    warnings.warn(f\"Exception caught during first function call: {e}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "muTlVCz1djh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import warnings\n",
        "\n",
        "try:\n",
        "    plot_hist(X_train = X_train, X_test =  X_test, y_train = y_train, y_test = y_test,  description = \"unscaled\")\n",
        "except Exception as e:\n",
        "    warnings.warn(f\"Exception caught during first function call: {e}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9BVd9j2SaJaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "RgrwfICDk7NJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjFn4oJgEJnN"
      },
      "outputs": [],
      "source": [
        "threshold = 0.7\n",
        "\n",
        "X_test = X_test\n",
        "y_train = y_train\n",
        "y_test = y_test\n",
        "#X_train_scaled, X_test_scaled, X_train, X_test, y_train, y_test = classifier.preprocess_data()\n",
        "\n",
        "\n",
        "train_variables = [X_train_pion_candidates, X_train_kaon_candidates, X_train_proton_candidates, X_train_non_candidates, X_train_momentum, X_train_refractive_index, X_train_phi, X_train_theta, X_train_mip_position, X_train_rad_position, X_train_mCluCharge, X_train_mCluSize]\n",
        "test_variables = [X_test_pion_candidates, X_test_kaon_candidates, X_test_proton_candidates, X_test_non_candidates, X_test_momentum, X_test_refractive_index, X_test_phi, X_test_theta, X_test_mip_position, X_test_rad_position, X_test_mCluCharge, X_test_mCluSize]\n",
        "\n",
        "y_pred_train = model.predict(train_variables)\n",
        "y_pred_test = model.predict(test_variables)\n",
        "\n",
        "# plot_individual_types(idx=index, x_pion=X_test_pion_candidates, x_kaon=X_test_kaon_candidates, x_proton=X_test_proton_candidates, x_non=X_test_non_candidates, MIP_charge=X_test_mCluCharge, MIP_position=X_test_mip_position, RAD_position=X_test_rad_position, phi=X_test_phi, y_train=y_test, log_scale=True, X_index=X_test_index_particle)\n",
        "\n",
        "\n",
        "#def plot_combined_types(i, x_pion, x_kaon, x_proton, x_non, MIP_position, RAD_position, phiP, thetaP, y_train):\n",
        "#\n",
        "\n",
        "class_labels  = [\"pion\", \"kaon\", \"proton\"]\n",
        "i = 0\n",
        "for pred, true in zip(y_pred_test, y_test):\n",
        "    max_prob = np.max(pred)\n",
        "    predicted_class = np.argmax(pred)\n",
        "    if max_prob > threshold:\n",
        "        label = class_labels[predicted_class]  # Assuming class_labels is a list of your labels\n",
        "    else:\n",
        "        label = 'reject'\n",
        "\n",
        "    print(f\"====================================================================================== \\n\")\n",
        "\n",
        "    str = f\" predicted_class = {predicted_class}  true class = {true}|  max_prob = {max_prob:.2f} pred = {pred}\"\n",
        "\n",
        "    plot_individual_types(idx=i, x_pion=X_test_pion_candidates, x_kaon=X_test_kaon_candidates, x_proton=X_test_proton_candidates, x_non=X_test_non_candidates, MIP_charge=X_test_mCluCharge, MIP_position=X_test_mip_position, RAD_position=X_test_rad_position, phi=X_test_phi, y_train=y_test, log_scale=True, X_index=X_test_index_particle)\n",
        "    plot_combined_types(i, X_test_pion_candidates, X_test_kaon_candidates, X_test_proton_candidates, X_test_non_candidates,X_test_mip_position, X_test_rad_position, X_test_phi, X_test_theta, y_train = y_test, X_index = X_test_index_particle)\n",
        "    print(str)\n",
        "    i = i + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zukgX2C29574"
      },
      "outputs": [],
      "source": [
        "mask = [1,1,1,1]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "fc1_unit = 64\n",
        "units = [fc1_unit * i for i in [64, 32, 16, 16, 8, 4, 4, 2, 2, 1]]\n",
        "units2 = [fc1_unit * i for i in [64, 32, 16, 16, 8, 4, 4, 2, 2, 1]]\n",
        "\n",
        "CNN_units = None\n",
        "\n",
        "classifier.train(X_train = X_train, X_test = X_test, y_train = y_train, y_test = y_test, mask = mask, num_epochs=150, include_cnn = True, units = units, units2 = units2, CNN_units = CNN_units)\n",
        "#classifier.train(X_train = X_train_scaled, X_test = X_test_scaled, y_train = y_train, y_test = y_test, mask = mask, num_epochs=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoauWOieMnWE"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/eflatlan/CNN_PID/models_sacved/helper_functions.py\n",
        "from helper_functions import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvEulDjrHxkx"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/eflatlan/CNN_PID/models_sacved/plot_helper_functions.py\n",
        "from plot_helper_functions import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxRzlwIzxxk6"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/eflatlan/CNN_PID/models_sacved/plot_helper_functions.py\n",
        "from plot_helper_functions import *"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPn8478Vt4rFTBDQbmfRrJI",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}