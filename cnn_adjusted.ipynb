{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNOaNaWp1QKbL4VzOH5L5Z5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eflatlan/CNN_PID/blob/populateAll/cnn_adjusted.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install h5py numpy\n",
        "\n",
        "import os\n",
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrPLFO_92Cvr",
        "outputId": "5c70f0df-dca9-4b54-9976-5d22a80295e3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.22.4)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "def plot_random_element(X_train_map):\n",
        "    index = random.randint(0, len(X_train_map) - 1)  # Pick a random index\n",
        "    element = X_train_map[index, :, :, 0]  # Retrieve the element\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(element, cmap='viridis', origin='lower')\n",
        "    plt.title(f\"Random Element from X_train_map (Index {index})\")\n",
        "    plt.colorbar(label='Intensity')\n",
        "    plt.xlabel('X Axis')\n",
        "    plt.ylabel('Y Axis')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FmXOGRP_UQZM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot5(X_test_map, particle_vector):\n",
        "\n",
        "  # Plotting random maps with information\n",
        "\n",
        "  # Select 5 random indices from the test data\n",
        "  random_indices = np.random.choice(range(X_test_map.shape[0]), size=5, replace=False)\n",
        "\n",
        "  # Create a subplot with 5 rows and 1 column\n",
        "  fig, axes = plt.subplots(nrows=5, ncols=1, figsize=(8, 20))\n",
        "\n",
        "  # Iterate over the random indices and plot each map with information\n",
        "  for i, index in enumerate(random_indices):\n",
        "      # Get the map and corresponding information\n",
        "      map_data = X_test_map[index, :, :, 0]\n",
        "      mass_category = particle_vector[index].mass_category\n",
        "      ckov = particle_vector[index].ckov\n",
        "      mip_position = particle_vector[index].mip_position\n",
        "      momentum = particle_vector[index].momentum\n",
        "\n",
        "      # Plot the map\n",
        "      axes[i].imshow(map_data, cmap='gray')\n",
        "\n",
        "      # Add a red dot at the MIP position\n",
        "      axes[i].plot(mip_position[0], mip_position[1], 'ro')\n",
        "\n",
        "      # Set the title with the information\n",
        "      axes[i].set_title(f\"Mass: {mass_category}, CKOV: {ckov}, MIP Position: {mip_position}, Momentum: {momentum}\")\n",
        "      axes[i].axis('off')\n",
        "\n",
        "  # Adjust the spacing between subplots\n",
        "  plt.tight_layout()\n",
        "\n",
        "  # Show the plot\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "PlFS0yz2Feuz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_lr_scheduler(num_epochs = 10):\n",
        "\n",
        "  tf.random.set_seed(42)\n",
        "  div = num_epochs/4\n",
        "  print(\"div =\", div)\n",
        "  print(\"1e-4 * 10**(epoch/div) = \", 1e-4 * 10**(num_epochs/div))\n",
        "  lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-4 * 10**(epoch/div)) # traverse a set of learning rate values starting from 1e-4, increasing by 10**(epoch/20) every epoch\n",
        "  return lr_scheduler\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def plot_lr(num_epochs = 10, history = None):\n",
        "  div = num_epochs/4\n",
        "  lrs = 1e-4 * (10 ** (np.arange(num_epochs)/div))\n",
        "  plt.figure(figsize=(10, 7))\n",
        "  plt.semilogx(lrs, history.history[\"loss\"]) # we want the x-axis (learning rate) to be log scale\n",
        "  plt.xlabel(\"Learning Rate\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.title(\"Learning rate vs. loss\");"
      ],
      "metadata": {
        "id": "pcY6LVGpxZQI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ParticleDataUtils:\n",
        "    class Candidate2:\n",
        "        def __init__(self, x, y, candStatus):\n",
        "            self.x = x\n",
        "            self.y = y\n",
        "            self.candStatus = candStatus\n",
        "\n",
        "    class ParticleInfo: # pls help me understand if any of these fields are missing from the X_train :\n",
        "        def __init__(self, momentum, mass, energy, refractiveIndex, ckov, xRad, yRad, xPC, yPC, thetaP, phiP, arrayInfo, candsCombined):\n",
        "            self.momentum = momentum # this dhould be with\n",
        "            self.mass = mass    # not with\n",
        "            self.energy = energy # with\n",
        "            self.refractiveIndex = refractiveIndex # with\n",
        "            self.ckov = ckov # not with\n",
        "            self.xRad = xRad # with\n",
        "            self.yRad = yRad # with\n",
        "            self.xPC = xPC# with\n",
        "            self.yPC = yPC# with\n",
        "            self.thetaP = thetaP# with\n",
        "            self.phiP = phiP# with\n",
        "            self.arrayInfo = arrayInfo # do not include this to model\n",
        "            self.candsCombined = candsCombined # with the field candStatus is a int that is 0..7, please make it categorical\n",
        "            self.mass_category = self.infer_mass_category(mass)\n",
        "            self.mip_position = [xPC, yPC]\n",
        "            self.rad_position = [xRad, yRad]\n",
        "\n",
        "        @staticmethod\n",
        "        def infer_mass_category(mass):\n",
        "            pion_mass = 0.1396\n",
        "            proton_mass = 0.938\n",
        "            kaon_mass = 0.4937\n",
        "\n",
        "            if abs(mass - pion_mass) < 1e-6:\n",
        "                return \"pion\"\n",
        "            elif abs(mass - proton_mass) < 1e-6:\n",
        "                return \"proton\"\n",
        "            elif abs(mass - kaon_mass) < 1e-6:\n",
        "                return \"kaon\"\n",
        "            else:\n",
        "                return \"unknown\"\n",
        "\n",
        "def load_particle_info_from_hdf5(filename):\n",
        "    particle_vector = []\n",
        "\n",
        "    with h5py.File(filename, 'r') as file:\n",
        "        first_group_name = list(file.keys())[0]\n",
        "        second_group_name = list(file.keys())[1] # print these values?\n",
        "        #second_group_name = list(file.keys())[1] # print these values?\n",
        "\n",
        "\n",
        "        for f in first_group_name: print(f)\n",
        "\n",
        "        group = file[first_group_name]\n",
        "\n",
        "        print(\"Attributes for group '{}':\".format(first_group_name))\n",
        "        for attr_name, attr_value in group.attrs.items():\n",
        "          print(\"{}: {}\".format(attr_name, attr_value))\n",
        "\n",
        "\n",
        "        print(\"Attributes for group '{}':\".format(second_group_name))\n",
        "        for attr_name, attr_value in group.attrs.items():\n",
        "          print(\"{}: {}\".format(attr_name, attr_value))\n",
        "\n",
        "        for i, group_name in enumerate(file):\n",
        "            group = file[group_name]\n",
        "\n",
        "            momentum = group.attrs['Momentum']\n",
        "            mass = group.attrs['Mass']\n",
        "            energy = group.attrs['Energy']\n",
        "            refractiveIndex = group.attrs['RefractiveIndex']\n",
        "            ckov = group.attrs['Ckov']\n",
        "            xRad = group.attrs['xRad']\n",
        "            yRad = group.attrs['yRad']\n",
        "            xPC = group.attrs['xPC']\n",
        "            yPC = group.attrs['yPC']\n",
        "            thetaP = group.attrs['ThetaP']\n",
        "            phiP = group.attrs['PhiP']\n",
        "\n",
        "            arrayInfo = [group.attrs['ArrayInfo0'], group.attrs['ArrayInfo1'], group.attrs['ArrayInfo2'], group.attrs['ArrayInfo3']]\n",
        "\n",
        "            # Read arrayInfo\n",
        "            # rrayInfo_dataset = group['ArrayInfo']\n",
        "            # rrayInfo_data = arrayInfo_dataset[...]\n",
        "\n",
        "            # Read candsCombined as complex type\n",
        "            #candsCombined_dataset = group['candsCombined']\n",
        "\n",
        "            candsCombined_dataset = group['candsCombined']\n",
        "\n",
        "            candsCombined_data = candsCombined_dataset[...]\n",
        "            #candsCombinedVec = [ParticleDataUtils.Candidate2(x['x'], x['y']) for x in candsCombined_data]\n",
        "            # candsCombinedStatVec = [ParticleDataUtils.Candidate2(x['candStatus']) for x in candsCombined_data]\n",
        "            #candsCombinedStatVec.to_list()\n",
        "            #candsCombinedVec.to_list()\n",
        "\n",
        "            candsCombined = [ParticleDataUtils.Candidate2(x['x'], x['y'], x['candStatus']) for x in candsCombined_data]\n",
        "\n",
        "            particle_info = ParticleDataUtils.ParticleInfo(\n",
        "                momentum, mass, energy, refractiveIndex, ckov, xRad, yRad, xPC, yPC, thetaP, phiP, arrayInfo = arrayInfo, candsCombined=candsCombined)\n",
        "\n",
        "            particle_vector.append(particle_info)\n",
        "\n",
        "    return particle_vector\n",
        "\n",
        "def read_particle_data_from_file(filename=\"particle.h5\"):\n",
        "    drive_path = '/content/drive/MyDrive/Colab Notebooks/CERN_ML/CNN_PID/test/'  # Update the path to your Google Drive folder\n",
        "    file_path = os.path.join(drive_path, filename)\n",
        "    particle_vector = load_particle_info_from_hdf5(file_path)\n",
        "    return particle_vector\n",
        "\n",
        "# Prepare the training data\n",
        "#X_train_candsCombined_xy = [np.array([(cand.x, cand.y) for cand in particle.candsCombined]) for particle in particle_vector]\n",
        "#X_train_candsCombined_status = [to_categorical([cand.candStatus for cand in particle.candsCombined], num_classes=8) for particle in particle_vector]\n",
        "\n",
        "\n",
        "# Prepare the additional training data\n",
        "X_train_candsCombined_xy = []\n",
        "X_train_candsCombined_status = []\n",
        "# Extract other fields\n",
        "\n",
        "\n",
        "\n",
        "filename = 'ParticleInfo.h5'\n",
        "particle_vector = read_particle_data_from_file(filename)\n",
        "\n",
        "\n",
        "for particle in particle_vector:\n",
        "    cand_xy_list = []\n",
        "    cand_status_list = []\n",
        "    for cand in particle.candsCombined:\n",
        "        cand_xy_list.append([cand.x, cand.y])\n",
        "        cand_status_list.append(cand.candStatus)\n",
        "        #print(cand.candStatus)\n",
        "        #print(cand.x)\n",
        "        #print(\"ca va\")\n",
        "    c = particle.refractiveIndex\n",
        "    #print(c)\n",
        "    #print((particle.candsCombined))\n",
        "    #print(np.array(particle.candsCombined))\n",
        "\n",
        "    #print(f\"length of t {len(cand_xy_list)}\")\n",
        "\n",
        "    X_train_candsCombined_xy.append(cand_xy_list)\n",
        "    X_train_candsCombined_status.append(cand_status_list)\n",
        "\n",
        "# Convert them into NumPy arrays\n",
        "X_train_candsCombined_xy = np.array(X_train_candsCombined_xy)\n",
        "X_train_candsCombined_status = np.array(X_train_candsCombined_status)\n",
        "# Normalize X_train_candsCombined_xy\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
        "std_scaler = StandardScaler()\n",
        "label_binarizer = LabelBinarizer()\n",
        "\n",
        "#X_train_candsCombined_xy = std_scaler.fit_transform(X_train_candsCombined_xy)\n",
        "\n",
        "# One-hot encode X_train_candsCombined_status\n",
        "\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "mlb = MultiLabelBinarizer()\n",
        "\n",
        "X_train_candsCombined_status_encoded = mlb.fit_transform(X_train_candsCombined_status)\n",
        "\n",
        "X_train_phi = [particle.phiP for particle in particle_vector]\n",
        "X_train_theta = [particle.thetaP for particle in particle_vector]\n",
        "X_train_energy = [particle.energy for particle in particle_vector]\n",
        "\n",
        "# Convert to NumPy arrays\n",
        "X_train_phi = np.array(X_train_phi)\n",
        "X_train_theta = np.array(X_train_theta)\n",
        "X_train_energy = np.array(X_train_energy)\n",
        "X_train_candsCombined_xy = np.array(X_train_candsCombined_xy)\n",
        "X_train_candsCombined_status = np.array(X_train_candsCombined_status)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# One-hot encode the labels\n",
        "label_binarizer = LabelBinarizer()\n",
        "y_train_encoded = label_binarizer.fit_transform([particle.mass_category for particle in particle_vector])\n",
        "y_test_encoded = label_binarizer.transform([particle.mass_category for particle in particle_vector])\n",
        "X_train_momentum = [particle.momentum for particle in particle_vector]\n",
        "\n",
        "\n",
        "X_train_momentum = np.array(X_train_momentum)\n",
        "\n",
        "\n",
        "X_train_mip_position = []\n",
        "for i, particle in enumerate(particle_vector):\n",
        "    # Reset the map_data for each particle\n",
        "    #map_data = np.zeros(map_shape)\n",
        "    X_train_mip_position.append(np.array(particle.mip_position))\n",
        "\n",
        "\n",
        "X_train_mip_position = np.array(X_train_mip_position)\n",
        "\n",
        "\n",
        "X_train_rad_position = []\n",
        "for i, particle in enumerate(particle_vector):\n",
        "    # Reset the map_data for each particle\n",
        "    #map_data = np.zeros(map_shape)\n",
        "    X_train_rad_position.append(np.array(particle.rad_position))\n",
        "\n",
        "X_train_rad_position = np.array(X_train_rad_position)\n",
        "\n",
        "# create a scaler object\n",
        "std_scaler = StandardScaler()\n",
        "# Normalize X_train_refractive_index\n",
        "X_train_refractive_index = np.array([particle.refractiveIndex for particle in particle_vector])\n",
        "X_train_refractive_index = X_train_refractive_index.reshape(-1, 1)\n",
        "X_train_refractive_index = std_scaler.fit_transform(X_train_refractive_index)\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Splitting the data into train and test sets\n",
        "X_train_momentum, X_test_momentum, X_train_refractive_index, X_test_refractive_index, X_train_mip_position, X_test_mip_position, X_train_rad_position, X_test_rad_position, X_train_candsCombined_xy, X_test_candsCombined_xy, X_train_candsCombined_status, X_test_candsCombined_status, X_train_rad_position, X_test_rad_position, X_train_phi, X_test_phi, X_train_theta, X_test_theta, X_train_energy, X_test_energy, y_train, y_test = train_test_split(\n",
        "    X_train_momentum,\n",
        "    X_train_refractive_index,\n",
        "    X_train_mip_position,\n",
        "    X_train_rad_position,\n",
        "    X_train_candsCombined_xy,\n",
        "    X_train_candsCombined_status,\n",
        "    X_train_rad_position,\n",
        "    X_train_phi,\n",
        "    X_train_theta,\n",
        "    X_train_energy,\n",
        "    y_train_encoded,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "X_train_candsCombined_xy_padded = pad_sequences(X_train_candsCombined_xy, maxlen=100, padding='post')\n",
        "X_train_candsCombined_xy_padded = np.array(X_train_candsCombined_xy_padded)\n",
        "\n",
        "# do the same for the test set\n",
        "X_test_candsCombined_xy_padded = pad_sequences(X_test_candsCombined_xy, maxlen=100, padding='post')\n",
        "X_test_candsCombined_xy_padded = np.array(X_test_candsCombined_xy_padded)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train_candsCombined_status_padded = pad_sequences(X_train_candsCombined_status, maxlen=100, padding='post')\n",
        "X_train_candsCombined_status_padded = X_train_candsCombined_status_padded[..., np.newaxis]  # Add a new dimension for features\n",
        "X_test_candsCombined_status_padded = pad_sequences(X_test_candsCombined_status, maxlen=100, padding='post')\n",
        "X_test_candsCombined_status_padded = X_test_candsCombined_status_padded[..., np.newaxis]  # Add a new dimension for features\n",
        "\n",
        "\n",
        "candsCombined_xy_shape = (100, 2)  # if you choose to pad your sequences to length 100\n",
        "candsCombined_status_shape = (100, 1)  # Now status is a sequence of length 100 with 1 feature\n",
        "\n",
        "\n",
        "\n",
        "# Define the input shapes\n",
        "momentum_shape = (1,)\n",
        "refractive_index_shape = (1,)\n",
        "mip_position_shape = (2,)\n",
        "#candsCombined_xy_shape = X_train_candsCombined_xy.shape[1:]  # adjust as necessary\n",
        "#candsCombined_status_shape = X_train_candsCombined_status.shape[1:]  # adjust as necessary\n",
        "rad_position_shape = (1,)\n",
        "phi_shape = (1,)\n",
        "theta_shape = (1,)\n",
        "energy_shape = (1,)\n",
        "\n",
        "\n",
        "from __future__ import print_function\n",
        "import os\n",
        "import h5py\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, concatenate, BatchNormalization, MaxPooling2D, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Define inputs\n",
        "# vectors, each element number n containing a x,y pair in  candsCombined_xy_input should be paired with the categorical value in\n",
        "# candsCombined_status_input\n",
        "candsCombined_xy_input = Input(shape=candsCombined_xy_shape, name='candsCombined_xy_input')\n",
        "candsCombined_status_input = Input(shape=candsCombined_status_shape, name='candsCombined_status_input')\n",
        "\n",
        "#scalars\n",
        "momentum_input = Input(shape=momentum_shape, name='momentum_input')\n",
        "refractive_index_input = Input(shape=refractive_index_shape, name='refractive_index_input')\n",
        "phi_input = Input(shape=phi_shape, name='phi_input')\n",
        "theta_input = Input(shape=theta_shape, name='theta_input')\n",
        "energy_input = Input(shape=energy_shape, name='energy_input')\n",
        "\n",
        "# x,y pairs\n",
        "mip_position_input = Input(shape=mip_position_shape, name='mip_position_input')\n",
        "rad_position_input = Input(shape=rad_position_shape, name='rad_position_input')\n",
        "\n",
        "\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Flatten, Input, concatenate, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Define an input processing layer for candsCombined_xy\n",
        "xy_dense = Dense(32, activation='relu')(candsCombined_xy_input)\n",
        "xy_dense = Dropout(0.2)(xy_dense)\n",
        "\n",
        "# Define an input processing layer for candsCombined_status\n",
        "status_dense = Dense(32, activation='relu')(candsCombined_status_input)\n",
        "status_dense = Dropout(0.2)(status_dense)\n",
        "\n",
        "# Concatenate the xy and status layers\n",
        "candsCombined_processed = concatenate([xy_dense, status_dense])\n",
        "\n",
        "# Define a dense layer for the concatenated xy and status\n",
        "candsCombined_dense = Dense(64, activation='relu')(candsCombined_processed)\n",
        "candsCombined_dense = Dropout(0.2)(candsCombined_dense)\n",
        "\n",
        "# Define the scalar inputs\n",
        "scalar_inputs = concatenate([momentum_input, refractive_index_input, phi_input, theta_input, energy_input])\n",
        "\n",
        "# Define a dense layer for the scalar inputs\n",
        "scalar_dense = Dense(64, activation='relu')(scalar_inputs)\n",
        "scalar_dense = Dropout(0.2)(scalar_dense)\n",
        "\n",
        "# Define the position inputs\n",
        "position_inputs = concatenate([mip_position_input, rad_position_input])\n",
        "\n",
        "# Define a dense layer for the position inputs\n",
        "position_dense = Dense(64, activation='relu')(position_inputs)\n",
        "position_dense = Dropout(0.2)(position_dense)\n",
        "\n",
        "# Concatenate all the processed inputs\n",
        "from tensorflow.keras.layers import RepeatVector\n",
        "\n",
        "# Define a dense layer for the scalar inputs\n",
        "scalar_dense = Dense(64, activation='relu')(scalar_inputs)\n",
        "scalar_dense = Dropout(0.2)(scalar_dense)\n",
        "scalar_dense = RepeatVector(100)(scalar_dense)  # Repeat along the time dimension to match the shape of candsCombined\n",
        "\n",
        "# Define a dense layer for the position inputs\n",
        "position_dense = Dense(64, activation='relu')(position_inputs)\n",
        "position_dense = Dropout(0.2)(position_dense)\n",
        "position_dense = RepeatVector(100)(position_dense)  # Repeat along the time dimension to match the shape of candsCombined\n",
        "\n",
        "\n",
        "concat = concatenate([candsCombined_dense, scalar_dense, position_dense])\n",
        "\n",
        "# Define the final fully connected layers\n",
        "fc1 = Dense(128, activation='relu')(concat)\n",
        "fc1 = BatchNormalization()(fc1)\n",
        "fc1 = Dropout(0.1)(fc1)\n",
        "\n",
        "fc2 = Dense(32, activation='relu')(fc1)\n",
        "fc2 = BatchNormalization()(fc2)\n",
        "fc2 = Dropout(0.1)(fc2)\n",
        "\n",
        "output = Dense(3, activation='softmax')(fc2)  # Predicting mass categories\n",
        "\n",
        "# Define the model\n",
        "model = Model(inputs=[candsCombined_xy_input, candsCombined_status_input, momentum_input, refractive_index_input, phi_input, theta_input, energy_input, mip_position_input, rad_position_input], outputs=output)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit([X_train_candsCombined_xy_padded, X_train_candsCombined_status_padded, X_train_momentum, X_train_refractive_index, X_train_phi, X_train_theta, X_train_energy, X_train_mip_position, X_train_rad_position],\n",
        "                    y_train,\n",
        "                    validation_data=([X_test_candsCombined_xy_padded, X_test_candsCombined_status_padded, X_test_momentum, X_test_refractive_index, X_test_phi, X_test_theta, X_test_energy, X_test_mip_position, X_test_rad_position], y_test),\n",
        "                    epochs=10,\n",
        "                    batch_size=32)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IjrxNzz0LMqU",
        "outputId": "f51dad74-f640-4e4b-deb7-09ba0d2b59ad"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P\n",
            "a\n",
            "r\n",
            "t\n",
            "i\n",
            "c\n",
            "l\n",
            "e\n",
            "0\n",
            "Attributes for group 'Particle0':\n",
            "ArrayInfo0: 67.0\n",
            "ArrayInfo1: 0.0\n",
            "ArrayInfo2: 0.0\n",
            "ArrayInfo3: 0.0\n",
            "Ckov: 0.6438444256782532\n",
            "Energy: 5.793657302856445\n",
            "Mass: 0.4936999976634979\n",
            "Momentum: 2.393141508102417\n",
            "PhiP: -2.6928224563598633\n",
            "RefractiveIndex: 1.2766509056091309\n",
            "ThetaP: 0.1838526725769043\n",
            "xPC: 14.573732376098633\n",
            "xRad: 16.123476028442383\n",
            "yPC: 130.95164489746094\n",
            "yRad: 131.69790649414062\n",
            "Attributes for group 'Particle1':\n",
            "ArrayInfo0: 67.0\n",
            "ArrayInfo1: 0.0\n",
            "ArrayInfo2: 0.0\n",
            "ArrayInfo3: 0.0\n",
            "Ckov: 0.6438444256782532\n",
            "Energy: 5.793657302856445\n",
            "Mass: 0.4936999976634979\n",
            "Momentum: 2.393141508102417\n",
            "PhiP: -2.6928224563598633\n",
            "RefractiveIndex: 1.2766509056091309\n",
            "ThetaP: 0.1838526725769043\n",
            "xPC: 14.573732376098633\n",
            "xRad: 16.123476028442383\n",
            "yPC: 130.95164489746094\n",
            "yRad: 131.69790649414062\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-15703af7f250>:147: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  X_train_candsCombined_xy = np.array(X_train_candsCombined_xy)\n",
            "<ipython-input-14-15703af7f250>:148: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  X_train_candsCombined_status = np.array(X_train_candsCombined_status)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-15703af7f250>\u001b[0m in \u001b[0;36m<cell line: 366>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0002\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m history = model.fit([X_train_candsCombined_xy_padded, X_train_candsCombined_status_padded, X_train_momentum, X_train_refractive_index, X_train_phi, X_train_theta, X_train_energy, X_train_mip_position, X_train_rad_position], \n\u001b[0m\u001b[1;32m    367\u001b[0m                     \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_test_candsCombined_xy_padded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_candsCombined_status_padded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_momentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_refractive_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_phi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_theta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_energy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_mip_position\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_rad_position\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/input_spec.py\", line 280, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'model_1' (type Functional).\n    \n    Input 0 of layer \"dense_29\" is incompatible with the layer: expected axis -1 of input shape to have value 3, but received input with shape (32, 4)\n    \n    Call arguments received by layer 'model_1' (type Functional):\n      • inputs=('tf.Tensor(shape=(32, 100, 2), dtype=int32)', 'tf.Tensor(shape=(32, 100, 1), dtype=int32)', 'tf.Tensor(shape=(32,), dtype=float32)', 'tf.Tensor(shape=(32, 1), dtype=float32)', 'tf.Tensor(shape=(32,), dtype=float32)', 'tf.Tensor(shape=(32,), dtype=float32)', 'tf.Tensor(shape=(32,), dtype=float32)', 'tf.Tensor(shape=(32, 2), dtype=float32)', 'tf.Tensor(shape=(32, 2), dtype=float32)')\n      • training=True\n      • mask=None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training loss and validation loss\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plot training accuracy and validation accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DxlPsrmTHFXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wTJhymBA5bFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bLBU0quO4IYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v-ED5GaZ5265"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_ulo0AI1buN"
      },
      "outputs": [],
      "source": []
    }
  ]
}