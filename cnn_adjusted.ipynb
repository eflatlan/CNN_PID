{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO1LZpTnocC8PzzlEjbtSnV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eflatlan/CNN_PID/blob/main/cnn_adjusted.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install h5py numpy\n",
        "\n",
        "import os\n",
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrPLFO_92Cvr",
        "outputId": "625aa140-73b8-44eb-c2a2-726b0585a9e7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.22.4)\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "def plot_random_element(X_train_map):\n",
        "    index = random.randint(0, len(X_train_map) - 1)  # Pick a random index\n",
        "    element = X_train_map[index, :, :, 0]  # Retrieve the element\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(element, cmap='viridis', origin='lower')\n",
        "    plt.title(f\"Random Element from X_train_map (Index {index})\")\n",
        "    plt.colorbar(label='Intensity')\n",
        "    plt.xlabel('X Axis')\n",
        "    plt.ylabel('Y Axis')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FmXOGRP_UQZM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot5(X_test_map, particle_vector):\n",
        "\n",
        "  # Plotting random maps with information\n",
        "\n",
        "  # Select 5 random indices from the test data\n",
        "  random_indices = np.random.choice(range(X_test_map.shape[0]), size=5, replace=False)\n",
        "\n",
        "  # Create a subplot with 5 rows and 1 column\n",
        "  fig, axes = plt.subplots(nrows=5, ncols=1, figsize=(8, 20))\n",
        "\n",
        "  # Iterate over the random indices and plot each map with information\n",
        "  for i, index in enumerate(random_indices):\n",
        "      # Get the map and corresponding information\n",
        "      map_data = X_test_map[index, :, :, 0]\n",
        "      mass_category = particle_vector[index].mass_category\n",
        "      ckov = particle_vector[index].ckov\n",
        "      mip_position = particle_vector[index].mip_position\n",
        "      momentum = particle_vector[index].momentum\n",
        "\n",
        "      # Plot the map\n",
        "      axes[i].imshow(map_data, cmap='gray')\n",
        "\n",
        "      # Add a red dot at the MIP position\n",
        "      axes[i].plot(mip_position[0], mip_position[1], 'ro')\n",
        "\n",
        "      # Set the title with the information\n",
        "      axes[i].set_title(f\"Mass: {mass_category}, CKOV: {ckov}, MIP Position: {mip_position}, Momentum: {momentum}\")\n",
        "      axes[i].axis('off')\n",
        "\n",
        "  # Adjust the spacing between subplots\n",
        "  plt.tight_layout()\n",
        "\n",
        "  # Show the plot\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "PlFS0yz2Feuz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_lr_scheduler(num_epochs = 10):\n",
        "\n",
        "  tf.random.set_seed(42)\n",
        "  div = num_epochs/4\n",
        "  print(\"div =\", div)\n",
        "  print(\"1e-4 * 10**(epoch/div) = \", 1e-4 * 10**(num_epochs/div))\n",
        "  lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-4 * 10**(epoch/div)) # traverse a set of learning rate values starting from 1e-4, increasing by 10**(epoch/20) every epoch\n",
        "  return lr_scheduler\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def plot_lr(num_epochs = 10, history = None):\n",
        "  div = num_epochs/4\n",
        "  lrs = 1e-4 * (10 ** (np.arange(num_epochs)/div))\n",
        "  plt.figure(figsize=(10, 7))\n",
        "  plt.semilogx(lrs, history.history[\"loss\"]) # we want the x-axis (learning rate) to be log scale\n",
        "  plt.xlabel(\"Learning Rate\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.title(\"Learning rate vs. loss\");"
      ],
      "metadata": {
        "id": "pcY6LVGpxZQI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ParticleDataUtils:\n",
        "    class Candidate2:\n",
        "        def __init__(self, x, y, candStatus):\n",
        "            self.x = x\n",
        "            self.y = y\n",
        "            self.candStatus = candStatus\n",
        "\n",
        "    class ParticleInfo: # pls help me understand if any of these fields are missing from the X_train :\n",
        "        def __init__(self, momentum, mass, energy, refractiveIndex, ckov, xRad, yRad, xPC, yPC, thetaP, phiP, arrayInfo, candsCombined):\n",
        "            self.momentum = momentum # this dhould be with\n",
        "            self.mass = mass    # not with\n",
        "            self.energy = energy # with\n",
        "            self.refractiveIndex = refractiveIndex # with\n",
        "            self.ckov = ckov # not with\n",
        "            self.xRad = xRad # with\n",
        "            self.yRad = yRad # with\n",
        "            self.xPC = xPC# with\n",
        "            self.yPC = yPC# with\n",
        "            self.thetaP = thetaP# with\n",
        "            self.phiP = phiP# with\n",
        "            self.arrayInfo = arrayInfo # do not include this to model\n",
        "            self.candsCombined = candsCombined # with the field candStatus is a int that is 0..7, please make it categorical\n",
        "            self.mass_category = self.infer_mass_category(mass)\n",
        "\n",
        "        @staticmethod\n",
        "        def infer_mass_category(mass):\n",
        "            pion_mass = 0.1396\n",
        "            proton_mass = 0.938\n",
        "            kaon_mass = 0.4937\n",
        "\n",
        "            if abs(mass - pion_mass) < 1e-6:\n",
        "                return \"pion\"\n",
        "            elif abs(mass - proton_mass) < 1e-6:\n",
        "                return \"proton\"\n",
        "            elif abs(mass - kaon_mass) < 1e-6:\n",
        "                return \"kaon\"\n",
        "            else:\n",
        "                return \"unknown\"\n",
        "\n",
        "def load_particle_info_from_hdf5(filename):\n",
        "    particle_vector = []\n",
        "\n",
        "    with h5py.File(filename, 'r') as file:\n",
        "        first_group_name = list(file.keys())[0]\n",
        "        group = file[first_group_name]\n",
        "\n",
        "        print(\"Attributes for group '{}':\".format(first_group_name))\n",
        "        for attr_name, attr_value in group.attrs.items():\n",
        "          print(\"{}: {}\".format(attr_name, attr_value))\n",
        "\n",
        "\n",
        "        for i, group_name in enumerate(file):\n",
        "            group = file[group_name]\n",
        "\n",
        "            momentum = group.attrs['Momentum']\n",
        "            mass = group.attrs['Mass']\n",
        "            energy = group.attrs['Energy']\n",
        "            refractiveIndex = group.attrs['RefractiveIndex']\n",
        "            ckov = group.attrs['Ckov']\n",
        "            xRad = group.attrs['xRad']\n",
        "            yRad = group.attrs['yRad']\n",
        "            xPC = group.attrs['xPC']\n",
        "            yPC = group.attrs['yPC']\n",
        "            thetaP = group.attrs['ThetaP']\n",
        "            phiP = group.attrs['PhiP']\n",
        "\n",
        "            # Read arrayInfo\n",
        "            arrayInfo_dataset = group['ArrayInfo']\n",
        "            arrayInfo_data = arrayInfo_dataset[...]\n",
        "\n",
        "            # Read candsCombined as complex type\n",
        "            candsCombined_dataset = group['candsCombined']\n",
        "            candsCombined_data = candsCombined_dataset[...]\n",
        "            candsCombined = [ParticleDataUtils.Candidate2(x['x'], x['y'], x['candStatus']) for x in candsCombined_data]\n",
        "\n",
        "            particle_info = ParticleDataUtils.ParticleInfo(\n",
        "                momentum, mass, energy, refractiveIndex, ckov, xRad, yRad, xPC, yPC, thetaP, phiP, arrayInfo=arrayInfo_data, candsCombined=candsCombined)\n",
        "\n",
        "            particle_vector.append(particle_info)\n",
        "\n",
        "    return particle_vector\n",
        "\n",
        "def read_particle_data_from_file(filename=\"particle.h5\"):\n",
        "    drive_path = '/content/drive/MyDrive/Colab Notebooks/CERN_ML/CNN_PID/test/'  # Update the path to your Google Drive folder\n",
        "    file_path = os.path.join(drive_path, filename)\n",
        "    particle_vector = load_particle_info_from_hdf5(file_path)\n",
        "    return particle_vector\n",
        "\n",
        "# Prepare the training data\n",
        "#X_train_candsCombined_xy = [np.array([(cand.x, cand.y) for cand in particle.candsCombined]) for particle in particle_vector]\n",
        "#X_train_candsCombined_status = [to_categorical([cand.candStatus for cand in particle.candsCombined], num_classes=8) for particle in particle_vector]\n",
        "\n",
        "\n",
        "# Prepare the additional training data\n",
        "X_train_candsCombined_xy = []\n",
        "X_train_candsCombined_status = []\n",
        "# Extract other fields\n",
        "\n",
        "\n",
        "\n",
        "filename = 'ParticleInfo15k.h5'\n",
        "particle_vector = read_particle_data_from_file(filename)\n",
        "\n",
        "\n",
        "for particle in particle_vector:\n",
        "    cand_xy_list = []\n",
        "    cand_status_list = []\n",
        "    for cand in particle.candsCombined:\n",
        "        cand_xy_list.append([cand.x, cand.y])\n",
        "        cand_status_list.append(cand.candStatus)\n",
        "\n",
        "    X_train_candsCombined_xy.append(cand_xy_list)\n",
        "    X_train_candsCombined_status.append(cand_status_list)\n",
        "\n",
        "# Convert them into NumPy arrays\n",
        "X_train_candsCombined_xy = np.array(X_train_candsCombined_xy)\n",
        "X_train_candsCombined_status = np.array(X_train_candsCombined_status)\n",
        "# Normalize X_train_candsCombined_xy\n",
        "X_train_candsCombined_xy = std_scaler.fit_transform(X_train_candsCombined_xy)\n",
        "\n",
        "\n",
        "# One-hot encode X_train_candsCombined_status\n",
        "X_train_candsCombined_status_encoded = label_binarizer.fit_transform(X_train_candsCombined_status)\n",
        "\n",
        "\n",
        "X_train_rad_position = [particle.xRad for particle in particle_vector]\n",
        "X_train_phi = [particle.phiP for particle in particle_vector]\n",
        "X_train_theta = [particle.thetaP for particle in particle_vector]\n",
        "X_train_energy = [particle.energy for particle in particle_vector]\n",
        "\n",
        "# Convert to NumPy arrays\n",
        "X_train_rad_position = np.array(X_train_rad_position)\n",
        "X_train_phi = np.array(X_train_phi)\n",
        "X_train_theta = np.array(X_train_theta)\n",
        "X_train_energy = np.array(X_train_energy)\n",
        "X_train_candsCombined_xy = np.array(X_train_candsCombined_xy)\n",
        "X_train_candsCombined_status = np.array(X_train_candsCombined_status)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# One-hot encode the labels\n",
        "label_binarizer = LabelBinarizer()\n",
        "y_train_encoded = label_binarizer.fit_transform([particle.mass_category for particle in particle_vector])\n",
        "y_test_encoded = label_binarizer.transform([particle.mass_category for particle in particle_vector])\n",
        "X_train_momentum = [particle.momentum for particle in particle_vector]\n",
        "\n",
        "X_train_momentum = np.array(X_train_momentum)\n",
        "# create a scaler object\n",
        "std_scaler = StandardScaler()\n",
        "# Normalize X_train_refractive_index\n",
        "X_train_refractive_index = X_train_refractive_index.reshape(-1, 1)\n",
        "X_train_refractive_index = std_scaler.fit_transform(X_train_refractive_index)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Splitting the data into train and test sets\n",
        "X_train_momentum, X_test_momentum, X_train_refractive_index, X_test_refractive_index, X_train_mip_position, X_test_mip_position, X_train_candsCombined_xy, X_test_candsCombined_xy, X_train_candsCombined_status, X_test_candsCombined_status, X_train_rad_position, X_test_rad_position, X_train_phi, X_test_phi, X_train_theta, X_test_theta, X_train_energy, X_test_energy, y_train, y_test = train_test_split(\n",
        "    X_train_momentum,\n",
        "    X_train_refractive_index,\n",
        "    X_train_mip_position,\n",
        "    X_train_candsCombined_xy,\n",
        "    X_train_candsCombined_status,\n",
        "    X_train_rad_position,\n",
        "    X_train_phi,\n",
        "    X_train_theta,\n",
        "    X_train_energy,\n",
        "    y_train_encoded,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Define the input shapes\n",
        "momentum_shape = (1,)\n",
        "refractive_index_shape = (1,)\n",
        "mip_position_shape = (2,)\n",
        "candsCombined_xy_shape = X_train_candsCombined_xy.shape[1:]  # adjust as necessary\n",
        "candsCombined_status_shape = X_train_candsCombined_status.shape[1:]  # adjust as necessary\n",
        "rad_position_shape = (1,)\n",
        "phi_shape = (1,)\n",
        "theta_shape = (1,)\n",
        "energy_shape = (1,)\n",
        "\n",
        "# Define inputs\n",
        "# vectors, each element number n containing a x,y pair in  candsCombined_xy_input should be paired with the categorical value in\n",
        "# candsCombined_status_input\n",
        "candsCombined_xy_input = Input(shape=candsCombined_xy_shape, name='candsCombined_xy_input')\n",
        "candsCombined_status_input = Input(shape=candsCombined_status_shape, name='candsCombined_status_input')\n",
        "\n",
        "#scalars\n",
        "momentum_input = Input(shape=momentum_shape, name='momentum_input')\n",
        "refractive_index_input = Input(shape=refractive_index_shape, name='refractive_index_input')\n",
        "phi_input = Input(shape=phi_shape, name='phi_input')\n",
        "theta_input = Input(shape=theta_shape, name='theta_input')\n",
        "energy_input = Input(shape=energy_shape, name='energy_input')\n",
        "\n",
        "# x,y pairs\n",
        "mip_position_input = Input(shape=mip_position_shape, name='mip_position_input')\n",
        "rad_position_input = Input(shape=rad_position_shape, name='rad_position_input')\n",
        "\n",
        "\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Flatten, Input, concatenate, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Define an input processing layer for candsCombined_xy\n",
        "xy_dense = Dense(32, activation='relu')(candsCombined_xy_input)\n",
        "xy_dense = Dropout(0.2)(xy_dense)\n",
        "\n",
        "# Define an input processing layer for candsCombined_status\n",
        "status_dense = Dense(32, activation='relu')(candsCombined_status_input)\n",
        "status_dense = Dropout(0.2)(status_dense)\n",
        "\n",
        "# Concatenate the xy and status layers\n",
        "candsCombined_processed = concatenate([xy_dense, status_dense])\n",
        "\n",
        "# Define a dense layer for the concatenated xy and status\n",
        "candsCombined_dense = Dense(64, activation='relu')(candsCombined_processed)\n",
        "candsCombined_dense = Dropout(0.2)(candsCombined_dense)\n",
        "\n",
        "# Define the scalar inputs\n",
        "scalar_inputs = concatenate([momentum_input, refractive_index_input, phi_input, theta_input, energy_input])\n",
        "\n",
        "# Define a dense layer for the scalar inputs\n",
        "scalar_dense = Dense(64, activation='relu')(scalar_inputs)\n",
        "scalar_dense = Dropout(0.2)(scalar_dense)\n",
        "\n",
        "# Define the position inputs\n",
        "position_inputs = concatenate([mip_position_input, rad_position_input])\n",
        "\n",
        "# Define a dense layer for the position inputs\n",
        "position_dense = Dense(64, activation='relu')(position_inputs)\n",
        "position_dense = Dropout(0.2)(position_dense)\n",
        "\n",
        "# Concatenate all the processed inputs\n",
        "concat = concatenate([candsCombined_dense, scalar_dense, position_dense])\n",
        "\n",
        "# Define the final fully connected layers\n",
        "fc1 = Dense(128, activation='relu')(concat)\n",
        "fc1 = BatchNormalization()(fc1)\n",
        "fc1 = Dropout(0.1)(fc1)\n",
        "\n",
        "fc2 = Dense(32, activation='relu')(fc1)\n",
        "fc2 = BatchNormalization()(fc2)\n",
        "fc2 = Dropout(0.1)(fc2)\n",
        "\n",
        "output = Dense(3, activation='softmax')(fc2)  # Predicting mass categories\n",
        "\n",
        "# Define the model\n",
        "model = Model(inputs=[candsCombined_xy_input, candsCombined_status_input, momentum_input, refractive_index_input, phi_input, theta_input, energy_input, mip_position_input, rad_position_input], outputs=output)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002), loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 797
        },
        "id": "IjrxNzz0LMqU",
        "outputId": "830040f4-82ba-4e19-9cd5-609bee1b674c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attributes for group 'Particle0':\n",
            "Ckov: 0.609717845916748\n",
            "Energy: 5.904012680053711\n",
            "Mass: 0.9380000233650208\n",
            "Momentum: 2.986387252807617\n",
            "PhiP: -0.4440668523311615\n",
            "RefractiveIndex: 1.278549075126648\n",
            "ThetaP: 0.1677367240190506\n",
            "xPC: 27.119104385375977\n",
            "xRad: 25.70473289489746\n",
            "yPC: 130.7153778076172\n",
            "yRad: 131.38827514648438\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-4e006c2012c1>\u001b[0m in \u001b[0;36m<cell line: 102>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ParticleInfo15k.h5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m \u001b[0mparticle_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_particle_data_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-4e006c2012c1>\u001b[0m in \u001b[0;36mread_particle_data_from_file\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mdrive_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Colab Notebooks/CERN_ML/CNN_PID/test/'\u001b[0m  \u001b[0;31m# Update the path to your Google Drive folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrive_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mparticle_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_particle_info_from_hdf5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mparticle_vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-4e006c2012c1>\u001b[0m in \u001b[0;36mload_particle_info_from_hdf5\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;31m# Read candsCombined as complex type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mcandsCombined_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'candsCombined'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0mcandsCombined_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcandsCombined_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mcandsCombined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mParticleDataUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCandidate2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'candStatus'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcandsCombined_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    355\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid HDF5 object reference\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0moid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_e\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             raise TypeError(\"Accessing a group is done with bytes or str, \"\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5o.pyx\u001b[0m in \u001b[0;36mh5py.h5o.open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"Unable to open object (object 'candsCombined' doesn't exist)\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training loss and validation loss\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plot training accuracy and validation accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DxlPsrmTHFXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import os\n",
        "import h5py\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, concatenate, BatchNormalization, MaxPooling2D, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class ParticleDataUtils:\n",
        "    class ParticleInfo:\n",
        "        def __init__(self, momentum, mass, energy, refractiveIndex, ckov, filledBins):\n",
        "            self.momentum = momentum\n",
        "            self.mass = mass\n",
        "            self.energy = energy\n",
        "            self.refractiveIndex = refractiveIndex\n",
        "            self.ckov = ckov\n",
        "            self.filledBins = filledBins\n",
        "            self.mass_category = self.infer_mass_category(mass)  # Infer mass category based on mass\n",
        "\n",
        "        @staticmethod\n",
        "        def infer_mass_category(mass):\n",
        "            pion_mass = 0.1396\n",
        "            proton_mass = 0.938\n",
        "            kaon_mass = 0.4937\n",
        "\n",
        "            if abs(mass - pion_mass) < 1e-6:\n",
        "                return \"pion\"\n",
        "            elif abs(mass - proton_mass) < 1e-6:\n",
        "                return \"proton\"\n",
        "            elif abs(mass - kaon_mass) < 1e-6:\n",
        "                return \"kaon\"\n",
        "            else:\n",
        "                return \"unknown\"\n",
        "\n",
        "def save_particle_info_to_hdf5(particle_vector, filename):\n",
        "    with h5py.File(filename, 'w') as file:\n",
        "        for i, particle in enumerate(particle_vector):\n",
        "            # Create a group for each particle\n",
        "            group = file.create_group(f'Particle{i}')\n",
        "\n",
        "            # Store scalar values\n",
        "            group.attrs['Momentum'] = particle.momentum\n",
        "            group.attrs['Mass'] = particle.mass\n",
        "            group.attrs['Energy'] = particle.energy\n",
        "            group.attrs['RefractiveIndex'] = particle.refractiveIndex\n",
        "            group.attrs['Ckov'] = particle.ckov\n",
        "            group.attrs['MassCategory'] = particle.massCategory  # Save the mass category\n",
        "\n",
        "            # Store filledBins to HDF5 file\n",
        "            group.create_dataset(\"FilledBins\", data=np.array(particle.filledBins, dtype=[('x', 'f'), ('y', 'f')]))\n",
        "\n",
        "def load_particle_info_from_hdf5(filename):\n",
        "    particle_vector = []\n",
        "\n",
        "    with h5py.File(filename, 'r') as file:\n",
        "        for i, group_name in enumerate(file):\n",
        "            group = file[group_name]\n",
        "\n",
        "            # Read scalar values\n",
        "            momentum = group.attrs['Momentum']\n",
        "            mass = group.attrs['Mass']\n",
        "            energy = group.attrs['Energy']\n",
        "            refractiveIndex = group.attrs['RefractiveIndex']\n",
        "            ckov = group.attrs['Ckov']\n",
        "            #massCategory = group.attrs['MassCategory']  # Load the mass category\n",
        "\n",
        "            # Read filledBins\n",
        "            filled_bins_dataset = group['FilledBins']\n",
        "            filled_bins_data = filled_bins_dataset[...]  # Retrieve the data as a numpy array\n",
        "\n",
        "            filled_bins = filled_bins_data.tolist()  # Convert the numpy array to a list\n",
        "\n",
        "            particle_info = ParticleDataUtils.ParticleInfo(\n",
        "                momentum, mass, energy, refractiveIndex, ckov, filledBins=filled_bins)\n",
        "\n",
        "            #particle_info.massCategory = massCategory  # Set the mass category\n",
        "\n",
        "            particle_vector.append(particle_info)\n",
        "\n",
        "    return particle_vector\n",
        "\n",
        "\n",
        "def read_particle_data_from_file(filename=\"particle.h5\"):\n",
        "    drive_path = '/content/drive/MyDrive/Colab Notebooks/CERN_ML/CNN_PID/test/'  # Update the path to your Google Drive folder\n",
        "    file_path = os.path.join(drive_path, filename)\n",
        "    particle_vector = load_particle_info_from_hdf5(file_path)\n",
        "    return particle_vector\n",
        "\n",
        "# Example usage\n",
        "filename = 'ParticleInfo.h5'\n",
        "particle_vector = read_particle_data_from_file(filename)\n",
        "\n",
        "# Create an empty list to store the map_data for all particles\n",
        "map_data_list = []\n",
        "\n",
        "# Create an empty 2D map\n",
        "map_shape = (144, 160)\n",
        "map_data = np.zeros(map_shape)\n",
        "\n",
        "# Iterate over all particles in the particle_vector\n",
        "for i, particle in enumerate(particle_vector):\n",
        "    # Reset the map_data for each particle\n",
        "    map_data = np.zeros(map_shape)\n",
        "\n",
        "    for entry in particle.filledBins:\n",
        "        x = int(round(entry[0]))\n",
        "        y = int(round(entry[1]))\n",
        "        # Shift the coordinates to the center\n",
        "        if 0 <= y < map_shape[0] and 0 <= x < map_shape[1]:\n",
        "            map_data[y, x] = 1\n",
        "\n",
        "    # Add the map_data to the list\n",
        "    map_data_list.append(map_data)\n",
        "\n",
        "# Convert the map_data_list to a NumPy array\n",
        "map_data_array = np.array(map_data_list)\n",
        "\n",
        "# Prepare the training data\n",
        "X_train_map = map_data_array  # Use the map_data_array as X_train_map\n",
        "X_train_momentum = np.array([particle.momentum for particle in particle_vector])\n",
        "X_train_refractive_index = np.array([particle.refractiveIndex for particle in particle_vector])\n",
        "y_train = np.array([particle.mass_category for particle in particle_vector])\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# create a scaler object\n",
        "std_scaler = StandardScaler()\n",
        "\n",
        "# fit and transform the data\n",
        "X_train_momentum = np.array(X_train_momentum).reshape(-1, 1)\n",
        "X_train_momentum = std_scaler.fit_transform(X_train_momentum)\n",
        "\n",
        "X_train_refractive_index = np.array(X_train_refractive_index).reshape(-1, 1)\n",
        "X_train_refractive_index = std_scaler.fit_transform(X_train_refractive_index)\n",
        "\n",
        "\n",
        "# # Split the data into train and test sets\n",
        "# X_train_map, X_test_map, X_train_momentum, X_test_momentum, X_train_refractive_index, X_test_refractive_index, y_train, y_test = train_test_split(\n",
        "#     X_train_map,\n",
        "#     X_train_momentum,\n",
        "#     X_train_refractive_index,\n",
        "#     y_train,\n",
        "#     test_size=0.2,\n",
        "#     random_state=42\n",
        "# )\n",
        "\n",
        "# Reshape the input map to include the channel dimension\n",
        "X_train_map = X_train_map.reshape(X_train_map.shape[0], X_train_map.shape[1], X_train_map.shape[2], 1)\n",
        "# X_test_map = X_test_map.reshape(X_test_map.shape[0], X_test_map.shape[1], X_test_map.shape[2], 1)\n",
        "\n",
        "# Define input shapes\n",
        "map_shape = X_train_map.shape[1:]\n",
        "momentum_shape = (1,)\n",
        "refractive_index_shape = (1,)\n",
        "\n",
        "# Define inputs\n",
        "map_input = Input(shape=map_shape, name='map_input')\n",
        "momentum_input = Input(shape=momentum_shape, name='momentum_input')\n",
        "refractive_index_input = Input(shape=refractive_index_shape, name='refractive_index_input')\n",
        "\n",
        "# Define convolutional layers for the map input\n",
        "conv1 = Conv2D(32, (3, 3))(map_input)\n",
        "conv1 = BatchNormalization()(conv1)\n",
        "conv1 = tf.keras.activations.relu(conv1)\n",
        "conv1 = MaxPooling2D((2, 2))(conv1)  # Add max pooling after conv1\n",
        "conv1 = Dropout(0.2)(conv1)  # Add dropout after max pooling\n",
        "\n",
        "\n",
        "conv2 = Conv2D(64, (5, 5))(conv1)\n",
        "conv2 = BatchNormalization()(conv2)\n",
        "conv2 = tf.keras.activations.relu(conv2)\n",
        "\n",
        "conv2 = MaxPooling2D((2, 2))(conv2)  # Add max pooling after conv2\n",
        "conv2 = Dropout(0.2)(conv2)  # Add dropout after max pooling\n",
        "\n",
        "\n",
        "conv3 = Conv2D(16, (7, 7), activation='relu')(conv2)\n",
        "\n",
        "conv3 = MaxPooling2D((2, 2))(conv3)  # Add max pooling after conv3\n",
        "conv3 = Dropout(0.3)(conv3)  # Add dropout after max pooling\n",
        "\n",
        "\n",
        "# Flatten\n",
        "flat_map = Flatten()(conv3)\n",
        "\n",
        "# Concatenate map features with other inputs\n",
        "concat = concatenate([flat_map, momentum_input, refractive_index_input])\n",
        "\n",
        "# Define fully connected layers\n",
        "fc1 = Dense(128)(concat)\n",
        "fc1 = BatchNormalization()(fc1)\n",
        "fc1 = tf.keras.activations.relu(fc1)\n",
        "fc1 = Dropout(0.3)(fc1)  # Add dropout after the first fully connected layer\n",
        "\n",
        "fc2 = Dense(32)(fc1)\n",
        "fc2 = BatchNormalization()(fc2)\n",
        "fc2 = tf.keras.activations.relu(fc2)\n",
        "fc2 = Dropout(0.3)(fc2)  # Add dropout after the second fully connected layer\n",
        "\n",
        "output = Dense(3, activation='softmax')(fc2)  # Predicting mass categories\n",
        "\n",
        "# Define the model\n",
        "model = Model(inputs=[map_input, momentum_input, refractive_index_input], outputs=output)\n",
        "\n",
        "# Compile the model\n",
        "#model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 0.0002), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 0.0002), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print the shapes of the input data\n",
        "print(\"Dimensions of the input data:\")\n",
        "print(\"Training:\")\n",
        "print(\"  X_train_map =\", X_train_map.shape)\n",
        "print(\"  X_train_momentum =\", X_train_momentum.shape)\n",
        "print(\"  X_train_refractive_index =\", X_train_refractive_index.shape)\n",
        "print(\"  y_train =\", y_train.shape)\n",
        "\n",
        "# print(\"Testing:\")\n",
        "# print(\"  X_test_map =\", X_test_map.shape)\n",
        "# print(\"  X_test_momentum =\", X_test_momentum.shape)\n",
        "# print(\"  X_test_refractive_index =\", X_test_refractive_index.shape)\n",
        "# print(\"  y_test =\", y_test.shape)\n",
        "\n",
        "print(np.any(X_train_map == None))\n",
        "print(np.any(X_train_momentum == None))\n",
        "print(np.any(X_train_refractive_index == None))\n",
        "print(np.any(y_train == None))\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "y_train_encoded = encoder.fit_transform(y_train)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train_map, X_test_map, \\\n",
        "X_train_momentum, X_test_momentum, \\\n",
        "X_train_refractive_index, X_test_refractive_index, \\\n",
        "y_train, y_test = train_test_split(X_train_map, X_train_momentum, X_train_refractive_index, y_train_encoded,\n",
        "                                   test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    x = [X_train_map, X_train_momentum, X_train_refractive_index],\n",
        "    y = y_train,\n",
        "    epochs=20,\n",
        "    validation_data=([X_test_map, X_test_momentum, X_test_refractive_index], y_test),\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "\n",
        "#plot_lr(num_epochs = 20, history = history)\n",
        "\n",
        "\n",
        "plot_random_element(X_train_map)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "ca45fa4c-da4e-4647-fd3d-8d078779d167",
        "id": "wTJhymBA5bFr"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-10e6fe076968>\u001b[0m in \u001b[0;36m<cell line: 92>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;31m# Example usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ParticleInfo.h5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m \u001b[0mparticle_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_particle_data_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;31m# Create an empty list to store the map_data for all particles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-10e6fe076968>\u001b[0m in \u001b[0;36mread_particle_data_from_file\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mdrive_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Colab Notebooks/CERN_ML/CNN_PID/test/'\u001b[0m  \u001b[0;31m# Update the path to your Google Drive folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrive_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0mparticle_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_particle_info_from_hdf5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mparticle_vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-10e6fe076968>\u001b[0m in \u001b[0;36mload_particle_info_from_hdf5\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# Read filledBins\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mfilled_bins_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'FilledBins'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0mfilled_bins_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilled_bins_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Retrieve the data as a numpy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    355\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid HDF5 object reference\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0moid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_e\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             raise TypeError(\"Accessing a group is done with bytes or str, \"\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5o.pyx\u001b[0m in \u001b[0;36mh5py.h5o.open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"Unable to open object (object 'FilledBins' doesn't exist)\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Make predictions on the validation data\n",
        "y_train_pred = model.predict([X_train_map, X_train_momentum, X_train_refractive_index])\n",
        "\n",
        "# Convert the predictions from categorical back to original labels\n",
        "y_train_pred_classes = np.argmax(y_train_pred, axis=1)\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(y_train, y_train_pred_classes)\n",
        "\n",
        "# Use seaborn to visualize the confusion matrix\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
        "plt.title('Confusion matrix Training Data')\n",
        "plt.ylabel('Actual label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bLBU0quO4IYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Make predictions on the validation data\n",
        "y_val_pred = model.predict([X_test_map, X_test_momentum, X_test_refractive_index])\n",
        "\n",
        "# Convert the predictions from categorical back to original labels\n",
        "y_val_pred_classes = np.argmax(y_val_pred, axis=1)\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_val_pred_classes)\n",
        "\n",
        "# Use seaborn to visualize the confusion matrix\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
        "plt.title('Confusion matrix Validation Data')\n",
        "plt.ylabel('Actual label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "v-ED5GaZ5265"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "O_ulo0AI1buN",
        "outputId": "bfac17c0-5d74-49b2-826b-2a8a631fabfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-0acf2c2b6d91>\u001b[0m in \u001b[0;36m<cell line: 71>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;31m# Example usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ParticleInfo.h5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0mparticle_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_particle_data_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m# Prepare the training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-0acf2c2b6d91>\u001b[0m in \u001b[0;36mread_particle_data_from_file\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mdrive_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Colab Notebooks/CERN_ML/CNN_PID/test/'\u001b[0m  \u001b[0;31m# Update the path to your Google Drive folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrive_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mparticle_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_particle_info_from_hdf5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mparticle_vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-0acf2c2b6d91>\u001b[0m in \u001b[0;36mload_particle_info_from_hdf5\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;31m# Read filledBins\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mfilled_bins_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'FilledBins'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m             \u001b[0mfilled_bins_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilled_bins_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Retrieve the data as a numpy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    355\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid HDF5 object reference\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0moid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_e\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             raise TypeError(\"Accessing a group is done with bytes or str, \"\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5o.pyx\u001b[0m in \u001b[0;36mh5py.h5o.open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"Unable to open object (object 'FilledBins' doesn't exist)\""
          ]
        }
      ],
      "source": [
        "import os\n",
        "import h5py\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, concatenate\n",
        "\n",
        "class ParticleDataUtils:\n",
        "    class ParticleInfo:\n",
        "        def __init__(self, momentum, mass, energy, refractiveIndex, ckov, filledBins):\n",
        "            self.momentum = momentum\n",
        "            self.mass = mass\n",
        "            self.energy = energy\n",
        "            self.refractiveIndex = refractiveIndex\n",
        "            self.ckov = ckov\n",
        "            self.filledBins = filledBins\n",
        "\n",
        "def save_particle_info_to_hdf5(particle_vector, filename):\n",
        "    with h5py.File(filename, 'w') as file:\n",
        "        for i, particle in enumerate(particle_vector):\n",
        "            # Create a group for each particle\n",
        "            group = file.create_group(f'Particle{i}')\n",
        "\n",
        "            # Store scalar values\n",
        "            group.attrs['Momentum'] = particle.momentum\n",
        "            group.attrs['Mass'] = particle.mass\n",
        "            group.attrs['Energy'] = particle.energy\n",
        "            group.attrs['RefractiveIndex'] = particle.refractiveIndex\n",
        "            group.attrs['Ckov'] = particle.ckov\n",
        "\n",
        "\n",
        "\n",
        "            # Store filledBins to HDF5 file\n",
        "            group.create_dataset(\"FilledBins\", data=np.array(particle.filledBins, dtype=[('x', 'f'), ('y', 'f')]))\n",
        "\n",
        "def load_particle_info_from_hdf5(filename):\n",
        "    particle_vector = []\n",
        "\n",
        "    with h5py.File(filename, 'r') as file:\n",
        "        for i, group_name in enumerate(file):\n",
        "            group = file[group_name]\n",
        "\n",
        "            # Read scalar values\n",
        "            momentum = group.attrs['Momentum']\n",
        "            mass = group.attrs['Mass']\n",
        "            energy = group.attrs['Energy']\n",
        "            refractiveIndex = group.attrs['RefractiveIndex']\n",
        "            ckov = group.attrs['Ckov']\n",
        "\n",
        "            # Read filledBins\n",
        "            filled_bins_dataset = group['FilledBins']\n",
        "            filled_bins_data = filled_bins_dataset[...]  # Retrieve the data as a numpy array\n",
        "\n",
        "            filled_bins = filled_bins_data.tolist()  # Convert the numpy array to a list\n",
        "\n",
        "            particle_info = ParticleDataUtils.ParticleInfo(\n",
        "                momentum, mass, energy, refractiveIndex, ckov, filledBins=filled_bins)\n",
        "\n",
        "            particle_vector.append(particle_info)\n",
        "\n",
        "    return particle_vector\n",
        "\n",
        "def read_particle_data_from_file(filename=\"particle.h5\"):\n",
        "    drive_path = '/content/drive/MyDrive/Colab Notebooks/CERN_ML/CNN_PID/test/'  # Update the path to your Google Drive folder\n",
        "    file_path = os.path.join(drive_path, filename)\n",
        "    particle_vector = load_particle_info_from_hdf5(file_path)\n",
        "    return particle_vector\n",
        "\n",
        "# Example usage\n",
        "filename = 'ParticleInfo.h5'\n",
        "particle_vector = read_particle_data_from_file(filename)\n",
        "\n",
        "# Prepare the training data\n",
        "X_train_map = [np.array(particle.filledBins) for particle in particle_vector]  # Convert to list of arrays\n",
        "X_train_momentum = np.array([particle.momentum for particle in particle_vector])\n",
        "X_train_refractive_index = np.array([particle.refractiveIndex for particle in particle_vector])\n",
        "y_train = np.array([particle.mass for particle in particle_vector])\n",
        "\n",
        "# Convert the NumPy arrays to TensorFlow tensors\n",
        "X_train_map = [tf.convert_to_tensor(arr) for arr in X_train_map]  # Convert to list of TensorFlow tensors\n",
        "X_train_momentum = tf.convert_to_tensor(X_train_momentum)\n",
        "X_train_refractive_index = tf.convert_to_tensor(X_train_refractive_index)\n",
        "y_train = tf.convert_to_tensor(y_train)\n",
        "\n",
        "# Define input shapes\n",
        "map_shape = (160, 144, 2)  # 160 x pads [0..159] | 144 y pads [0..143] | 2D bin values\n",
        "momentum_shape = (1,)  # Scalar input\n",
        "refractive_index_shape = (1,)  # Scalar input\n",
        "\n",
        "# Define inputs\n",
        "map_input = Input(shape=map_shape, name='map_input')\n",
        "momentum_input = Input(shape=momentum_shape, name='momentum_input')\n",
        "refractive_index_input = Input(shape=refractive_index_shape, name='refractive_index_input')\n",
        "\n",
        "# Define convolutional layers for the map input\n",
        "conv1 = Conv2D(32, (3, 3), activation='relu')(map_input)\n",
        "conv2 = Conv2D(32, (3, 3), activation='relu')(conv1)\n",
        "conv3 = Conv2D(32, (3, 3), activation='relu')(conv2)\n",
        "flat_map = Flatten()(conv3)\n",
        "\n",
        "# Concatenate map features with other inputs\n",
        "concat = concatenate([flat_map, momentum_input, refractive_index_input])\n",
        "\n",
        "# Define fully connected layers\n",
        "fc1 = Dense(128, activation='relu')(concat)\n",
        "fc2 = Dense(64, activation='relu')(fc1)\n",
        "output = Dense(1, activation='linear')(fc2)  # Predicting mass as a scalar value\n",
        "\n",
        "# Define the model\n",
        "model = Model(inputs=[map_input, momentum_input, refractive_index_input], outputs=output)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n"
      ]
    }
  ]
}